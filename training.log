2024-05-26 20:05:38,562 - Hyperparameters : {'DATA_PATH': '/scratch/x2817a02/workspace/kaggle/ClimSim/data/', 'BATCH_SIZE': 1024, 'MIN_STD': '1e-8', 'SCHEDULER_PATIENCE': 3, 'SCHEDULER_FACTOR': 0.316, 'EPOCHS': 2, 'PATIENCE': 1, 'PRINT_FREQ': 100, 'BEST_MODEL_PATH': 'best_model.pth', 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 0.01}
2024-05-26 20:28:11,972 - Hyperparameters :
2024-05-26 20:28:11,989 - DATA_PATH: /scratch/x2817a02/workspace/kaggle/ClimSim/data/
2024-05-26 20:28:11,989 - BATCH_SIZE: 1024
2024-05-26 20:28:11,989 - MIN_STD: 1e-8
2024-05-26 20:28:11,989 - SCHEDULER_PATIENCE: 3
2024-05-26 20:28:11,989 - SCHEDULER_FACTOR: 0.316
2024-05-26 20:28:11,989 - EPOCHS: 2
2024-05-26 20:28:11,990 - PATIENCE: 1
2024-05-26 20:28:11,990 - PRINT_FREQ: 100
2024-05-26 20:28:11,990 - BEST_MODEL_PATH: best_model.pth
2024-05-26 20:28:11,990 - LEARNING_RATE: 0.001
2024-05-26 20:28:11,990 - WEIGHT_DECAY: 0.01
2024-05-26 20:31:03,558 - 
Epoch: 1  Val Loss: 0.5009  R2 score: -25.5122
2024-05-26 20:31:03,558 - Validation loss decreased, saving new best model and resetting patience counter.
2024-05-26 20:33:31,631 - 
Epoch: 2  Val Loss: 0.4741  R2 score: -5.2721
2024-05-26 20:33:31,632 - Validation loss decreased, saving new best model and resetting patience counter.
2024-05-30 21:27:21,113 - Hyperparameters :
2024-05-30 21:27:21,122 - DATA_PATH: /data01/jhko/LEAP/
2024-05-30 21:27:21,122 - BATCH_SIZE: 1024
2024-05-30 21:27:21,122 - MIN_STD: 1e-8
2024-05-30 21:27:21,122 - SCHEDULER_PATIENCE: 3
2024-05-30 21:27:21,122 - SCHEDULER_FACTOR: 0.316
2024-05-30 21:27:21,122 - EPOCHS: 2
2024-05-30 21:27:21,122 - PATIENCE: 1
2024-05-30 21:27:21,122 - PRINT_FREQ: 100
2024-05-30 21:27:21,122 - BEST_MODEL_PATH: best_model.pth
2024-05-30 21:27:21,122 - LEARNING_RATE: 0.001
2024-05-30 21:27:21,122 - WEIGHT_DECAY: 0.01
2024-05-30 21:51:11,093 - Hyperparameters :
2024-05-30 21:51:11,094 - DATA_PATH: /data01/jhko/LEAP/
2024-05-30 21:51:11,094 - BATCH_SIZE: 1024
2024-05-30 21:51:11,094 - MIN_STD: 1e-8
2024-05-30 21:51:11,094 - SCHEDULER_PATIENCE: 3
2024-05-30 21:51:11,094 - SCHEDULER_FACTOR: 0.316
2024-05-30 21:51:11,094 - EPOCHS: 2
2024-05-30 21:51:11,094 - PATIENCE: 1
2024-05-30 21:51:11,094 - PRINT_FREQ: 100
2024-05-30 21:51:11,094 - BEST_MODEL_PATH: best_model.pth
2024-05-30 21:51:11,094 - LEARNING_RATE: 0.001
2024-05-30 21:51:11,094 - WEIGHT_DECAY: 0.01
2024-05-30 21:53:48,536 - Hyperparameters :
2024-05-30 21:53:48,536 - DATA_PATH: /data01/jhko/LEAP/
2024-05-30 21:53:48,536 - BATCH_SIZE: 1024
2024-05-30 21:53:48,536 - MIN_STD: 1e-8
2024-05-30 21:53:48,536 - SCHEDULER_PATIENCE: 3
2024-05-30 21:53:48,536 - SCHEDULER_FACTOR: 0.316
2024-05-30 21:53:48,536 - EPOCHS: 2
2024-05-30 21:53:48,536 - PATIENCE: 1
2024-05-30 21:53:48,536 - PRINT_FREQ: 100
2024-05-30 21:53:48,536 - BEST_MODEL_PATH: best_model.pth
2024-05-30 21:53:48,536 - LEARNING_RATE: 0.001
2024-05-30 21:53:48,536 - WEIGHT_DECAY: 0.01
2024-05-30 22:00:12,881 - Hyperparameters :
2024-05-30 22:00:12,898 - DATA_PATH: /data01/jhko/LEAP/
2024-05-30 22:00:12,898 - BATCH_SIZE: 1024
2024-05-30 22:00:12,898 - MIN_STD: 1e-8
2024-05-30 22:00:12,898 - SCHEDULER_PATIENCE: 3
2024-05-30 22:00:12,898 - SCHEDULER_FACTOR: 0.316
2024-05-30 22:00:12,899 - EPOCHS: 2
2024-05-30 22:00:12,899 - PATIENCE: 1
2024-05-30 22:00:12,899 - PRINT_FREQ: 100
2024-05-30 22:00:12,899 - BEST_MODEL_PATH: best_model.pth
2024-05-30 22:00:12,899 - LEARNING_RATE: 0.001
2024-05-30 22:00:12,899 - WEIGHT_DECAY: 0.01
2024-05-30 22:00:34,663 - Epoch: 1  Batch: 100/2198  Train Loss: 0.9438  LR: 1.0e-03  Time: 0:01:42
2024-05-30 22:00:54,832 - Epoch: 1  Batch: 200/2198  Train Loss: 0.8216  LR: 1.0e-03  Time: 0:02:02
2024-05-30 22:01:12,332 - Epoch: 1  Batch: 300/2198  Train Loss: 0.7833  LR: 1.0e-03  Time: 0:02:20
2024-05-30 22:01:29,778 - Epoch: 1  Batch: 400/2198  Train Loss: 0.7607  LR: 1.0e-03  Time: 0:02:37
2024-05-30 22:01:47,379 - Epoch: 1  Batch: 500/2198  Train Loss: 0.7462  LR: 1.0e-03  Time: 0:02:55
2024-05-30 22:02:04,998 - Epoch: 1  Batch: 600/2198  Train Loss: 0.7362  LR: 1.0e-03  Time: 0:03:12
2024-05-30 22:02:21,572 - Epoch: 1  Batch: 700/2198  Train Loss: 0.7289  LR: 1.0e-03  Time: 0:03:29
2024-05-30 22:02:38,442 - Epoch: 1  Batch: 800/2198  Train Loss: 0.7223  LR: 1.0e-03  Time: 0:03:46
2024-05-30 22:02:54,868 - Epoch: 1  Batch: 900/2198  Train Loss: 0.7177  LR: 1.0e-03  Time: 0:04:02
2024-05-30 22:03:12,411 - Epoch: 1  Batch: 1000/2198  Train Loss: 0.7138  LR: 1.0e-03  Time: 0:04:20
2024-05-30 22:03:29,767 - Epoch: 1  Batch: 1100/2198  Train Loss: 0.7101  LR: 1.0e-03  Time: 0:04:37
2024-05-30 22:03:46,576 - Epoch: 1  Batch: 1200/2198  Train Loss: 0.7066  LR: 1.0e-03  Time: 0:04:54
2024-05-30 22:04:03,907 - Epoch: 1  Batch: 1300/2198  Train Loss: 0.7036  LR: 1.0e-03  Time: 0:05:11
2024-05-30 22:04:21,684 - Epoch: 1  Batch: 1400/2198  Train Loss: 0.7015  LR: 1.0e-03  Time: 0:05:29
2024-05-30 22:04:39,335 - Epoch: 1  Batch: 1500/2198  Train Loss: 0.6981  LR: 1.0e-03  Time: 0:05:47
2024-05-30 22:04:56,682 - Epoch: 1  Batch: 1600/2198  Train Loss: 0.6977  LR: 1.0e-03  Time: 0:06:04
2024-05-30 22:05:14,279 - Epoch: 1  Batch: 1700/2198  Train Loss: 0.6944  LR: 1.0e-03  Time: 0:06:22
2024-05-30 22:05:31,498 - Epoch: 1  Batch: 1800/2198  Train Loss: 0.6926  LR: 1.0e-03  Time: 0:06:39
2024-05-30 22:05:49,189 - Epoch: 1  Batch: 1900/2198  Train Loss: 0.6885  LR: 1.0e-03  Time: 0:06:57
2024-05-30 22:06:07,126 - Epoch: 1  Batch: 2000/2198  Train Loss: 0.6882  LR: 1.0e-03  Time: 0:07:14
2024-05-30 22:06:24,694 - Epoch: 1  Batch: 2100/2198  Train Loss: 0.6857  LR: 1.0e-03  Time: 0:07:32
2024-05-30 22:07:25,257 - 
Epoch: 1  Val Loss: 0.6697  R2 score: -12.0860
2024-05-30 22:07:25,257 - Validation loss decreased, saving new best model and resetting patience counter.
2024-05-30 22:07:43,739 - Epoch: 2  Batch: 100/2198  Train Loss: 0.6823  LR: 1.0e-03  Time: 0:08:51
2024-05-30 22:08:02,260 - Epoch: 2  Batch: 200/2198  Train Loss: 0.6817  LR: 1.0e-03  Time: 0:09:10
2024-05-30 22:08:20,226 - Epoch: 2  Batch: 300/2198  Train Loss: 0.6806  LR: 1.0e-03  Time: 0:09:28
2024-05-30 22:08:37,032 - Epoch: 2  Batch: 400/2198  Train Loss: 0.6784  LR: 1.0e-03  Time: 0:09:44
2024-05-30 22:08:54,560 - Epoch: 2  Batch: 500/2198  Train Loss: 0.6781  LR: 1.0e-03  Time: 0:10:02
2024-05-30 22:09:11,680 - Epoch: 2  Batch: 600/2198  Train Loss: 0.6757  LR: 1.0e-03  Time: 0:10:19
2024-05-30 22:09:29,323 - Epoch: 2  Batch: 700/2198  Train Loss: 0.6765  LR: 1.0e-03  Time: 0:10:37
2024-05-30 22:09:46,194 - Epoch: 2  Batch: 800/2198  Train Loss: 0.6736  LR: 1.0e-03  Time: 0:10:54
2024-05-30 22:10:04,336 - Epoch: 2  Batch: 900/2198  Train Loss: 0.6735  LR: 1.0e-03  Time: 0:11:12
2024-05-30 22:10:22,072 - Epoch: 2  Batch: 1000/2198  Train Loss: 0.6725  LR: 1.0e-03  Time: 0:11:29
2024-05-30 22:10:39,166 - Epoch: 2  Batch: 1100/2198  Train Loss: 0.6715  LR: 1.0e-03  Time: 0:11:46
2024-05-30 22:10:57,087 - Epoch: 2  Batch: 1200/2198  Train Loss: 0.6692  LR: 1.0e-03  Time: 0:12:04
2024-05-30 22:11:15,615 - Epoch: 2  Batch: 1300/2198  Train Loss: 0.6700  LR: 1.0e-03  Time: 0:12:23
2024-05-30 22:11:32,743 - Epoch: 2  Batch: 1400/2198  Train Loss: 0.6684  LR: 1.0e-03  Time: 0:12:40
2024-05-30 22:11:54,076 - Epoch: 2  Batch: 1500/2198  Train Loss: 0.6676  LR: 1.0e-03  Time: 0:13:01
2024-05-30 22:12:11,249 - Epoch: 2  Batch: 1600/2198  Train Loss: 0.6666  LR: 1.0e-03  Time: 0:13:19
2024-05-30 22:12:31,989 - Epoch: 2  Batch: 1700/2198  Train Loss: 0.6660  LR: 1.0e-03  Time: 0:13:39
2024-05-30 22:12:48,333 - Epoch: 2  Batch: 1800/2198  Train Loss: 0.6658  LR: 1.0e-03  Time: 0:13:56
2024-05-30 22:13:05,991 - Epoch: 2  Batch: 1900/2198  Train Loss: 0.6656  LR: 1.0e-03  Time: 0:14:13
2024-05-30 22:13:22,862 - Epoch: 2  Batch: 2000/2198  Train Loss: 0.6641  LR: 1.0e-03  Time: 0:14:30
2024-05-30 22:13:40,588 - Epoch: 2  Batch: 2100/2198  Train Loss: 0.6633  LR: 1.0e-03  Time: 0:14:48
2024-05-30 22:14:37,658 - 
Epoch: 2  Val Loss: 0.6474  R2 score: 0.2119
2024-05-30 22:14:37,659 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-16 21:25:48,875 - Hyperparameters :
2024-06-16 21:25:48,883 - DATA_PATH: /data01/jhko/LEAP/
2024-06-16 21:25:48,883 - BATCH_SIZE: 1024
2024-06-16 21:25:48,883 - MIN_STD: 1e-8
2024-06-16 21:25:48,883 - SCHEDULER_PATIENCE: 3
2024-06-16 21:25:48,883 - SCHEDULER_FACTOR: 0.316
2024-06-16 21:25:48,883 - EPOCHS: 19
2024-06-16 21:25:48,883 - PATIENCE: 6
2024-06-16 21:25:48,883 - PRINT_FREQ: 50
2024-06-16 21:25:48,883 - BEST_MODEL_PATH: best_model.pth
2024-06-16 21:25:48,883 - LEARNING_RATE: 0.001
2024-06-16 21:25:48,883 - WEIGHT_DECAY: 0.01
2024-06-16 21:25:58,346 - Epoch: 1  Batch: 50/2198  Train Loss: 1.0168  LR: 1.0e-03  Time: 0:01:46
2024-06-16 21:26:06,640 - Epoch: 1  Batch: 100/2198  Train Loss: 0.8708  LR: 1.0e-03  Time: 0:01:55
2024-06-16 21:26:14,225 - Epoch: 1  Batch: 150/2198  Train Loss: 0.8349  LR: 1.0e-03  Time: 0:02:02
2024-06-16 21:26:21,597 - Epoch: 1  Batch: 200/2198  Train Loss: 0.8082  LR: 1.0e-03  Time: 0:02:10
2024-06-16 21:26:28,819 - Epoch: 1  Batch: 250/2198  Train Loss: 0.7899  LR: 1.0e-03  Time: 0:02:17
2024-06-16 21:26:37,914 - Epoch: 1  Batch: 300/2198  Train Loss: 0.7768  LR: 1.0e-03  Time: 0:02:26
2024-06-16 21:26:47,666 - Epoch: 1  Batch: 350/2198  Train Loss: 0.7646  LR: 1.0e-03  Time: 0:02:36
2024-06-16 21:26:56,266 - Epoch: 1  Batch: 400/2198  Train Loss: 0.7568  LR: 1.0e-03  Time: 0:02:44
2024-06-16 21:27:05,118 - Epoch: 1  Batch: 450/2198  Train Loss: 0.7490  LR: 1.0e-03  Time: 0:02:53
2024-06-16 21:27:20,779 - Epoch: 1  Batch: 500/2198  Train Loss: 0.7434  LR: 1.0e-03  Time: 0:03:09
2024-06-16 21:27:31,752 - Epoch: 1  Batch: 550/2198  Train Loss: 0.7384  LR: 1.0e-03  Time: 0:03:20
2024-06-16 21:27:43,141 - Epoch: 1  Batch: 600/2198  Train Loss: 0.7340  LR: 1.0e-03  Time: 0:03:31
2024-06-16 21:27:51,901 - Epoch: 1  Batch: 650/2198  Train Loss: 0.7298  LR: 1.0e-03  Time: 0:03:40
2024-06-16 21:28:00,814 - Epoch: 1  Batch: 700/2198  Train Loss: 0.7281  LR: 1.0e-03  Time: 0:03:49
2024-06-16 21:28:08,268 - Epoch: 1  Batch: 750/2198  Train Loss: 0.7238  LR: 1.0e-03  Time: 0:03:56
2024-06-16 21:28:16,831 - Epoch: 1  Batch: 800/2198  Train Loss: 0.7208  LR: 1.0e-03  Time: 0:04:05
2024-06-16 21:28:24,364 - Epoch: 1  Batch: 850/2198  Train Loss: 0.7197  LR: 1.0e-03  Time: 0:04:12
2024-06-16 21:28:33,092 - Epoch: 1  Batch: 900/2198  Train Loss: 0.7157  LR: 1.0e-03  Time: 0:04:21
2024-06-16 21:28:40,734 - Epoch: 1  Batch: 950/2198  Train Loss: 0.7140  LR: 1.0e-03  Time: 0:04:29
2024-06-16 21:28:50,205 - Epoch: 1  Batch: 1000/2198  Train Loss: 0.7136  LR: 1.0e-03  Time: 0:04:38
2024-06-16 21:28:59,264 - Epoch: 1  Batch: 1050/2198  Train Loss: 0.7106  LR: 1.0e-03  Time: 0:04:47
2024-06-16 21:29:09,700 - Epoch: 1  Batch: 1100/2198  Train Loss: 0.7096  LR: 1.0e-03  Time: 0:04:58
2024-06-16 21:29:17,598 - Epoch: 1  Batch: 1150/2198  Train Loss: 0.7079  LR: 1.0e-03  Time: 0:05:06
2024-06-16 21:29:25,949 - Epoch: 1  Batch: 1200/2198  Train Loss: 0.7052  LR: 1.0e-03  Time: 0:05:14
2024-06-16 21:29:34,715 - Epoch: 1  Batch: 1250/2198  Train Loss: 0.7036  LR: 1.0e-03  Time: 0:05:23
2024-06-16 21:29:42,047 - Epoch: 1  Batch: 1300/2198  Train Loss: 0.7036  LR: 1.0e-03  Time: 0:05:30
2024-06-16 21:29:50,646 - Epoch: 1  Batch: 1350/2198  Train Loss: 0.7021  LR: 1.0e-03  Time: 0:05:39
2024-06-16 21:29:57,983 - Epoch: 1  Batch: 1400/2198  Train Loss: 0.7009  LR: 1.0e-03  Time: 0:05:46
2024-06-16 21:30:07,179 - Epoch: 1  Batch: 1450/2198  Train Loss: 0.6991  LR: 1.0e-03  Time: 0:05:55
2024-06-16 21:30:15,806 - Epoch: 1  Batch: 1500/2198  Train Loss: 0.6971  LR: 1.0e-03  Time: 0:06:04
2024-06-16 21:30:23,807 - Epoch: 1  Batch: 1550/2198  Train Loss: 0.6976  LR: 1.0e-03  Time: 0:06:12
2024-06-16 21:30:32,574 - Epoch: 1  Batch: 1600/2198  Train Loss: 0.6978  LR: 1.0e-03  Time: 0:06:21
2024-06-16 21:30:40,380 - Epoch: 1  Batch: 1650/2198  Train Loss: 0.6946  LR: 1.0e-03  Time: 0:06:28
2024-06-16 21:30:49,857 - Epoch: 1  Batch: 1700/2198  Train Loss: 0.6943  LR: 1.0e-03  Time: 0:06:38
2024-06-16 21:30:57,961 - Epoch: 1  Batch: 1750/2198  Train Loss: 0.6929  LR: 1.0e-03  Time: 0:06:46
2024-06-16 21:31:06,412 - Epoch: 1  Batch: 1800/2198  Train Loss: 0.6922  LR: 1.0e-03  Time: 0:06:54
2024-06-16 21:31:15,524 - Epoch: 1  Batch: 1850/2198  Train Loss: 0.6887  LR: 1.0e-03  Time: 0:07:04
2024-06-16 21:31:24,264 - Epoch: 1  Batch: 1900/2198  Train Loss: 0.6883  LR: 1.0e-03  Time: 0:07:12
2024-06-16 21:31:32,208 - Epoch: 1  Batch: 1950/2198  Train Loss: 0.6886  LR: 1.0e-03  Time: 0:07:20
2024-06-16 21:31:40,775 - Epoch: 1  Batch: 2000/2198  Train Loss: 0.6879  LR: 1.0e-03  Time: 0:07:29
2024-06-16 21:31:49,078 - Epoch: 1  Batch: 2050/2198  Train Loss: 0.6856  LR: 1.0e-03  Time: 0:07:37
2024-06-16 21:31:57,193 - Epoch: 1  Batch: 2100/2198  Train Loss: 0.6858  LR: 1.0e-03  Time: 0:07:45
2024-06-16 21:32:04,971 - Epoch: 1  Batch: 2150/2198  Train Loss: 0.6851  LR: 1.0e-03  Time: 0:07:53
2024-06-16 21:32:40,190 - 
Epoch: 1  Val Loss: 0.6697  R2 score: -12.0860
2024-06-16 21:32:40,190 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-16 21:32:49,298 - Epoch: 2  Batch: 50/2198  Train Loss: 0.6842  LR: 1.0e-03  Time: 0:08:37
2024-06-16 21:32:56,792 - Epoch: 2  Batch: 100/2198  Train Loss: 0.6805  LR: 1.0e-03  Time: 0:08:45
2024-06-16 21:33:05,114 - Epoch: 2  Batch: 150/2198  Train Loss: 0.6828  LR: 1.0e-03  Time: 0:08:53
2024-06-16 21:33:12,991 - Epoch: 2  Batch: 200/2198  Train Loss: 0.6807  LR: 1.0e-03  Time: 0:09:01
2024-06-16 21:33:21,194 - Epoch: 2  Batch: 250/2198  Train Loss: 0.6808  LR: 1.0e-03  Time: 0:09:09
2024-06-16 21:33:28,980 - Epoch: 2  Batch: 300/2198  Train Loss: 0.6804  LR: 1.0e-03  Time: 0:09:17
2024-06-16 21:33:36,636 - Epoch: 2  Batch: 350/2198  Train Loss: 0.6798  LR: 1.0e-03  Time: 0:09:25
2024-06-16 21:33:44,839 - Epoch: 2  Batch: 400/2198  Train Loss: 0.6770  LR: 1.0e-03  Time: 0:09:33
2024-06-16 21:33:52,591 - Epoch: 2  Batch: 450/2198  Train Loss: 0.6780  LR: 1.0e-03  Time: 0:09:41
2024-06-16 21:34:01,147 - Epoch: 2  Batch: 500/2198  Train Loss: 0.6783  LR: 1.0e-03  Time: 0:09:49
2024-06-16 21:34:08,672 - Epoch: 2  Batch: 550/2198  Train Loss: 0.6760  LR: 1.0e-03  Time: 0:09:57
2024-06-16 21:34:17,438 - Epoch: 2  Batch: 600/2198  Train Loss: 0.6754  LR: 1.0e-03  Time: 0:10:05
2024-06-16 21:34:24,939 - Epoch: 2  Batch: 650/2198  Train Loss: 0.6758  LR: 1.0e-03  Time: 0:10:13
2024-06-16 21:34:33,230 - Epoch: 2  Batch: 700/2198  Train Loss: 0.6772  LR: 1.0e-03  Time: 0:10:21
2024-06-16 21:34:40,874 - Epoch: 2  Batch: 750/2198  Train Loss: 0.6743  LR: 1.0e-03  Time: 0:10:29
2024-06-16 21:34:48,118 - Epoch: 2  Batch: 800/2198  Train Loss: 0.6729  LR: 1.0e-03  Time: 0:10:36
2024-06-16 21:34:56,422 - Epoch: 2  Batch: 850/2198  Train Loss: 0.6726  LR: 1.0e-03  Time: 0:10:44
2024-06-16 21:35:03,877 - Epoch: 2  Batch: 900/2198  Train Loss: 0.6744  LR: 1.0e-03  Time: 0:10:52
2024-06-16 21:35:11,994 - Epoch: 2  Batch: 950/2198  Train Loss: 0.6724  LR: 1.0e-03  Time: 0:11:00
2024-06-16 21:35:20,276 - Epoch: 2  Batch: 1000/2198  Train Loss: 0.6726  LR: 1.0e-03  Time: 0:11:08
2024-06-16 21:35:27,609 - Epoch: 2  Batch: 1050/2198  Train Loss: 0.6728  LR: 1.0e-03  Time: 0:11:16
2024-06-16 21:35:36,214 - Epoch: 2  Batch: 1100/2198  Train Loss: 0.6702  LR: 1.0e-03  Time: 0:11:24
2024-06-16 21:35:43,591 - Epoch: 2  Batch: 1150/2198  Train Loss: 0.6694  LR: 1.0e-03  Time: 0:11:32
2024-06-16 21:35:52,385 - Epoch: 2  Batch: 1200/2198  Train Loss: 0.6691  LR: 1.0e-03  Time: 0:11:40
2024-06-16 21:35:59,731 - Epoch: 2  Batch: 1250/2198  Train Loss: 0.6707  LR: 1.0e-03  Time: 0:11:48
2024-06-16 21:36:08,656 - Epoch: 2  Batch: 1300/2198  Train Loss: 0.6692  LR: 1.0e-03  Time: 0:11:57
2024-06-16 21:36:16,292 - Epoch: 2  Batch: 1350/2198  Train Loss: 0.6693  LR: 1.0e-03  Time: 0:12:04
2024-06-16 21:36:24,797 - Epoch: 2  Batch: 1400/2198  Train Loss: 0.6675  LR: 1.0e-03  Time: 0:12:13
2024-06-16 21:36:32,057 - Epoch: 2  Batch: 1450/2198  Train Loss: 0.6678  LR: 1.0e-03  Time: 0:12:20
2024-06-16 21:36:39,680 - Epoch: 2  Batch: 1500/2198  Train Loss: 0.6673  LR: 1.0e-03  Time: 0:12:28
2024-06-16 21:36:47,699 - Epoch: 2  Batch: 1550/2198  Train Loss: 0.6666  LR: 1.0e-03  Time: 0:12:36
2024-06-16 21:36:55,300 - Epoch: 2  Batch: 1600/2198  Train Loss: 0.6666  LR: 1.0e-03  Time: 0:12:43
2024-06-16 21:37:03,383 - Epoch: 2  Batch: 1650/2198  Train Loss: 0.6675  LR: 1.0e-03  Time: 0:12:51
2024-06-16 21:37:10,621 - Epoch: 2  Batch: 1700/2198  Train Loss: 0.6646  LR: 1.0e-03  Time: 0:12:59
2024-06-16 21:37:19,216 - Epoch: 2  Batch: 1750/2198  Train Loss: 0.6651  LR: 1.0e-03  Time: 0:13:07
2024-06-16 21:37:26,751 - Epoch: 2  Batch: 1800/2198  Train Loss: 0.6664  LR: 1.0e-03  Time: 0:13:15
2024-06-16 21:37:35,537 - Epoch: 2  Batch: 1850/2198  Train Loss: 0.6660  LR: 1.0e-03  Time: 0:13:24
2024-06-16 21:37:43,833 - Epoch: 2  Batch: 1900/2198  Train Loss: 0.6653  LR: 1.0e-03  Time: 0:13:32
2024-06-16 21:37:51,661 - Epoch: 2  Batch: 1950/2198  Train Loss: 0.6643  LR: 1.0e-03  Time: 0:13:40
2024-06-16 21:38:00,324 - Epoch: 2  Batch: 2000/2198  Train Loss: 0.6639  LR: 1.0e-03  Time: 0:13:48
2024-06-16 21:38:07,604 - Epoch: 2  Batch: 2050/2198  Train Loss: 0.6637  LR: 1.0e-03  Time: 0:13:56
2024-06-16 21:38:16,175 - Epoch: 2  Batch: 2100/2198  Train Loss: 0.6629  LR: 1.0e-03  Time: 0:14:04
2024-06-16 21:38:24,050 - Epoch: 2  Batch: 2150/2198  Train Loss: 0.6627  LR: 1.0e-03  Time: 0:14:12
2024-06-16 21:38:59,941 - 
Epoch: 2  Val Loss: 0.6474  R2 score: 0.2119
2024-06-16 21:38:59,941 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-16 21:39:08,562 - Epoch: 3  Batch: 50/2198  Train Loss: 0.6623  LR: 1.0e-03  Time: 0:14:57
2024-06-16 21:39:16,065 - Epoch: 3  Batch: 100/2198  Train Loss: 0.6606  LR: 1.0e-03  Time: 0:15:04
2024-06-16 21:39:24,581 - Epoch: 3  Batch: 150/2198  Train Loss: 0.6599  LR: 1.0e-03  Time: 0:15:13
2024-06-16 21:39:31,968 - Epoch: 3  Batch: 200/2198  Train Loss: 0.6603  LR: 1.0e-03  Time: 0:15:20
2024-06-16 21:39:40,697 - Epoch: 3  Batch: 250/2198  Train Loss: 0.6605  LR: 1.0e-03  Time: 0:15:29
2024-06-16 21:39:48,049 - Epoch: 3  Batch: 300/2198  Train Loss: 0.6603  LR: 1.0e-03  Time: 0:15:36
2024-06-16 21:39:56,603 - Epoch: 3  Batch: 350/2198  Train Loss: 0.6606  LR: 1.0e-03  Time: 0:15:45
2024-06-16 21:40:04,495 - Epoch: 3  Batch: 400/2198  Train Loss: 0.6604  LR: 1.0e-03  Time: 0:15:53
2024-06-16 21:40:13,024 - Epoch: 3  Batch: 450/2198  Train Loss: 0.6594  LR: 1.0e-03  Time: 0:16:01
2024-06-16 21:40:21,037 - Epoch: 3  Batch: 500/2198  Train Loss: 0.6577  LR: 1.0e-03  Time: 0:16:09
2024-06-16 21:40:28,456 - Epoch: 3  Batch: 550/2198  Train Loss: 0.6566  LR: 1.0e-03  Time: 0:16:16
2024-06-16 21:40:36,990 - Epoch: 3  Batch: 600/2198  Train Loss: 0.6583  LR: 1.0e-03  Time: 0:16:25
2024-06-16 21:40:44,760 - Epoch: 3  Batch: 650/2198  Train Loss: 0.6576  LR: 1.0e-03  Time: 0:16:33
2024-06-16 21:40:52,451 - Epoch: 3  Batch: 700/2198  Train Loss: 0.6581  LR: 1.0e-03  Time: 0:16:40
2024-06-16 21:41:00,977 - Epoch: 3  Batch: 750/2198  Train Loss: 0.6580  LR: 1.0e-03  Time: 0:16:49
2024-06-16 21:41:08,400 - Epoch: 3  Batch: 800/2198  Train Loss: 0.6566  LR: 1.0e-03  Time: 0:16:56
2024-06-16 21:41:17,107 - Epoch: 3  Batch: 850/2198  Train Loss: 0.6567  LR: 1.0e-03  Time: 0:17:05
2024-06-16 21:41:25,090 - Epoch: 3  Batch: 900/2198  Train Loss: 0.6575  LR: 1.0e-03  Time: 0:17:13
2024-06-16 21:41:33,877 - Epoch: 3  Batch: 950/2198  Train Loss: 0.6553  LR: 1.0e-03  Time: 0:17:22
2024-06-16 21:41:41,751 - Epoch: 3  Batch: 1000/2198  Train Loss: 0.6576  LR: 1.0e-03  Time: 0:17:30
2024-06-16 21:41:50,448 - Epoch: 3  Batch: 1050/2198  Train Loss: 0.6550  LR: 1.0e-03  Time: 0:17:38
2024-06-16 21:41:58,197 - Epoch: 3  Batch: 1100/2198  Train Loss: 0.6561  LR: 1.0e-03  Time: 0:17:46
2024-06-16 21:42:07,029 - Epoch: 3  Batch: 1150/2198  Train Loss: 0.6570  LR: 1.0e-03  Time: 0:17:55
2024-06-16 21:42:14,273 - Epoch: 3  Batch: 1200/2198  Train Loss: 0.6562  LR: 1.0e-03  Time: 0:18:02
2024-06-16 21:42:22,916 - Epoch: 3  Batch: 1250/2198  Train Loss: 0.6539  LR: 1.0e-03  Time: 0:18:11
2024-06-16 21:42:30,188 - Epoch: 3  Batch: 1300/2198  Train Loss: 0.6536  LR: 1.0e-03  Time: 0:18:18
2024-06-16 21:42:38,599 - Epoch: 3  Batch: 1350/2198  Train Loss: 0.6544  LR: 1.0e-03  Time: 0:18:27
2024-06-16 21:42:46,181 - Epoch: 3  Batch: 1400/2198  Train Loss: 0.6539  LR: 1.0e-03  Time: 0:18:34
2024-06-16 21:42:53,892 - Epoch: 3  Batch: 1450/2198  Train Loss: 0.6537  LR: 1.0e-03  Time: 0:18:42
2024-06-16 21:43:02,104 - Epoch: 3  Batch: 1500/2198  Train Loss: 0.6556  LR: 1.0e-03  Time: 0:18:50
2024-06-16 21:43:09,745 - Epoch: 3  Batch: 1550/2198  Train Loss: 0.6510  LR: 1.0e-03  Time: 0:18:58
2024-06-16 21:43:18,307 - Epoch: 3  Batch: 1600/2198  Train Loss: 0.6531  LR: 1.0e-03  Time: 0:19:06
2024-06-16 21:43:25,701 - Epoch: 3  Batch: 1650/2198  Train Loss: 0.6532  LR: 1.0e-03  Time: 0:19:14
2024-06-16 21:43:34,203 - Epoch: 3  Batch: 1700/2198  Train Loss: 0.6537  LR: 1.0e-03  Time: 0:19:22
2024-06-16 21:43:42,066 - Epoch: 3  Batch: 1750/2198  Train Loss: 0.6526  LR: 1.0e-03  Time: 0:19:30
2024-06-16 21:43:50,674 - Epoch: 3  Batch: 1800/2198  Train Loss: 0.6519  LR: 1.0e-03  Time: 0:19:39
2024-06-16 21:43:58,189 - Epoch: 3  Batch: 1850/2198  Train Loss: 0.6533  LR: 1.0e-03  Time: 0:19:46
2024-06-16 21:44:06,050 - Epoch: 3  Batch: 1900/2198  Train Loss: 0.6511  LR: 1.0e-03  Time: 0:19:54
2024-06-16 21:44:14,314 - Epoch: 3  Batch: 1950/2198  Train Loss: 0.6532  LR: 1.0e-03  Time: 0:20:02
2024-06-16 21:44:21,657 - Epoch: 3  Batch: 2000/2198  Train Loss: 0.6518  LR: 1.0e-03  Time: 0:20:10
2024-06-16 21:44:29,206 - Epoch: 3  Batch: 2050/2198  Train Loss: 0.6508  LR: 1.0e-03  Time: 0:20:17
2024-06-16 21:44:37,481 - Epoch: 3  Batch: 2100/2198  Train Loss: 0.6506  LR: 1.0e-03  Time: 0:20:25
2024-06-16 21:44:45,000 - Epoch: 3  Batch: 2150/2198  Train Loss: 0.6499  LR: 1.0e-03  Time: 0:20:33
2024-06-16 21:45:21,311 - 
Epoch: 3  Val Loss: 0.6340  R2 score: 0.3851
2024-06-16 21:45:21,312 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-16 21:45:31,399 - Epoch: 4  Batch: 50/2198  Train Loss: 0.6502  LR: 1.0e-03  Time: 0:21:19
2024-06-16 21:45:39,477 - Epoch: 4  Batch: 100/2198  Train Loss: 0.6472  LR: 1.0e-03  Time: 0:21:27
2024-06-16 21:45:47,743 - Epoch: 4  Batch: 150/2198  Train Loss: 0.6480  LR: 1.0e-03  Time: 0:21:36
2024-06-16 21:45:56,320 - Epoch: 4  Batch: 200/2198  Train Loss: 0.6490  LR: 1.0e-03  Time: 0:21:44
2024-06-16 21:46:04,433 - Epoch: 4  Batch: 250/2198  Train Loss: 0.6473  LR: 1.0e-03  Time: 0:21:52
2024-06-16 21:46:13,588 - Epoch: 4  Batch: 300/2198  Train Loss: 0.6482  LR: 1.0e-03  Time: 0:22:02
2024-06-16 21:46:28,953 - Epoch: 4  Batch: 350/2198  Train Loss: 0.6466  LR: 1.0e-03  Time: 0:22:17
2024-06-16 21:46:43,933 - Epoch: 4  Batch: 400/2198  Train Loss: 0.6467  LR: 1.0e-03  Time: 0:22:32
2024-06-16 21:46:52,853 - Epoch: 4  Batch: 450/2198  Train Loss: 0.6480  LR: 1.0e-03  Time: 0:22:41
2024-06-16 21:47:02,512 - Epoch: 4  Batch: 500/2198  Train Loss: 0.6478  LR: 1.0e-03  Time: 0:22:51
2024-06-16 21:47:12,820 - Epoch: 4  Batch: 550/2198  Train Loss: 0.6492  LR: 1.0e-03  Time: 0:23:01
2024-06-16 21:47:22,042 - Epoch: 4  Batch: 600/2198  Train Loss: 0.6486  LR: 1.0e-03  Time: 0:23:10
2024-06-16 21:47:32,493 - Epoch: 4  Batch: 650/2198  Train Loss: 0.6462  LR: 1.0e-03  Time: 0:23:21
2024-06-16 21:47:42,245 - Epoch: 4  Batch: 700/2198  Train Loss: 0.6478  LR: 1.0e-03  Time: 0:23:30
2024-06-16 21:47:50,925 - Epoch: 4  Batch: 750/2198  Train Loss: 0.6477  LR: 1.0e-03  Time: 0:23:39
2024-06-16 21:47:58,892 - Epoch: 4  Batch: 800/2198  Train Loss: 0.6477  LR: 1.0e-03  Time: 0:23:47
2024-06-16 21:48:07,760 - Epoch: 4  Batch: 850/2198  Train Loss: 0.6467  LR: 1.0e-03  Time: 0:23:56
2024-06-16 21:48:16,053 - Epoch: 4  Batch: 900/2198  Train Loss: 0.6459  LR: 1.0e-03  Time: 0:24:04
2024-06-16 21:48:25,855 - Epoch: 4  Batch: 950/2198  Train Loss: 0.6467  LR: 1.0e-03  Time: 0:24:14
2024-06-16 21:48:35,023 - Epoch: 4  Batch: 1000/2198  Train Loss: 0.6479  LR: 1.0e-03  Time: 0:24:23
2024-06-16 21:48:43,254 - Epoch: 4  Batch: 1050/2198  Train Loss: 0.6444  LR: 1.0e-03  Time: 0:24:31
2024-06-16 21:48:51,545 - Epoch: 4  Batch: 1100/2198  Train Loss: 0.6456  LR: 1.0e-03  Time: 0:24:40
2024-06-16 21:49:00,070 - Epoch: 4  Batch: 1150/2198  Train Loss: 0.6448  LR: 1.0e-03  Time: 0:24:48
2024-06-16 21:49:08,208 - Epoch: 4  Batch: 1200/2198  Train Loss: 0.6464  LR: 1.0e-03  Time: 0:24:56
2024-06-16 21:49:16,749 - Epoch: 4  Batch: 1250/2198  Train Loss: 0.6447  LR: 1.0e-03  Time: 0:25:05
2024-06-16 21:49:24,910 - Epoch: 4  Batch: 1300/2198  Train Loss: 0.6455  LR: 1.0e-03  Time: 0:25:13
2024-06-16 21:49:33,733 - Epoch: 4  Batch: 1350/2198  Train Loss: 0.6454  LR: 1.0e-03  Time: 0:25:22
2024-06-16 21:49:41,120 - Epoch: 4  Batch: 1400/2198  Train Loss: 0.6448  LR: 1.0e-03  Time: 0:25:29
2024-06-16 21:49:49,943 - Epoch: 4  Batch: 1450/2198  Train Loss: 0.6449  LR: 1.0e-03  Time: 0:25:38
2024-06-16 21:49:57,802 - Epoch: 4  Batch: 1500/2198  Train Loss: 0.6437  LR: 1.0e-03  Time: 0:25:46
2024-06-16 21:50:06,235 - Epoch: 4  Batch: 1550/2198  Train Loss: 0.6449  LR: 1.0e-03  Time: 0:25:54
2024-06-16 21:50:14,098 - Epoch: 4  Batch: 1600/2198  Train Loss: 0.6455  LR: 1.0e-03  Time: 0:26:02
2024-06-16 21:50:22,507 - Epoch: 4  Batch: 1650/2198  Train Loss: 0.6444  LR: 1.0e-03  Time: 0:26:11
2024-06-16 21:50:31,436 - Epoch: 4  Batch: 1700/2198  Train Loss: 0.6446  LR: 1.0e-03  Time: 0:26:19
2024-06-16 21:50:38,900 - Epoch: 4  Batch: 1750/2198  Train Loss: 0.6433  LR: 1.0e-03  Time: 0:26:27
2024-06-16 21:50:47,430 - Epoch: 4  Batch: 1800/2198  Train Loss: 0.6435  LR: 1.0e-03  Time: 0:26:35
2024-06-16 21:50:55,914 - Epoch: 4  Batch: 1850/2198  Train Loss: 0.6429  LR: 1.0e-03  Time: 0:26:44
2024-06-16 21:51:03,719 - Epoch: 4  Batch: 1900/2198  Train Loss: 0.6444  LR: 1.0e-03  Time: 0:26:52
2024-06-16 21:51:11,541 - Epoch: 4  Batch: 1950/2198  Train Loss: 0.6430  LR: 1.0e-03  Time: 0:27:00
2024-06-16 21:51:20,539 - Epoch: 4  Batch: 2000/2198  Train Loss: 0.6431  LR: 1.0e-03  Time: 0:27:09
2024-06-16 21:51:29,059 - Epoch: 4  Batch: 2050/2198  Train Loss: 0.6423  LR: 1.0e-03  Time: 0:27:17
2024-06-16 21:51:38,963 - Epoch: 4  Batch: 2100/2198  Train Loss: 0.6431  LR: 1.0e-03  Time: 0:27:27
2024-06-16 21:51:49,665 - Epoch: 4  Batch: 2150/2198  Train Loss: 0.6435  LR: 1.0e-03  Time: 0:27:38
2024-06-16 21:52:27,193 - 
Epoch: 4  Val Loss: 0.6244  R2 score: 0.4316
2024-06-16 21:52:27,193 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-16 21:52:36,781 - Epoch: 5  Batch: 50/2198  Train Loss: 0.6403  LR: 1.0e-03  Time: 0:28:25
2024-06-16 21:52:45,321 - Epoch: 5  Batch: 100/2198  Train Loss: 0.6423  LR: 1.0e-03  Time: 0:28:33
2024-06-16 21:52:54,097 - Epoch: 5  Batch: 150/2198  Train Loss: 0.6400  LR: 1.0e-03  Time: 0:28:42
2024-06-16 21:53:01,636 - Epoch: 5  Batch: 200/2198  Train Loss: 0.6402  LR: 1.0e-03  Time: 0:28:50
2024-06-16 21:53:10,306 - Epoch: 5  Batch: 250/2198  Train Loss: 0.6395  LR: 1.0e-03  Time: 0:28:58
2024-06-16 21:53:17,584 - Epoch: 5  Batch: 300/2198  Train Loss: 0.6404  LR: 1.0e-03  Time: 0:29:06
2024-06-16 21:53:25,699 - Epoch: 5  Batch: 350/2198  Train Loss: 0.6422  LR: 1.0e-03  Time: 0:29:14
2024-06-16 21:53:33,450 - Epoch: 5  Batch: 400/2198  Train Loss: 0.6411  LR: 1.0e-03  Time: 0:29:21
2024-06-16 21:53:40,851 - Epoch: 5  Batch: 450/2198  Train Loss: 0.6390  LR: 1.0e-03  Time: 0:29:29
2024-06-16 21:53:49,035 - Epoch: 5  Batch: 500/2198  Train Loss: 0.6406  LR: 1.0e-03  Time: 0:29:37
2024-06-16 21:53:56,314 - Epoch: 5  Batch: 550/2198  Train Loss: 0.6399  LR: 1.0e-03  Time: 0:29:44
2024-06-16 21:54:04,918 - Epoch: 5  Batch: 600/2198  Train Loss: 0.6407  LR: 1.0e-03  Time: 0:29:53
2024-06-16 21:54:12,386 - Epoch: 5  Batch: 650/2198  Train Loss: 0.6397  LR: 1.0e-03  Time: 0:30:00
2024-06-16 21:54:20,883 - Epoch: 5  Batch: 700/2198  Train Loss: 0.6386  LR: 1.0e-03  Time: 0:30:09
2024-06-16 21:54:27,956 - Epoch: 5  Batch: 750/2198  Train Loss: 0.6385  LR: 1.0e-03  Time: 0:30:16
2024-06-16 21:54:36,771 - Epoch: 5  Batch: 800/2198  Train Loss: 0.6390  LR: 1.0e-03  Time: 0:30:25
2024-06-16 21:54:44,124 - Epoch: 5  Batch: 850/2198  Train Loss: 0.6391  LR: 1.0e-03  Time: 0:30:32
2024-06-16 21:54:52,389 - Epoch: 5  Batch: 900/2198  Train Loss: 0.6395  LR: 1.0e-03  Time: 0:30:40
2024-06-16 21:54:59,379 - Epoch: 5  Batch: 950/2198  Train Loss: 0.6380  LR: 1.0e-03  Time: 0:30:47
2024-06-16 21:55:08,016 - Epoch: 5  Batch: 1000/2198  Train Loss: 0.6387  LR: 1.0e-03  Time: 0:30:56
2024-06-16 21:55:15,193 - Epoch: 5  Batch: 1050/2198  Train Loss: 0.6386  LR: 1.0e-03  Time: 0:31:03
2024-06-16 21:55:22,826 - Epoch: 5  Batch: 1100/2198  Train Loss: 0.6391  LR: 1.0e-03  Time: 0:31:11
2024-06-16 21:55:30,962 - Epoch: 5  Batch: 1150/2198  Train Loss: 0.6367  LR: 1.0e-03  Time: 0:31:19
2024-06-16 21:55:38,633 - Epoch: 5  Batch: 1200/2198  Train Loss: 0.6383  LR: 1.0e-03  Time: 0:31:27
2024-06-16 21:55:46,578 - Epoch: 5  Batch: 1250/2198  Train Loss: 0.6393  LR: 1.0e-03  Time: 0:31:35
2024-06-16 21:55:53,564 - Epoch: 5  Batch: 1300/2198  Train Loss: 0.6373  LR: 1.0e-03  Time: 0:31:42
2024-06-16 21:56:01,814 - Epoch: 5  Batch: 1350/2198  Train Loss: 0.6373  LR: 1.0e-03  Time: 0:31:50
2024-06-16 21:56:09,090 - Epoch: 5  Batch: 1400/2198  Train Loss: 0.6381  LR: 1.0e-03  Time: 0:31:57
2024-06-16 21:56:16,372 - Epoch: 5  Batch: 1450/2198  Train Loss: 0.6373  LR: 1.0e-03  Time: 0:32:04
2024-06-16 21:56:24,206 - Epoch: 5  Batch: 1500/2198  Train Loss: 0.6372  LR: 1.0e-03  Time: 0:32:12
2024-06-16 21:56:31,234 - Epoch: 5  Batch: 1550/2198  Train Loss: 0.6392  LR: 1.0e-03  Time: 0:32:19
2024-06-16 21:56:39,163 - Epoch: 5  Batch: 1600/2198  Train Loss: 0.6376  LR: 1.0e-03  Time: 0:32:27
2024-06-16 21:56:46,670 - Epoch: 5  Batch: 1650/2198  Train Loss: 0.6373  LR: 1.0e-03  Time: 0:32:35
2024-06-16 21:56:54,206 - Epoch: 5  Batch: 1700/2198  Train Loss: 0.6359  LR: 1.0e-03  Time: 0:32:42
2024-06-16 21:57:02,290 - Epoch: 5  Batch: 1750/2198  Train Loss: 0.6378  LR: 1.0e-03  Time: 0:32:50
2024-06-16 21:57:09,728 - Epoch: 5  Batch: 1800/2198  Train Loss: 0.6376  LR: 1.0e-03  Time: 0:32:58
2024-06-16 21:57:17,733 - Epoch: 5  Batch: 1850/2198  Train Loss: 0.6370  LR: 1.0e-03  Time: 0:33:06
2024-06-16 21:57:25,308 - Epoch: 5  Batch: 1900/2198  Train Loss: 0.6365  LR: 1.0e-03  Time: 0:33:13
2024-06-16 21:57:33,498 - Epoch: 5  Batch: 1950/2198  Train Loss: 0.6370  LR: 1.0e-03  Time: 0:33:22
2024-06-16 21:57:41,489 - Epoch: 5  Batch: 2000/2198  Train Loss: 0.6369  LR: 1.0e-03  Time: 0:33:29
2024-06-16 21:57:49,311 - Epoch: 5  Batch: 2050/2198  Train Loss: 0.6360  LR: 1.0e-03  Time: 0:33:37
2024-06-16 21:57:57,319 - Epoch: 5  Batch: 2100/2198  Train Loss: 0.6351  LR: 1.0e-03  Time: 0:33:45
2024-06-16 21:58:04,944 - Epoch: 5  Batch: 2150/2198  Train Loss: 0.6364  LR: 1.0e-03  Time: 0:33:53
2024-06-16 21:58:38,228 - 
Epoch: 5  Val Loss: 0.6190  R2 score: 0.0212
2024-06-16 21:58:38,228 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-16 21:58:45,840 - Epoch: 6  Batch: 50/2198  Train Loss: 0.6356  LR: 1.0e-03  Time: 0:34:34
2024-06-16 21:58:54,750 - Epoch: 6  Batch: 100/2198  Train Loss: 0.6346  LR: 1.0e-03  Time: 0:34:43
2024-06-16 21:59:02,243 - Epoch: 6  Batch: 150/2198  Train Loss: 0.6327  LR: 1.0e-03  Time: 0:34:50
2024-06-16 21:59:10,790 - Epoch: 6  Batch: 200/2198  Train Loss: 0.6340  LR: 1.0e-03  Time: 0:34:59
2024-06-16 21:59:18,091 - Epoch: 6  Batch: 250/2198  Train Loss: 0.6340  LR: 1.0e-03  Time: 0:35:06
2024-06-16 21:59:25,249 - Epoch: 6  Batch: 300/2198  Train Loss: 0.6341  LR: 1.0e-03  Time: 0:35:13
2024-06-16 21:59:35,644 - Epoch: 6  Batch: 350/2198  Train Loss: 0.6341  LR: 1.0e-03  Time: 0:35:24
2024-06-16 21:59:42,968 - Epoch: 6  Batch: 400/2198  Train Loss: 0.6350  LR: 1.0e-03  Time: 0:35:31
2024-06-16 21:59:50,850 - Epoch: 6  Batch: 450/2198  Train Loss: 0.6348  LR: 1.0e-03  Time: 0:35:39
2024-06-16 21:59:59,025 - Epoch: 6  Batch: 500/2198  Train Loss: 0.6337  LR: 1.0e-03  Time: 0:35:47
2024-06-16 22:00:06,708 - Epoch: 6  Batch: 550/2198  Train Loss: 0.6339  LR: 1.0e-03  Time: 0:35:55
2024-06-16 22:00:15,077 - Epoch: 6  Batch: 600/2198  Train Loss: 0.6328  LR: 1.0e-03  Time: 0:36:03
2024-06-16 22:00:22,401 - Epoch: 6  Batch: 650/2198  Train Loss: 0.6316  LR: 1.0e-03  Time: 0:36:10
2024-06-16 22:00:30,122 - Epoch: 6  Batch: 700/2198  Train Loss: 0.6343  LR: 1.0e-03  Time: 0:36:18
2024-06-16 22:00:37,986 - Epoch: 6  Batch: 750/2198  Train Loss: 0.6339  LR: 1.0e-03  Time: 0:36:26
2024-06-16 22:00:45,174 - Epoch: 6  Batch: 800/2198  Train Loss: 0.6336  LR: 1.0e-03  Time: 0:36:33
2024-06-16 22:00:52,806 - Epoch: 6  Batch: 850/2198  Train Loss: 0.6330  LR: 1.0e-03  Time: 0:36:41
2024-06-16 22:01:00,933 - Epoch: 6  Batch: 900/2198  Train Loss: 0.6342  LR: 1.0e-03  Time: 0:36:49
2024-06-16 22:01:07,997 - Epoch: 6  Batch: 950/2198  Train Loss: 0.6330  LR: 1.0e-03  Time: 0:36:56
2024-06-16 22:01:15,696 - Epoch: 6  Batch: 1000/2198  Train Loss: 0.6327  LR: 1.0e-03  Time: 0:37:04
2024-06-16 22:01:23,602 - Epoch: 6  Batch: 1050/2198  Train Loss: 0.6317  LR: 1.0e-03  Time: 0:37:12
2024-06-16 22:01:30,661 - Epoch: 6  Batch: 1100/2198  Train Loss: 0.6321  LR: 1.0e-03  Time: 0:37:19
2024-06-16 22:01:38,778 - Epoch: 6  Batch: 1150/2198  Train Loss: 0.6318  LR: 1.0e-03  Time: 0:37:27
2024-06-16 22:01:46,328 - Epoch: 6  Batch: 1200/2198  Train Loss: 0.6319  LR: 1.0e-03  Time: 0:37:34
2024-06-16 22:01:53,755 - Epoch: 6  Batch: 1250/2198  Train Loss: 0.6340  LR: 1.0e-03  Time: 0:37:42
2024-06-16 22:02:02,112 - Epoch: 6  Batch: 1300/2198  Train Loss: 0.6312  LR: 1.0e-03  Time: 0:37:50
2024-06-16 22:02:09,246 - Epoch: 6  Batch: 1350/2198  Train Loss: 0.6320  LR: 1.0e-03  Time: 0:37:57
2024-06-16 22:02:17,060 - Epoch: 6  Batch: 1400/2198  Train Loss: 0.6328  LR: 1.0e-03  Time: 0:38:05
2024-06-16 22:02:25,049 - Epoch: 6  Batch: 1450/2198  Train Loss: 0.6323  LR: 1.0e-03  Time: 0:38:13
2024-06-16 22:02:32,125 - Epoch: 6  Batch: 1500/2198  Train Loss: 0.6332  LR: 1.0e-03  Time: 0:38:20
2024-06-16 22:02:39,825 - Epoch: 6  Batch: 1550/2198  Train Loss: 0.6315  LR: 1.0e-03  Time: 0:38:28
2024-06-16 22:02:47,933 - Epoch: 6  Batch: 1600/2198  Train Loss: 0.6319  LR: 1.0e-03  Time: 0:38:36
2024-06-16 22:02:55,001 - Epoch: 6  Batch: 1650/2198  Train Loss: 0.6319  LR: 1.0e-03  Time: 0:38:43
2024-06-16 22:03:03,101 - Epoch: 6  Batch: 1700/2198  Train Loss: 0.6325  LR: 1.0e-03  Time: 0:38:51
2024-06-16 22:03:10,764 - Epoch: 6  Batch: 1750/2198  Train Loss: 0.6329  LR: 1.0e-03  Time: 0:38:59
2024-06-16 22:03:17,829 - Epoch: 6  Batch: 1800/2198  Train Loss: 0.6312  LR: 1.0e-03  Time: 0:39:06
2024-06-16 22:03:26,874 - Epoch: 6  Batch: 1850/2198  Train Loss: 0.6306  LR: 1.0e-03  Time: 0:39:15
2024-06-16 22:03:34,598 - Epoch: 6  Batch: 1900/2198  Train Loss: 0.6317  LR: 1.0e-03  Time: 0:39:23
2024-06-16 22:03:43,460 - Epoch: 6  Batch: 1950/2198  Train Loss: 0.6319  LR: 1.0e-03  Time: 0:39:31
2024-06-16 22:03:50,769 - Epoch: 6  Batch: 2000/2198  Train Loss: 0.6321  LR: 1.0e-03  Time: 0:39:39
2024-06-16 22:03:59,310 - Epoch: 6  Batch: 2050/2198  Train Loss: 0.6307  LR: 1.0e-03  Time: 0:39:47
2024-06-16 22:04:06,780 - Epoch: 6  Batch: 2100/2198  Train Loss: 0.6303  LR: 1.0e-03  Time: 0:39:55
2024-06-16 22:04:14,000 - Epoch: 6  Batch: 2150/2198  Train Loss: 0.6318  LR: 1.0e-03  Time: 0:40:02
2024-06-16 22:04:50,138 - 
Epoch: 6  Val Loss: 0.6116  R2 score: 0.4093
2024-06-16 22:04:50,138 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-16 22:04:58,538 - Epoch: 7  Batch: 50/2198  Train Loss: 0.6285  LR: 1.0e-03  Time: 0:40:47
2024-06-16 22:05:08,021 - Epoch: 7  Batch: 100/2198  Train Loss: 0.6293  LR: 1.0e-03  Time: 0:40:56
2024-06-16 22:05:16,756 - Epoch: 7  Batch: 150/2198  Train Loss: 0.6279  LR: 1.0e-03  Time: 0:41:05
2024-06-16 22:05:31,521 - Epoch: 7  Batch: 200/2198  Train Loss: 0.6290  LR: 1.0e-03  Time: 0:41:20
2024-06-16 22:05:39,810 - Epoch: 7  Batch: 250/2198  Train Loss: 0.6286  LR: 1.0e-03  Time: 0:41:28
2024-06-16 22:05:48,152 - Epoch: 7  Batch: 300/2198  Train Loss: 0.6288  LR: 1.0e-03  Time: 0:41:36
2024-06-16 22:06:02,181 - Epoch: 7  Batch: 350/2198  Train Loss: 0.6284  LR: 1.0e-03  Time: 0:41:50
2024-06-16 22:06:10,320 - Epoch: 7  Batch: 400/2198  Train Loss: 0.6309  LR: 1.0e-03  Time: 0:41:58
2024-06-16 22:06:17,891 - Epoch: 7  Batch: 450/2198  Train Loss: 0.6302  LR: 1.0e-03  Time: 0:42:06
2024-06-16 22:06:25,218 - Epoch: 7  Batch: 500/2198  Train Loss: 0.6283  LR: 1.0e-03  Time: 0:42:13
2024-06-16 22:06:33,579 - Epoch: 7  Batch: 550/2198  Train Loss: 0.6302  LR: 1.0e-03  Time: 0:42:22
2024-06-16 22:06:41,016 - Epoch: 7  Batch: 600/2198  Train Loss: 0.6287  LR: 1.0e-03  Time: 0:42:29
2024-06-16 22:06:48,927 - Epoch: 7  Batch: 650/2198  Train Loss: 0.6296  LR: 1.0e-03  Time: 0:42:37
2024-06-16 22:06:56,809 - Epoch: 7  Batch: 700/2198  Train Loss: 0.6286  LR: 1.0e-03  Time: 0:42:45
2024-06-16 22:07:03,966 - Epoch: 7  Batch: 750/2198  Train Loss: 0.6301  LR: 1.0e-03  Time: 0:42:52
2024-06-16 22:07:12,434 - Epoch: 7  Batch: 800/2198  Train Loss: 0.6273  LR: 1.0e-03  Time: 0:43:00
2024-06-16 22:07:19,517 - Epoch: 7  Batch: 850/2198  Train Loss: 0.6282  LR: 1.0e-03  Time: 0:43:08
2024-06-16 22:07:27,488 - Epoch: 7  Batch: 900/2198  Train Loss: 0.6291  LR: 1.0e-03  Time: 0:43:15
2024-06-16 22:07:35,420 - Epoch: 7  Batch: 950/2198  Train Loss: 0.6284  LR: 1.0e-03  Time: 0:43:23
2024-06-16 22:07:42,772 - Epoch: 7  Batch: 1000/2198  Train Loss: 0.6292  LR: 1.0e-03  Time: 0:43:31
2024-06-16 22:07:51,338 - Epoch: 7  Batch: 1050/2198  Train Loss: 0.6267  LR: 1.0e-03  Time: 0:43:39
2024-06-16 22:07:58,899 - Epoch: 7  Batch: 1100/2198  Train Loss: 0.6287  LR: 1.0e-03  Time: 0:43:47
2024-06-16 22:08:07,550 - Epoch: 7  Batch: 1150/2198  Train Loss: 0.6286  LR: 1.0e-03  Time: 0:43:56
2024-06-16 22:08:14,925 - Epoch: 7  Batch: 1200/2198  Train Loss: 0.6277  LR: 1.0e-03  Time: 0:44:03
2024-06-16 22:08:23,578 - Epoch: 7  Batch: 1250/2198  Train Loss: 0.6286  LR: 1.0e-03  Time: 0:44:12
2024-06-16 22:08:31,580 - Epoch: 7  Batch: 1300/2198  Train Loss: 0.6294  LR: 1.0e-03  Time: 0:44:20
2024-06-16 22:08:40,220 - Epoch: 7  Batch: 1350/2198  Train Loss: 0.6284  LR: 1.0e-03  Time: 0:44:28
2024-06-16 22:08:48,359 - Epoch: 7  Batch: 1400/2198  Train Loss: 0.6280  LR: 1.0e-03  Time: 0:44:36
2024-06-16 22:08:56,043 - Epoch: 7  Batch: 1450/2198  Train Loss: 0.6263  LR: 1.0e-03  Time: 0:44:44
2024-06-16 22:09:04,098 - Epoch: 7  Batch: 1500/2198  Train Loss: 0.6283  LR: 1.0e-03  Time: 0:44:52
2024-06-16 22:09:11,747 - Epoch: 7  Batch: 1550/2198  Train Loss: 0.6276  LR: 1.0e-03  Time: 0:45:00
2024-06-16 22:09:19,458 - Epoch: 7  Batch: 1600/2198  Train Loss: 0.6269  LR: 1.0e-03  Time: 0:45:07
2024-06-16 22:09:27,455 - Epoch: 7  Batch: 1650/2198  Train Loss: 0.6274  LR: 1.0e-03  Time: 0:45:15
2024-06-16 22:09:34,472 - Epoch: 7  Batch: 1700/2198  Train Loss: 0.6274  LR: 1.0e-03  Time: 0:45:22
2024-06-16 22:09:42,565 - Epoch: 7  Batch: 1750/2198  Train Loss: 0.6276  LR: 1.0e-03  Time: 0:45:31
2024-06-16 22:09:50,224 - Epoch: 7  Batch: 1800/2198  Train Loss: 0.6276  LR: 1.0e-03  Time: 0:45:38
2024-06-16 22:10:09,851 - Epoch: 7  Batch: 1850/2198  Train Loss: 0.6275  LR: 1.0e-03  Time: 0:45:58
2024-06-16 22:10:18,738 - Epoch: 7  Batch: 1900/2198  Train Loss: 0.6279  LR: 1.0e-03  Time: 0:46:07
2024-06-16 22:10:26,450 - Epoch: 7  Batch: 1950/2198  Train Loss: 0.6277  LR: 1.0e-03  Time: 0:46:14
2024-06-16 22:10:34,355 - Epoch: 7  Batch: 2000/2198  Train Loss: 0.6274  LR: 1.0e-03  Time: 0:46:22
2024-06-16 22:10:41,625 - Epoch: 7  Batch: 2050/2198  Train Loss: 0.6274  LR: 1.0e-03  Time: 0:46:30
2024-06-16 22:10:50,657 - Epoch: 7  Batch: 2100/2198  Train Loss: 0.6277  LR: 1.0e-03  Time: 0:46:39
2024-06-16 22:10:58,096 - Epoch: 7  Batch: 2150/2198  Train Loss: 0.6267  LR: 1.0e-03  Time: 0:46:46
2024-06-16 22:11:36,562 - 
Epoch: 7  Val Loss: 0.6085  R2 score: -1.1101
2024-06-16 22:11:36,562 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-16 22:11:45,427 - Epoch: 8  Batch: 50/2198  Train Loss: 0.6255  LR: 1.0e-03  Time: 0:47:33
2024-06-16 22:11:53,202 - Epoch: 8  Batch: 100/2198  Train Loss: 0.6243  LR: 1.0e-03  Time: 0:47:41
2024-06-16 22:12:02,407 - Epoch: 8  Batch: 150/2198  Train Loss: 0.6242  LR: 1.0e-03  Time: 0:47:50
2024-06-16 22:12:10,184 - Epoch: 8  Batch: 200/2198  Train Loss: 0.6257  LR: 1.0e-03  Time: 0:47:58
2024-06-16 22:12:18,646 - Epoch: 8  Batch: 250/2198  Train Loss: 0.6254  LR: 1.0e-03  Time: 0:48:07
2024-06-16 22:12:26,657 - Epoch: 8  Batch: 300/2198  Train Loss: 0.6258  LR: 1.0e-03  Time: 0:48:15
2024-06-16 22:12:34,831 - Epoch: 8  Batch: 350/2198  Train Loss: 0.6257  LR: 1.0e-03  Time: 0:48:23
2024-06-16 22:12:42,417 - Epoch: 8  Batch: 400/2198  Train Loss: 0.6261  LR: 1.0e-03  Time: 0:48:30
2024-06-16 22:12:50,992 - Epoch: 8  Batch: 450/2198  Train Loss: 0.6252  LR: 1.0e-03  Time: 0:48:39
2024-06-16 22:12:58,255 - Epoch: 8  Batch: 500/2198  Train Loss: 0.6251  LR: 1.0e-03  Time: 0:48:46
2024-06-16 22:13:07,115 - Epoch: 8  Batch: 550/2198  Train Loss: 0.6247  LR: 1.0e-03  Time: 0:48:55
2024-06-16 22:13:15,137 - Epoch: 8  Batch: 600/2198  Train Loss: 0.6238  LR: 1.0e-03  Time: 0:49:03
2024-06-16 22:13:23,201 - Epoch: 8  Batch: 650/2198  Train Loss: 0.6257  LR: 1.0e-03  Time: 0:49:11
2024-06-16 22:13:31,335 - Epoch: 8  Batch: 700/2198  Train Loss: 0.6251  LR: 1.0e-03  Time: 0:49:19
2024-06-16 22:13:39,680 - Epoch: 8  Batch: 750/2198  Train Loss: 0.6247  LR: 1.0e-03  Time: 0:49:28
2024-06-16 22:13:47,670 - Epoch: 8  Batch: 800/2198  Train Loss: 0.6260  LR: 1.0e-03  Time: 0:49:36
2024-06-16 22:13:55,548 - Epoch: 8  Batch: 850/2198  Train Loss: 0.6250  LR: 1.0e-03  Time: 0:49:44
2024-06-16 22:14:02,847 - Epoch: 8  Batch: 900/2198  Train Loss: 0.6243  LR: 1.0e-03  Time: 0:49:51
2024-06-16 22:14:12,068 - Epoch: 8  Batch: 950/2198  Train Loss: 0.6239  LR: 1.0e-03  Time: 0:50:00
2024-06-16 22:14:20,942 - Epoch: 8  Batch: 1000/2198  Train Loss: 0.6242  LR: 1.0e-03  Time: 0:50:09
2024-06-16 22:14:29,502 - Epoch: 8  Batch: 1050/2198  Train Loss: 0.6254  LR: 1.0e-03  Time: 0:50:18
2024-06-16 22:15:01,002 - Epoch: 8  Batch: 1100/2198  Train Loss: 0.6248  LR: 1.0e-03  Time: 0:50:49
2024-06-16 22:15:11,138 - Epoch: 8  Batch: 1150/2198  Train Loss: 0.6244  LR: 1.0e-03  Time: 0:50:59
2024-06-16 22:15:21,709 - Epoch: 8  Batch: 1200/2198  Train Loss: 0.6238  LR: 1.0e-03  Time: 0:51:10
2024-06-16 22:15:32,797 - Epoch: 8  Batch: 1250/2198  Train Loss: 0.6250  LR: 1.0e-03  Time: 0:51:21
2024-06-16 22:15:41,781 - Epoch: 8  Batch: 1300/2198  Train Loss: 0.6260  LR: 1.0e-03  Time: 0:51:30
2024-06-16 22:15:50,232 - Epoch: 8  Batch: 1350/2198  Train Loss: 0.6242  LR: 1.0e-03  Time: 0:51:38
2024-06-16 22:15:59,096 - Epoch: 8  Batch: 1400/2198  Train Loss: 0.6231  LR: 1.0e-03  Time: 0:51:47
2024-06-16 22:16:08,015 - Epoch: 8  Batch: 1450/2198  Train Loss: 0.6247  LR: 1.0e-03  Time: 0:51:56
2024-06-16 22:16:17,093 - Epoch: 8  Batch: 1500/2198  Train Loss: 0.6245  LR: 1.0e-03  Time: 0:52:05
2024-06-16 22:16:24,779 - Epoch: 8  Batch: 1550/2198  Train Loss: 0.6250  LR: 1.0e-03  Time: 0:52:13
2024-06-16 22:16:33,890 - Epoch: 8  Batch: 1600/2198  Train Loss: 0.6228  LR: 1.0e-03  Time: 0:52:22
2024-06-16 22:16:43,132 - Epoch: 8  Batch: 1650/2198  Train Loss: 0.6232  LR: 1.0e-03  Time: 0:52:31
2024-06-16 22:16:52,224 - Epoch: 8  Batch: 1700/2198  Train Loss: 0.6236  LR: 1.0e-03  Time: 0:52:40
2024-06-16 22:17:02,454 - Epoch: 8  Batch: 1750/2198  Train Loss: 0.6232  LR: 1.0e-03  Time: 0:52:50
2024-06-16 22:17:10,828 - Epoch: 8  Batch: 1800/2198  Train Loss: 0.6253  LR: 1.0e-03  Time: 0:52:59
2024-06-16 22:17:20,171 - Epoch: 8  Batch: 1850/2198  Train Loss: 0.6240  LR: 1.0e-03  Time: 0:53:08
2024-06-16 22:17:28,975 - Epoch: 8  Batch: 1900/2198  Train Loss: 0.6232  LR: 1.0e-03  Time: 0:53:17
2024-06-16 22:17:38,812 - Epoch: 8  Batch: 1950/2198  Train Loss: 0.6246  LR: 1.0e-03  Time: 0:53:27
2024-06-16 22:17:47,842 - Epoch: 8  Batch: 2000/2198  Train Loss: 0.6231  LR: 1.0e-03  Time: 0:53:36
2024-06-16 22:17:56,292 - Epoch: 8  Batch: 2050/2198  Train Loss: 0.6236  LR: 1.0e-03  Time: 0:53:44
2024-06-16 22:18:04,956 - Epoch: 8  Batch: 2100/2198  Train Loss: 0.6238  LR: 1.0e-03  Time: 0:53:53
2024-06-16 22:18:14,006 - Epoch: 8  Batch: 2150/2198  Train Loss: 0.6263  LR: 1.0e-03  Time: 0:54:02
2024-06-16 22:19:47,050 - 
Epoch: 8  Val Loss: 0.6043  R2 score: 0.3799
2024-06-16 22:19:47,069 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-16 22:20:32,079 - Epoch: 9  Batch: 50/2198  Train Loss: 0.6230  LR: 1.0e-03  Time: 0:56:20
2024-06-16 22:20:41,976 - Epoch: 9  Batch: 100/2198  Train Loss: 0.6232  LR: 1.0e-03  Time: 0:56:30
2024-06-16 22:20:43,411 - Hyperparameters :
2024-06-16 22:20:43,411 - DATA_PATH: /data01/jhko/LEAP/
2024-06-16 22:20:43,411 - BATCH_SIZE: 1024
2024-06-16 22:20:43,411 - MIN_STD: 1e-8
2024-06-16 22:20:43,411 - SCHEDULER_PATIENCE: 3
2024-06-16 22:20:43,411 - SCHEDULER_FACTOR: 0.316
2024-06-16 22:20:43,411 - EPOCHS: 19
2024-06-16 22:20:43,411 - PATIENCE: 6
2024-06-16 22:20:43,412 - PRINT_FREQ: 50
2024-06-16 22:20:43,412 - BEST_MODEL_PATH: best_model.pth
2024-06-16 22:20:43,412 - LEARNING_RATE: 0.001
2024-06-16 22:20:43,412 - WEIGHT_DECAY: 0.01
2024-06-16 22:20:55,614 - Epoch: 9  Batch: 150/2198  Train Loss: 0.6223  LR: 1.0e-03  Time: 0:56:44
2024-06-16 22:21:03,632 - Epoch: 9  Batch: 200/2198  Train Loss: 0.6214  LR: 1.0e-03  Time: 0:56:52
2024-06-16 22:21:11,770 - Epoch: 9  Batch: 250/2198  Train Loss: 0.6221  LR: 1.0e-03  Time: 0:57:00
2024-06-16 22:21:19,676 - Epoch: 9  Batch: 300/2198  Train Loss: 0.6211  LR: 1.0e-03  Time: 0:57:08
2024-06-16 22:21:27,637 - Epoch: 9  Batch: 350/2198  Train Loss: 0.6216  LR: 1.0e-03  Time: 0:57:16
2024-06-16 22:21:35,593 - Epoch: 9  Batch: 400/2198  Train Loss: 0.6214  LR: 1.0e-03  Time: 0:57:24
2024-06-16 22:21:44,395 - Epoch: 9  Batch: 450/2198  Train Loss: 0.6227  LR: 1.0e-03  Time: 0:57:32
2024-06-16 22:21:53,320 - Epoch: 9  Batch: 500/2198  Train Loss: 0.6223  LR: 1.0e-03  Time: 0:57:41
2024-06-16 22:22:01,537 - Epoch: 9  Batch: 550/2198  Train Loss: 0.6227  LR: 1.0e-03  Time: 0:57:50
2024-06-16 22:22:09,164 - Epoch: 9  Batch: 600/2198  Train Loss: 0.6219  LR: 1.0e-03  Time: 0:57:57
2024-06-16 22:22:17,082 - Epoch: 9  Batch: 650/2198  Train Loss: 0.6214  LR: 1.0e-03  Time: 0:58:05
2024-06-16 22:22:24,996 - Epoch: 9  Batch: 700/2198  Train Loss: 0.6212  LR: 1.0e-03  Time: 0:58:13
2024-06-16 22:22:33,355 - Epoch: 9  Batch: 750/2198  Train Loss: 0.6221  LR: 1.0e-03  Time: 0:58:21
2024-06-16 22:22:41,446 - Epoch: 9  Batch: 800/2198  Train Loss: 0.6200  LR: 1.0e-03  Time: 0:58:29
2024-06-16 22:22:50,023 - Epoch: 9  Batch: 850/2198  Train Loss: 0.6210  LR: 1.0e-03  Time: 0:58:38
2024-06-16 22:22:58,042 - Epoch: 9  Batch: 900/2198  Train Loss: 0.6211  LR: 1.0e-03  Time: 0:58:46
2024-06-16 22:23:09,670 - Epoch: 9  Batch: 950/2198  Train Loss: 0.6214  LR: 1.0e-03  Time: 0:58:58
2024-06-16 22:23:18,688 - Epoch: 9  Batch: 1000/2198  Train Loss: 0.6218  LR: 1.0e-03  Time: 0:59:07
2024-06-16 22:23:28,560 - Epoch: 9  Batch: 1050/2198  Train Loss: 0.6209  LR: 1.0e-03  Time: 0:59:17
2024-06-16 22:23:39,374 - Epoch: 9  Batch: 1100/2198  Train Loss: 0.6221  LR: 1.0e-03  Time: 0:59:27
2024-06-16 22:23:48,691 - Epoch: 9  Batch: 1150/2198  Train Loss: 0.6229  LR: 1.0e-03  Time: 0:59:37
2024-06-16 22:23:58,731 - Epoch: 9  Batch: 1200/2198  Train Loss: 0.6207  LR: 1.0e-03  Time: 0:59:47
2024-06-16 22:24:40,991 - Epoch: 9  Batch: 1250/2198  Train Loss: 0.6211  LR: 1.0e-03  Time: 1:00:29
2024-06-16 22:25:16,155 - Epoch: 9  Batch: 1300/2198  Train Loss: 0.6216  LR: 1.0e-03  Time: 1:01:04
2024-06-16 22:25:56,616 - Epoch: 9  Batch: 1350/2198  Train Loss: 0.6207  LR: 1.0e-03  Time: 1:01:45
2024-06-16 22:26:15,404 - Epoch: 9  Batch: 1400/2198  Train Loss: 0.6225  LR: 1.0e-03  Time: 1:02:03
2024-06-16 22:26:24,010 - Epoch: 9  Batch: 1450/2198  Train Loss: 0.6215  LR: 1.0e-03  Time: 1:02:12
2024-06-16 22:26:32,484 - Epoch: 9  Batch: 1500/2198  Train Loss: 0.6209  LR: 1.0e-03  Time: 1:02:20
2024-06-16 22:26:39,574 - Epoch: 9  Batch: 1550/2198  Train Loss: 0.6211  LR: 1.0e-03  Time: 1:02:28
2024-06-16 22:26:47,703 - Epoch: 9  Batch: 1600/2198  Train Loss: 0.6196  LR: 1.0e-03  Time: 1:02:36
2024-06-16 22:26:55,234 - Epoch: 9  Batch: 1650/2198  Train Loss: 0.6212  LR: 1.0e-03  Time: 1:02:43
2024-06-16 22:27:02,439 - Epoch: 9  Batch: 1700/2198  Train Loss: 0.6208  LR: 1.0e-03  Time: 1:02:50
2024-06-16 22:27:10,854 - Epoch: 9  Batch: 1750/2198  Train Loss: 0.6211  LR: 1.0e-03  Time: 1:02:59
2024-06-16 22:27:18,235 - Epoch: 9  Batch: 1800/2198  Train Loss: 0.6211  LR: 1.0e-03  Time: 1:03:06
2024-06-16 22:27:25,369 - Epoch: 9  Batch: 1850/2198  Train Loss: 0.6219  LR: 1.0e-03  Time: 1:03:13
2024-06-16 22:27:32,965 - Epoch: 9  Batch: 1900/2198  Train Loss: 0.6208  LR: 1.0e-03  Time: 1:03:21
2024-06-16 22:27:41,218 - Epoch: 9  Batch: 1950/2198  Train Loss: 0.6224  LR: 1.0e-03  Time: 1:03:29
2024-06-16 22:27:48,441 - Epoch: 9  Batch: 2000/2198  Train Loss: 0.6198  LR: 1.0e-03  Time: 1:03:36
2024-06-16 22:27:55,574 - Epoch: 9  Batch: 2050/2198  Train Loss: 0.6197  LR: 1.0e-03  Time: 1:03:44
2024-06-16 22:28:03,262 - Epoch: 9  Batch: 2100/2198  Train Loss: 0.6213  LR: 1.0e-03  Time: 1:03:51
2024-06-16 22:28:11,479 - Epoch: 9  Batch: 2150/2198  Train Loss: 0.6208  LR: 1.0e-03  Time: 1:03:59
2024-06-16 22:28:52,205 - 
Epoch: 9  Val Loss: 0.6007  R2 score: -3.2230
2024-06-16 22:28:52,205 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-16 22:29:41,231 - Epoch: 10  Batch: 50/2198  Train Loss: 0.6193  LR: 1.0e-03  Time: 1:05:29
2024-06-16 22:30:19,364 - Epoch: 10  Batch: 100/2198  Train Loss: 0.6183  LR: 1.0e-03  Time: 1:06:07
2024-06-16 22:30:51,924 - Epoch: 10  Batch: 150/2198  Train Loss: 0.6186  LR: 1.0e-03  Time: 1:06:40
2024-06-16 22:30:55,773 - Hyperparameters :
2024-06-16 22:30:55,773 - DATA_PATH: /data01/jhko/LEAP/
2024-06-16 22:30:55,773 - BATCH_SIZE: 1024
2024-06-16 22:30:55,773 - MIN_STD: 1e-8
2024-06-16 22:30:55,773 - SCHEDULER_PATIENCE: 3
2024-06-16 22:30:55,774 - SCHEDULER_FACTOR: 0.316
2024-06-16 22:30:55,774 - EPOCHS: 19
2024-06-16 22:30:55,774 - PATIENCE: 6
2024-06-16 22:30:55,774 - PRINT_FREQ: 50
2024-06-16 22:30:55,774 - BEST_MODEL_PATH: best_model.pth
2024-06-16 22:30:55,774 - LEARNING_RATE: 0.001
2024-06-16 22:30:55,774 - WEIGHT_DECAY: 0.01
2024-06-16 22:31:03,237 - Epoch: 10  Batch: 200/2198  Train Loss: 0.6205  LR: 1.0e-03  Time: 1:06:51
2024-06-16 22:31:11,347 - Epoch: 10  Batch: 250/2198  Train Loss: 0.6191  LR: 1.0e-03  Time: 1:06:59
2024-06-16 22:31:19,953 - Epoch: 10  Batch: 300/2198  Train Loss: 0.6201  LR: 1.0e-03  Time: 1:07:08
2024-06-16 22:31:27,525 - Epoch: 10  Batch: 350/2198  Train Loss: 0.6184  LR: 1.0e-03  Time: 1:07:16
2024-06-16 22:31:36,017 - Epoch: 10  Batch: 400/2198  Train Loss: 0.6202  LR: 1.0e-03  Time: 1:07:24
2024-06-16 22:31:43,503 - Epoch: 10  Batch: 450/2198  Train Loss: 0.6193  LR: 1.0e-03  Time: 1:07:32
2024-06-16 22:31:51,226 - Epoch: 10  Batch: 500/2198  Train Loss: 0.6180  LR: 1.0e-03  Time: 1:07:39
2024-06-16 22:31:59,389 - Epoch: 10  Batch: 550/2198  Train Loss: 0.6197  LR: 1.0e-03  Time: 1:07:47
2024-06-16 22:32:07,519 - Epoch: 10  Batch: 600/2198  Train Loss: 0.6194  LR: 1.0e-03  Time: 1:07:56
2024-06-16 22:32:15,454 - Epoch: 10  Batch: 650/2198  Train Loss: 0.6186  LR: 1.0e-03  Time: 1:08:03
2024-06-16 22:32:24,608 - Epoch: 10  Batch: 700/2198  Train Loss: 0.6193  LR: 1.0e-03  Time: 1:08:13
2024-06-16 22:32:32,771 - Epoch: 10  Batch: 750/2198  Train Loss: 0.6172  LR: 1.0e-03  Time: 1:08:21
2024-06-16 22:32:41,655 - Epoch: 10  Batch: 800/2198  Train Loss: 0.6200  LR: 1.0e-03  Time: 1:08:30
2024-06-16 22:32:50,673 - Epoch: 10  Batch: 850/2198  Train Loss: 0.6189  LR: 1.0e-03  Time: 1:08:39
2024-06-16 22:32:58,148 - Epoch: 10  Batch: 900/2198  Train Loss: 0.6179  LR: 1.0e-03  Time: 1:08:46
2024-06-16 22:33:06,842 - Epoch: 10  Batch: 950/2198  Train Loss: 0.6197  LR: 1.0e-03  Time: 1:08:55
2024-06-16 22:33:14,159 - Epoch: 10  Batch: 1000/2198  Train Loss: 0.6172  LR: 1.0e-03  Time: 1:09:02
2024-06-16 22:33:22,903 - Epoch: 10  Batch: 1050/2198  Train Loss: 0.6189  LR: 1.0e-03  Time: 1:09:11
2024-06-16 22:33:30,654 - Epoch: 10  Batch: 1100/2198  Train Loss: 0.6189  LR: 1.0e-03  Time: 1:09:19
2024-06-16 22:33:39,491 - Epoch: 10  Batch: 1150/2198  Train Loss: 0.6191  LR: 1.0e-03  Time: 1:09:27
2024-06-16 22:33:47,828 - Epoch: 10  Batch: 1200/2198  Train Loss: 0.6192  LR: 1.0e-03  Time: 1:09:36
2024-06-16 22:33:55,737 - Epoch: 10  Batch: 1250/2198  Train Loss: 0.6188  LR: 1.0e-03  Time: 1:09:44
2024-06-16 22:34:04,141 - Epoch: 10  Batch: 1300/2198  Train Loss: 0.6190  LR: 1.0e-03  Time: 1:09:52
2024-06-16 22:34:11,926 - Epoch: 10  Batch: 1350/2198  Train Loss: 0.6202  LR: 1.0e-03  Time: 1:10:00
2024-06-16 22:34:19,791 - Epoch: 10  Batch: 1400/2198  Train Loss: 0.6192  LR: 1.0e-03  Time: 1:10:08
2024-06-16 22:34:28,209 - Epoch: 10  Batch: 1450/2198  Train Loss: 0.6202  LR: 1.0e-03  Time: 1:10:16
2024-06-16 22:34:38,845 - Epoch: 10  Batch: 1500/2198  Train Loss: 0.6198  LR: 1.0e-03  Time: 1:10:27
2024-06-16 22:34:46,403 - Epoch: 10  Batch: 1550/2198  Train Loss: 0.6183  LR: 1.0e-03  Time: 1:10:34
2024-06-16 22:34:55,345 - Epoch: 10  Batch: 1600/2198  Train Loss: 0.6182  LR: 1.0e-03  Time: 1:10:43
2024-06-16 22:35:02,686 - Epoch: 10  Batch: 1650/2198  Train Loss: 0.6192  LR: 1.0e-03  Time: 1:10:51
2024-06-16 22:35:10,523 - Epoch: 10  Batch: 1700/2198  Train Loss: 0.6187  LR: 1.0e-03  Time: 1:10:59
2024-06-16 22:35:18,545 - Epoch: 10  Batch: 1750/2198  Train Loss: 0.6165  LR: 1.0e-03  Time: 1:11:07
2024-06-16 22:35:25,759 - Epoch: 10  Batch: 1800/2198  Train Loss: 0.6180  LR: 1.0e-03  Time: 1:11:14
2024-06-16 22:35:34,200 - Epoch: 10  Batch: 1850/2198  Train Loss: 0.6191  LR: 1.0e-03  Time: 1:11:22
2024-06-16 22:35:42,816 - Epoch: 10  Batch: 1900/2198  Train Loss: 0.6185  LR: 1.0e-03  Time: 1:11:31
2024-06-16 22:35:55,426 - Epoch: 10  Batch: 1950/2198  Train Loss: 0.6172  LR: 1.0e-03  Time: 1:11:43
2024-06-16 22:36:06,923 - Epoch: 10  Batch: 2000/2198  Train Loss: 0.6186  LR: 1.0e-03  Time: 1:11:55
2024-06-16 22:36:16,200 - Epoch: 10  Batch: 2050/2198  Train Loss: 0.6178  LR: 1.0e-03  Time: 1:12:04
2024-06-16 22:36:23,911 - Epoch: 10  Batch: 2100/2198  Train Loss: 0.6178  LR: 1.0e-03  Time: 1:12:12
2024-06-16 22:36:33,001 - Epoch: 10  Batch: 2150/2198  Train Loss: 0.6185  LR: 1.0e-03  Time: 1:12:21
2024-06-16 22:37:10,824 - 
Epoch: 10  Val Loss: 0.5979  R2 score: -2.2107
2024-06-16 22:37:10,825 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-16 22:37:19,751 - Epoch: 11  Batch: 50/2198  Train Loss: 0.6160  LR: 1.0e-03  Time: 1:13:08
2024-06-16 22:37:58,092 - Epoch: 11  Batch: 100/2198  Train Loss: 0.6166  LR: 1.0e-03  Time: 1:13:46
2024-06-16 22:38:45,093 - Epoch: 11  Batch: 150/2198  Train Loss: 0.6160  LR: 1.0e-03  Time: 1:14:33
2024-06-16 22:39:30,513 - Epoch: 11  Batch: 200/2198  Train Loss: 0.6155  LR: 1.0e-03  Time: 1:15:19
2024-06-16 22:39:35,747 - Hyperparameters :
2024-06-16 22:39:35,747 - DATA_PATH: /data01/jhko/LEAP/
2024-06-16 22:39:35,747 - BATCH_SIZE: 1024
2024-06-16 22:39:35,747 - MIN_STD: 1e-8
2024-06-16 22:39:35,747 - SCHEDULER_PATIENCE: 3
2024-06-16 22:39:35,747 - SCHEDULER_FACTOR: 0.316
2024-06-16 22:39:35,747 - EPOCHS: 19
2024-06-16 22:39:35,747 - PATIENCE: 6
2024-06-16 22:39:35,748 - PRINT_FREQ: 50
2024-06-16 22:39:35,748 - BEST_MODEL_PATH: best_model.pth
2024-06-16 22:39:35,748 - LEARNING_RATE: 0.001
2024-06-16 22:39:35,748 - WEIGHT_DECAY: 0.01
2024-06-16 22:39:43,451 - Epoch: 11  Batch: 250/2198  Train Loss: 0.6161  LR: 1.0e-03  Time: 1:15:31
2024-06-16 22:39:51,091 - Epoch: 11  Batch: 300/2198  Train Loss: 0.6161  LR: 1.0e-03  Time: 1:15:39
2024-06-16 22:39:59,646 - Epoch: 11  Batch: 350/2198  Train Loss: 0.6164  LR: 1.0e-03  Time: 1:15:48
2024-06-16 22:40:06,855 - Epoch: 11  Batch: 400/2198  Train Loss: 0.6163  LR: 1.0e-03  Time: 1:15:55
2024-06-16 22:40:15,347 - Epoch: 11  Batch: 450/2198  Train Loss: 0.6169  LR: 1.0e-03  Time: 1:16:03
2024-06-16 22:40:22,709 - Epoch: 11  Batch: 500/2198  Train Loss: 0.6183  LR: 1.0e-03  Time: 1:16:11
2024-06-16 22:40:30,553 - Epoch: 11  Batch: 550/2198  Train Loss: 0.6151  LR: 1.0e-03  Time: 1:16:19
2024-06-16 22:40:39,506 - Epoch: 11  Batch: 600/2198  Train Loss: 0.6182  LR: 1.0e-03  Time: 1:16:28
2024-06-16 22:40:54,992 - Epoch: 11  Batch: 650/2198  Train Loss: 0.6159  LR: 1.0e-03  Time: 1:16:43
2024-06-16 22:41:05,067 - Epoch: 11  Batch: 700/2198  Train Loss: 0.6158  LR: 1.0e-03  Time: 1:16:53
2024-06-16 22:41:17,394 - Epoch: 11  Batch: 750/2198  Train Loss: 0.6176  LR: 1.0e-03  Time: 1:17:05
2024-06-16 22:42:04,035 - Epoch: 11  Batch: 800/2198  Train Loss: 0.6161  LR: 1.0e-03  Time: 1:17:52
2024-06-16 22:42:50,782 - Epoch: 11  Batch: 850/2198  Train Loss: 0.6179  LR: 1.0e-03  Time: 1:18:39
2024-06-16 22:43:11,466 - Hyperparameters :
2024-06-16 22:43:11,466 - DATA_PATH: /data01/jhko/LEAP/
2024-06-16 22:43:11,466 - BATCH_SIZE: 1024
2024-06-16 22:43:11,466 - MIN_STD: 1e-8
2024-06-16 22:43:11,466 - SCHEDULER_PATIENCE: 3
2024-06-16 22:43:11,466 - SCHEDULER_FACTOR: 0.316
2024-06-16 22:43:11,466 - EPOCHS: 19
2024-06-16 22:43:11,466 - PATIENCE: 6
2024-06-16 22:43:11,466 - PRINT_FREQ: 50
2024-06-16 22:43:11,466 - BEST_MODEL_PATH: best_model.pth
2024-06-16 22:43:11,466 - LEARNING_RATE: 0.001
2024-06-16 22:43:11,467 - WEIGHT_DECAY: 0.01
2024-06-16 22:43:13,651 - Epoch: 11  Batch: 900/2198  Train Loss: 0.6174  LR: 1.0e-03  Time: 1:19:02
2024-06-16 22:43:22,016 - Epoch: 11  Batch: 950/2198  Train Loss: 0.6157  LR: 1.0e-03  Time: 1:19:10
2024-06-16 22:43:30,859 - Epoch: 11  Batch: 1000/2198  Train Loss: 0.6166  LR: 1.0e-03  Time: 1:19:19
2024-06-16 22:43:38,076 - Epoch: 11  Batch: 1050/2198  Train Loss: 0.6178  LR: 1.0e-03  Time: 1:19:26
2024-06-16 22:43:46,582 - Epoch: 11  Batch: 1100/2198  Train Loss: 0.6164  LR: 1.0e-03  Time: 1:19:35
2024-06-16 22:43:53,801 - Epoch: 11  Batch: 1150/2198  Train Loss: 0.6177  LR: 1.0e-03  Time: 1:19:42
2024-06-16 22:44:02,625 - Epoch: 11  Batch: 1200/2198  Train Loss: 0.6173  LR: 1.0e-03  Time: 1:19:51
2024-06-16 22:44:10,383 - Epoch: 11  Batch: 1250/2198  Train Loss: 0.6173  LR: 1.0e-03  Time: 1:19:58
2024-06-16 22:44:18,960 - Epoch: 11  Batch: 1300/2198  Train Loss: 0.6174  LR: 1.0e-03  Time: 1:20:07
2024-06-16 22:44:27,759 - Epoch: 11  Batch: 1350/2198  Train Loss: 0.6162  LR: 1.0e-03  Time: 1:20:16
2024-06-16 22:44:35,579 - Epoch: 11  Batch: 1400/2198  Train Loss: 0.6154  LR: 1.0e-03  Time: 1:20:24
2024-06-16 22:44:43,922 - Epoch: 11  Batch: 1450/2198  Train Loss: 0.6153  LR: 1.0e-03  Time: 1:20:32
2024-06-16 22:44:51,756 - Epoch: 11  Batch: 1500/2198  Train Loss: 0.6170  LR: 1.0e-03  Time: 1:20:40
2024-06-16 22:45:00,040 - Epoch: 11  Batch: 1550/2198  Train Loss: 0.6173  LR: 1.0e-03  Time: 1:20:48
2024-06-16 22:45:09,889 - Epoch: 11  Batch: 1600/2198  Train Loss: 0.6180  LR: 1.0e-03  Time: 1:20:58
2024-06-16 22:45:56,843 - Epoch: 11  Batch: 1650/2198  Train Loss: 0.6163  LR: 1.0e-03  Time: 1:21:45
2024-06-16 22:46:39,992 - Epoch: 11  Batch: 1700/2198  Train Loss: 0.6162  LR: 1.0e-03  Time: 1:22:28
2024-06-16 22:47:05,558 - Hyperparameters :
2024-06-16 22:47:05,558 - DATA_PATH: /data01/jhko/LEAP/
2024-06-16 22:47:05,558 - BATCH_SIZE: 1024
2024-06-16 22:47:05,558 - MIN_STD: 1e-8
2024-06-16 22:47:05,558 - SCHEDULER_PATIENCE: 3
2024-06-16 22:47:05,558 - SCHEDULER_FACTOR: 0.316
2024-06-16 22:47:05,558 - EPOCHS: 19
2024-06-16 22:47:05,558 - PATIENCE: 6
2024-06-16 22:47:05,558 - PRINT_FREQ: 50
2024-06-16 22:47:05,558 - BEST_MODEL_PATH: best_model.pth
2024-06-16 22:47:05,558 - LEARNING_RATE: 0.001
2024-06-16 22:47:05,558 - WEIGHT_DECAY: 0.01
2024-06-16 22:47:08,928 - Epoch: 11  Batch: 1750/2198  Train Loss: 0.6151  LR: 1.0e-03  Time: 1:22:57
2024-06-16 22:47:16,584 - Epoch: 11  Batch: 1800/2198  Train Loss: 0.6172  LR: 1.0e-03  Time: 1:23:05
2024-06-16 22:47:24,865 - Epoch: 11  Batch: 1850/2198  Train Loss: 0.6168  LR: 1.0e-03  Time: 1:23:13
2024-06-16 22:47:32,965 - Epoch: 11  Batch: 1900/2198  Train Loss: 0.6154  LR: 1.0e-03  Time: 1:23:21
2024-06-16 22:47:40,749 - Epoch: 11  Batch: 1950/2198  Train Loss: 0.6151  LR: 1.0e-03  Time: 1:23:29
2024-06-16 22:47:48,202 - Epoch: 11  Batch: 2000/2198  Train Loss: 0.6157  LR: 1.0e-03  Time: 1:23:36
2024-06-16 22:47:56,746 - Epoch: 11  Batch: 2050/2198  Train Loss: 0.6169  LR: 1.0e-03  Time: 1:23:45
2024-06-16 22:48:05,018 - Epoch: 11  Batch: 2100/2198  Train Loss: 0.6168  LR: 1.0e-03  Time: 1:23:53
2024-06-16 22:48:13,155 - Epoch: 11  Batch: 2150/2198  Train Loss: 0.6161  LR: 1.0e-03  Time: 1:24:01
2024-06-16 22:48:49,103 - 
Epoch: 11  Val Loss: 0.5962  R2 score: 0.3118
2024-06-16 22:48:49,104 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-16 22:48:58,278 - Epoch: 12  Batch: 50/2198  Train Loss: 0.6146  LR: 1.0e-03  Time: 1:24:46
2024-06-16 22:49:06,660 - Epoch: 12  Batch: 100/2198  Train Loss: 0.6145  LR: 1.0e-03  Time: 1:24:55
2024-06-16 22:49:14,725 - Epoch: 12  Batch: 150/2198  Train Loss: 0.6143  LR: 1.0e-03  Time: 1:25:03
2024-06-16 22:49:23,403 - Epoch: 12  Batch: 200/2198  Train Loss: 0.6146  LR: 1.0e-03  Time: 1:25:11
2024-06-16 22:49:31,391 - Epoch: 12  Batch: 250/2198  Train Loss: 0.6140  LR: 1.0e-03  Time: 1:25:19
2024-06-16 22:49:40,680 - Epoch: 12  Batch: 300/2198  Train Loss: 0.6157  LR: 1.0e-03  Time: 1:25:29
2024-06-16 22:49:55,073 - Epoch: 12  Batch: 350/2198  Train Loss: 0.6144  LR: 1.0e-03  Time: 1:25:43
2024-06-16 22:50:08,010 - Epoch: 12  Batch: 400/2198  Train Loss: 0.6155  LR: 1.0e-03  Time: 1:25:56
2024-06-16 22:50:16,603 - Epoch: 12  Batch: 450/2198  Train Loss: 0.6150  LR: 1.0e-03  Time: 1:26:05
2024-06-16 22:50:28,107 - Epoch: 12  Batch: 500/2198  Train Loss: 0.6151  LR: 1.0e-03  Time: 1:26:16
2024-06-16 22:50:37,586 - Epoch: 12  Batch: 550/2198  Train Loss: 0.6153  LR: 1.0e-03  Time: 1:26:26
2024-06-16 22:51:21,091 - Epoch: 12  Batch: 600/2198  Train Loss: 0.6141  LR: 1.0e-03  Time: 1:27:09
2024-06-16 22:52:03,309 - Epoch: 12  Batch: 650/2198  Train Loss: 0.6150  LR: 1.0e-03  Time: 1:27:51
2024-06-16 22:52:35,345 - Hyperparameters :
2024-06-16 22:52:35,346 - DATA_PATH: /data01/jhko/LEAP/
2024-06-16 22:52:35,346 - BATCH_SIZE: 1024
2024-06-16 22:52:35,346 - MIN_STD: 1e-8
2024-06-16 22:52:35,346 - SCHEDULER_PATIENCE: 3
2024-06-16 22:52:35,346 - SCHEDULER_FACTOR: 0.316
2024-06-16 22:52:35,346 - EPOCHS: 19
2024-06-16 22:52:35,346 - PATIENCE: 6
2024-06-16 22:52:35,346 - PRINT_FREQ: 50
2024-06-16 22:52:35,346 - BEST_MODEL_PATH: best_model.pth
2024-06-16 22:52:35,346 - LEARNING_RATE: 0.001
2024-06-16 22:52:35,346 - WEIGHT_DECAY: 0.01
2024-06-16 22:52:38,243 - Epoch: 12  Batch: 700/2198  Train Loss: 0.6144  LR: 1.0e-03  Time: 1:28:26
2024-06-16 22:52:46,347 - Epoch: 12  Batch: 750/2198  Train Loss: 0.6153  LR: 1.0e-03  Time: 1:28:34
2024-06-16 22:52:54,925 - Epoch: 12  Batch: 800/2198  Train Loss: 0.6125  LR: 1.0e-03  Time: 1:28:43
2024-06-16 22:53:02,013 - Epoch: 12  Batch: 850/2198  Train Loss: 0.6149  LR: 1.0e-03  Time: 1:28:50
2024-06-16 22:53:10,132 - Epoch: 12  Batch: 900/2198  Train Loss: 0.6153  LR: 1.0e-03  Time: 1:28:58
2024-06-16 22:53:17,958 - Epoch: 12  Batch: 950/2198  Train Loss: 0.6147  LR: 1.0e-03  Time: 1:29:06
2024-06-16 22:53:26,436 - Epoch: 12  Batch: 1000/2198  Train Loss: 0.6154  LR: 1.0e-03  Time: 1:29:14
2024-06-16 22:53:33,820 - Epoch: 12  Batch: 1050/2198  Train Loss: 0.6146  LR: 1.0e-03  Time: 1:29:22
2024-06-16 22:53:40,988 - Epoch: 12  Batch: 1100/2198  Train Loss: 0.6150  LR: 1.0e-03  Time: 1:29:29
2024-06-16 22:53:49,193 - Epoch: 12  Batch: 1150/2198  Train Loss: 0.6146  LR: 1.0e-03  Time: 1:29:37
2024-06-16 22:53:56,514 - Epoch: 12  Batch: 1200/2198  Train Loss: 0.6138  LR: 1.0e-03  Time: 1:29:45
2024-06-16 22:54:03,585 - Epoch: 12  Batch: 1250/2198  Train Loss: 0.6145  LR: 1.0e-03  Time: 1:29:52
2024-06-16 22:54:10,880 - Epoch: 12  Batch: 1300/2198  Train Loss: 0.6132  LR: 1.0e-03  Time: 1:29:59
2024-06-16 22:54:18,625 - Epoch: 12  Batch: 1350/2198  Train Loss: 0.6136  LR: 1.0e-03  Time: 1:30:07
2024-06-16 22:54:26,011 - Epoch: 12  Batch: 1400/2198  Train Loss: 0.6131  LR: 1.0e-03  Time: 1:30:14
2024-06-16 22:54:33,941 - Epoch: 12  Batch: 1450/2198  Train Loss: 0.6146  LR: 1.0e-03  Time: 1:30:22
2024-06-16 22:54:41,881 - Epoch: 12  Batch: 1500/2198  Train Loss: 0.6130  LR: 1.0e-03  Time: 1:30:30
2024-06-16 22:54:49,911 - Epoch: 12  Batch: 1550/2198  Train Loss: 0.6143  LR: 1.0e-03  Time: 1:30:38
2024-06-16 22:54:57,747 - Epoch: 12  Batch: 1600/2198  Train Loss: 0.6139  LR: 1.0e-03  Time: 1:30:46
2024-06-16 22:55:04,882 - Epoch: 12  Batch: 1650/2198  Train Loss: 0.6147  LR: 1.0e-03  Time: 1:30:53
2024-06-16 22:55:12,912 - Epoch: 12  Batch: 1700/2198  Train Loss: 0.6135  LR: 1.0e-03  Time: 1:31:01
2024-06-16 22:55:20,523 - Epoch: 12  Batch: 1750/2198  Train Loss: 0.6154  LR: 1.0e-03  Time: 1:31:09
2024-06-16 22:55:28,322 - Epoch: 12  Batch: 1800/2198  Train Loss: 0.6149  LR: 1.0e-03  Time: 1:31:16
2024-06-16 22:55:38,403 - Epoch: 12  Batch: 1850/2198  Train Loss: 0.6156  LR: 1.0e-03  Time: 1:31:26
2024-06-16 22:55:46,650 - Epoch: 12  Batch: 1900/2198  Train Loss: 0.6138  LR: 1.0e-03  Time: 1:31:35
2024-06-16 22:55:54,550 - Epoch: 12  Batch: 1950/2198  Train Loss: 0.6158  LR: 1.0e-03  Time: 1:31:43
2024-06-16 22:56:01,852 - Epoch: 12  Batch: 2000/2198  Train Loss: 0.6143  LR: 1.0e-03  Time: 1:31:50
2024-06-16 22:56:09,720 - Epoch: 12  Batch: 2050/2198  Train Loss: 0.6150  LR: 1.0e-03  Time: 1:31:58
2024-06-16 22:56:17,398 - Epoch: 12  Batch: 2100/2198  Train Loss: 0.6155  LR: 1.0e-03  Time: 1:32:05
2024-06-16 22:56:24,634 - Epoch: 12  Batch: 2150/2198  Train Loss: 0.6133  LR: 1.0e-03  Time: 1:32:13
2024-06-16 22:56:58,974 - 
Epoch: 12  Val Loss: 0.5935  R2 score: 0.0943
2024-06-16 22:56:58,974 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-16 22:57:06,472 - Epoch: 13  Batch: 50/2198  Train Loss: 0.6115  LR: 1.0e-03  Time: 1:32:54
2024-06-16 22:57:14,483 - Epoch: 13  Batch: 100/2198  Train Loss: 0.6119  LR: 1.0e-03  Time: 1:33:02
2024-06-16 22:57:22,568 - Epoch: 13  Batch: 150/2198  Train Loss: 0.6135  LR: 1.0e-03  Time: 1:33:11
2024-06-16 22:57:29,839 - Epoch: 13  Batch: 200/2198  Train Loss: 0.6135  LR: 1.0e-03  Time: 1:33:18
2024-06-16 22:57:37,441 - Epoch: 13  Batch: 250/2198  Train Loss: 0.6121  LR: 1.0e-03  Time: 1:33:25
2024-06-16 22:57:45,621 - Epoch: 13  Batch: 300/2198  Train Loss: 0.6123  LR: 1.0e-03  Time: 1:33:34
2024-06-16 22:57:52,699 - Epoch: 13  Batch: 350/2198  Train Loss: 0.6139  LR: 1.0e-03  Time: 1:33:41
2024-06-16 22:58:00,248 - Epoch: 13  Batch: 400/2198  Train Loss: 0.6110  LR: 1.0e-03  Time: 1:33:48
2024-06-16 22:58:08,266 - Epoch: 13  Batch: 450/2198  Train Loss: 0.6113  LR: 1.0e-03  Time: 1:33:56
2024-06-16 22:58:15,313 - Epoch: 13  Batch: 500/2198  Train Loss: 0.6143  LR: 1.0e-03  Time: 1:34:03
2024-06-16 22:58:22,990 - Epoch: 13  Batch: 550/2198  Train Loss: 0.6138  LR: 1.0e-03  Time: 1:34:11
2024-06-16 22:58:31,504 - Epoch: 13  Batch: 600/2198  Train Loss: 0.6118  LR: 1.0e-03  Time: 1:34:20
2024-06-16 22:58:38,965 - Epoch: 13  Batch: 650/2198  Train Loss: 0.6125  LR: 1.0e-03  Time: 1:34:27
2024-06-16 22:58:48,025 - Epoch: 13  Batch: 700/2198  Train Loss: 0.6124  LR: 1.0e-03  Time: 1:34:36
2024-06-16 22:58:55,759 - Epoch: 13  Batch: 750/2198  Train Loss: 0.6137  LR: 1.0e-03  Time: 1:34:44
2024-06-16 22:59:04,376 - Epoch: 13  Batch: 800/2198  Train Loss: 0.6134  LR: 1.0e-03  Time: 1:34:52
2024-06-16 22:59:11,534 - Epoch: 13  Batch: 850/2198  Train Loss: 0.6121  LR: 1.0e-03  Time: 1:35:00
2024-06-16 22:59:21,114 - Epoch: 13  Batch: 900/2198  Train Loss: 0.6125  LR: 1.0e-03  Time: 1:35:09
2024-06-16 22:59:29,793 - Epoch: 13  Batch: 950/2198  Train Loss: 0.6125  LR: 1.0e-03  Time: 1:35:18
2024-06-16 22:59:36,925 - Epoch: 13  Batch: 1000/2198  Train Loss: 0.6125  LR: 1.0e-03  Time: 1:35:25
2024-06-16 22:59:45,413 - Epoch: 13  Batch: 1050/2198  Train Loss: 0.6135  LR: 1.0e-03  Time: 1:35:33
2024-06-16 22:59:52,798 - Epoch: 13  Batch: 1100/2198  Train Loss: 0.6140  LR: 1.0e-03  Time: 1:35:41
2024-06-16 23:00:01,929 - Epoch: 13  Batch: 1150/2198  Train Loss: 0.6131  LR: 1.0e-03  Time: 1:35:50
2024-06-16 23:00:41,304 - Epoch: 13  Batch: 1200/2198  Train Loss: 0.6132  LR: 1.0e-03  Time: 1:36:29
2024-06-16 23:01:22,924 - Epoch: 13  Batch: 1250/2198  Train Loss: 0.6127  LR: 1.0e-03  Time: 1:37:11
2024-06-16 23:01:49,538 - Epoch: 13  Batch: 1300/2198  Train Loss: 0.6127  LR: 1.0e-03  Time: 1:37:38
2024-06-16 23:01:50,157 - Hyperparameters :
2024-06-16 23:01:50,157 - DATA_PATH: /data01/jhko/LEAP/
2024-06-16 23:01:50,157 - BATCH_SIZE: 1024
2024-06-16 23:01:50,157 - MIN_STD: 1e-8
2024-06-16 23:01:50,157 - SCHEDULER_PATIENCE: 3
2024-06-16 23:01:50,157 - SCHEDULER_FACTOR: 0.316
2024-06-16 23:01:50,157 - EPOCHS: 19
2024-06-16 23:01:50,157 - PATIENCE: 6
2024-06-16 23:01:50,157 - PRINT_FREQ: 50
2024-06-16 23:01:50,157 - BEST_MODEL_PATH: best_model_seq.pth
2024-06-16 23:01:50,157 - LEARNING_RATE: 0.001
2024-06-16 23:01:50,157 - WEIGHT_DECAY: 0.01
2024-06-16 23:01:59,738 - Epoch: 13  Batch: 1350/2198  Train Loss: 0.6129  LR: 1.0e-03  Time: 1:37:48
2024-06-16 23:02:08,313 - Epoch: 13  Batch: 1400/2198  Train Loss: 0.6128  LR: 1.0e-03  Time: 1:37:56
2024-06-16 23:02:15,728 - Epoch: 13  Batch: 1450/2198  Train Loss: 0.6125  LR: 1.0e-03  Time: 1:38:04
2024-06-16 23:02:24,629 - Epoch: 13  Batch: 1500/2198  Train Loss: 0.6125  LR: 1.0e-03  Time: 1:38:13
2024-06-16 23:02:33,113 - Epoch: 13  Batch: 1550/2198  Train Loss: 0.6119  LR: 1.0e-03  Time: 1:38:21
2024-06-16 23:02:40,351 - Epoch: 13  Batch: 1600/2198  Train Loss: 0.6131  LR: 1.0e-03  Time: 1:38:28
2024-06-16 23:02:47,935 - Epoch: 13  Batch: 1650/2198  Train Loss: 0.6131  LR: 1.0e-03  Time: 1:38:36
2024-06-16 23:02:55,872 - Epoch: 13  Batch: 1700/2198  Train Loss: 0.6125  LR: 1.0e-03  Time: 1:38:44
2024-06-16 23:03:02,984 - Epoch: 13  Batch: 1750/2198  Train Loss: 0.6117  LR: 1.0e-03  Time: 1:38:51
2024-06-16 23:03:10,161 - Epoch: 13  Batch: 1800/2198  Train Loss: 0.6140  LR: 1.0e-03  Time: 1:38:58
2024-06-16 23:03:18,420 - Epoch: 13  Batch: 1850/2198  Train Loss: 0.6124  LR: 1.0e-03  Time: 1:39:06
2024-06-16 23:03:25,738 - Epoch: 13  Batch: 1900/2198  Train Loss: 0.6134  LR: 1.0e-03  Time: 1:39:14
2024-06-16 23:03:33,473 - Epoch: 13  Batch: 1950/2198  Train Loss: 0.6128  LR: 1.0e-03  Time: 1:39:21
2024-06-16 23:03:41,879 - Epoch: 13  Batch: 2000/2198  Train Loss: 0.6126  LR: 1.0e-03  Time: 1:39:30
2024-06-16 23:03:49,414 - Epoch: 13  Batch: 2050/2198  Train Loss: 0.6133  LR: 1.0e-03  Time: 1:39:37
2024-06-16 23:03:57,971 - Epoch: 13  Batch: 2100/2198  Train Loss: 0.6124  LR: 1.0e-03  Time: 1:39:46
2024-06-16 23:04:06,065 - Epoch: 13  Batch: 2150/2198  Train Loss: 0.6132  LR: 1.0e-03  Time: 1:39:54
2024-06-16 23:04:40,630 - 
Epoch: 13  Val Loss: 0.5913  R2 score: -2.3359
2024-06-16 23:04:40,630 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-16 23:04:48,911 - Epoch: 14  Batch: 50/2198  Train Loss: 0.6121  LR: 1.0e-03  Time: 1:40:37
2024-06-16 23:04:56,694 - Epoch: 14  Batch: 100/2198  Train Loss: 0.6095  LR: 1.0e-03  Time: 1:40:45
2024-06-16 23:05:03,862 - Epoch: 14  Batch: 150/2198  Train Loss: 0.6119  LR: 1.0e-03  Time: 1:40:52
2024-06-16 23:05:12,410 - Epoch: 14  Batch: 200/2198  Train Loss: 0.6096  LR: 1.0e-03  Time: 1:41:00
2024-06-16 23:05:19,603 - Epoch: 14  Batch: 250/2198  Train Loss: 0.6110  LR: 1.0e-03  Time: 1:41:08
2024-06-16 23:05:26,627 - Epoch: 14  Batch: 300/2198  Train Loss: 0.6111  LR: 1.0e-03  Time: 1:41:15
2024-06-16 23:05:34,754 - Epoch: 14  Batch: 350/2198  Train Loss: 0.6091  LR: 1.0e-03  Time: 1:41:23
2024-06-16 23:05:42,833 - Epoch: 14  Batch: 400/2198  Train Loss: 0.6104  LR: 1.0e-03  Time: 1:41:31
2024-06-16 23:05:50,268 - Epoch: 14  Batch: 450/2198  Train Loss: 0.6120  LR: 1.0e-03  Time: 1:41:38
2024-06-16 23:05:59,070 - Epoch: 14  Batch: 500/2198  Train Loss: 0.6106  LR: 1.0e-03  Time: 1:41:47
2024-06-16 23:06:06,931 - Epoch: 14  Batch: 550/2198  Train Loss: 0.6099  LR: 1.0e-03  Time: 1:41:55
2024-06-16 23:06:15,261 - Epoch: 14  Batch: 600/2198  Train Loss: 0.6113  LR: 1.0e-03  Time: 1:42:03
2024-06-16 23:06:23,271 - Epoch: 14  Batch: 650/2198  Train Loss: 0.6094  LR: 1.0e-03  Time: 1:42:11
2024-06-16 23:06:31,235 - Epoch: 14  Batch: 700/2198  Train Loss: 0.6129  LR: 1.0e-03  Time: 1:42:19
2024-06-16 23:06:39,099 - Epoch: 14  Batch: 750/2198  Train Loss: 0.6118  LR: 1.0e-03  Time: 1:42:27
2024-06-16 23:06:48,097 - Epoch: 14  Batch: 800/2198  Train Loss: 0.6104  LR: 1.0e-03  Time: 1:42:36
2024-06-16 23:06:57,348 - Epoch: 14  Batch: 850/2198  Train Loss: 0.6120  LR: 1.0e-03  Time: 1:42:45
2024-06-16 23:07:09,466 - Epoch: 14  Batch: 900/2198  Train Loss: 0.6133  LR: 1.0e-03  Time: 1:42:57
2024-06-16 23:07:57,893 - Epoch: 14  Batch: 950/2198  Train Loss: 0.6116  LR: 1.0e-03  Time: 1:43:46
2024-06-16 23:08:43,379 - Epoch: 14  Batch: 1000/2198  Train Loss: 0.6115  LR: 1.0e-03  Time: 1:44:31
2024-06-16 23:09:00,694 - Epoch: 14  Batch: 1050/2198  Train Loss: 0.6109  LR: 1.0e-03  Time: 1:44:49
2024-06-16 23:09:08,054 - Epoch: 14  Batch: 1100/2198  Train Loss: 0.6124  LR: 1.0e-03  Time: 1:44:56
2024-06-16 23:09:16,677 - Epoch: 14  Batch: 1150/2198  Train Loss: 0.6116  LR: 1.0e-03  Time: 1:45:05
2024-06-16 23:09:24,098 - Epoch: 14  Batch: 1200/2198  Train Loss: 0.6120  LR: 1.0e-03  Time: 1:45:12
2024-06-16 23:09:32,619 - Epoch: 14  Batch: 1250/2198  Train Loss: 0.6108  LR: 1.0e-03  Time: 1:45:21
2024-06-16 23:09:39,737 - Epoch: 14  Batch: 1300/2198  Train Loss: 0.6119  LR: 1.0e-03  Time: 1:45:28
2024-06-16 23:09:47,900 - Epoch: 14  Batch: 1350/2198  Train Loss: 0.6095  LR: 1.0e-03  Time: 1:45:36
2024-06-16 23:09:55,582 - Epoch: 14  Batch: 1400/2198  Train Loss: 0.6108  LR: 1.0e-03  Time: 1:45:44
2024-06-16 23:10:02,829 - Epoch: 14  Batch: 1450/2198  Train Loss: 0.6123  LR: 1.0e-03  Time: 1:45:51
2024-06-16 23:10:11,034 - Epoch: 14  Batch: 1500/2198  Train Loss: 0.6100  LR: 1.0e-03  Time: 1:45:59
2024-06-16 23:10:18,611 - Epoch: 14  Batch: 1550/2198  Train Loss: 0.6101  LR: 1.0e-03  Time: 1:46:07
2024-06-16 23:10:25,797 - Epoch: 14  Batch: 1600/2198  Train Loss: 0.6111  LR: 1.0e-03  Time: 1:46:14
2024-06-16 23:10:34,720 - Epoch: 14  Batch: 1650/2198  Train Loss: 0.6107  LR: 1.0e-03  Time: 1:46:23
2024-06-16 23:10:42,224 - Epoch: 14  Batch: 1700/2198  Train Loss: 0.6104  LR: 1.0e-03  Time: 1:46:30
2024-06-16 23:10:49,729 - Epoch: 14  Batch: 1750/2198  Train Loss: 0.6112  LR: 1.0e-03  Time: 1:46:38
2024-06-16 23:10:57,627 - Epoch: 14  Batch: 1800/2198  Train Loss: 0.6118  LR: 1.0e-03  Time: 1:46:46
2024-06-16 23:11:05,302 - Epoch: 14  Batch: 1850/2198  Train Loss: 0.6105  LR: 1.0e-03  Time: 1:46:53
2024-06-16 23:11:12,560 - Epoch: 14  Batch: 1900/2198  Train Loss: 0.6115  LR: 1.0e-03  Time: 1:47:01
2024-06-16 23:11:20,866 - Epoch: 14  Batch: 1950/2198  Train Loss: 0.6118  LR: 1.0e-03  Time: 1:47:09
2024-06-16 23:11:28,518 - Epoch: 14  Batch: 2000/2198  Train Loss: 0.6131  LR: 1.0e-03  Time: 1:47:17
2024-06-16 23:11:37,164 - Epoch: 14  Batch: 2050/2198  Train Loss: 0.6120  LR: 1.0e-03  Time: 1:47:25
2024-06-16 23:11:45,931 - Epoch: 14  Batch: 2100/2198  Train Loss: 0.6105  LR: 1.0e-03  Time: 1:47:34
2024-06-16 23:11:53,228 - Epoch: 14  Batch: 2150/2198  Train Loss: 0.6120  LR: 1.0e-03  Time: 1:47:41
2024-06-16 23:12:29,159 - 
Epoch: 14  Val Loss: 0.5904  R2 score: 0.1947
2024-06-16 23:12:29,160 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-16 23:12:37,044 - Epoch: 15  Batch: 50/2198  Train Loss: 0.6083  LR: 1.0e-03  Time: 1:48:25
2024-06-16 23:12:44,615 - Epoch: 15  Batch: 100/2198  Train Loss: 0.6090  LR: 1.0e-03  Time: 1:48:33
2024-06-16 23:12:53,175 - Epoch: 15  Batch: 150/2198  Train Loss: 0.6093  LR: 1.0e-03  Time: 1:48:41
2024-06-16 23:13:00,912 - Epoch: 15  Batch: 200/2198  Train Loss: 0.6093  LR: 1.0e-03  Time: 1:48:49
2024-06-16 23:13:09,093 - Epoch: 15  Batch: 250/2198  Train Loss: 0.6096  LR: 1.0e-03  Time: 1:48:57
2024-06-16 23:13:16,324 - Epoch: 15  Batch: 300/2198  Train Loss: 0.6102  LR: 1.0e-03  Time: 1:49:04
2024-06-16 23:13:23,869 - Epoch: 15  Batch: 350/2198  Train Loss: 0.6098  LR: 1.0e-03  Time: 1:49:12
2024-06-16 23:13:32,015 - Epoch: 15  Batch: 400/2198  Train Loss: 0.6090  LR: 1.0e-03  Time: 1:49:20
2024-06-16 23:13:39,277 - Epoch: 15  Batch: 450/2198  Train Loss: 0.6095  LR: 1.0e-03  Time: 1:49:27
2024-06-16 23:13:47,263 - Epoch: 15  Batch: 500/2198  Train Loss: 0.6105  LR: 1.0e-03  Time: 1:49:35
2024-06-16 23:13:55,398 - Epoch: 15  Batch: 550/2198  Train Loss: 0.6115  LR: 1.0e-03  Time: 1:49:43
2024-06-16 23:14:04,225 - Epoch: 15  Batch: 600/2198  Train Loss: 0.6104  LR: 1.0e-03  Time: 1:49:52
2024-06-16 23:14:11,492 - Epoch: 15  Batch: 650/2198  Train Loss: 0.6086  LR: 1.0e-03  Time: 1:50:00
2024-06-16 23:14:19,711 - Epoch: 15  Batch: 700/2198  Train Loss: 0.6100  LR: 1.0e-03  Time: 1:50:08
2024-06-16 23:14:27,313 - Epoch: 15  Batch: 750/2198  Train Loss: 0.6092  LR: 1.0e-03  Time: 1:50:15
2024-06-16 23:14:35,023 - Epoch: 15  Batch: 800/2198  Train Loss: 0.6105  LR: 1.0e-03  Time: 1:50:23
2024-06-16 23:14:43,416 - Epoch: 15  Batch: 850/2198  Train Loss: 0.6108  LR: 1.0e-03  Time: 1:50:31
2024-06-16 23:14:50,622 - Epoch: 15  Batch: 900/2198  Train Loss: 0.6095  LR: 1.0e-03  Time: 1:50:39
2024-06-16 23:14:57,853 - Epoch: 15  Batch: 950/2198  Train Loss: 0.6103  LR: 1.0e-03  Time: 1:50:46
2024-06-16 23:15:06,090 - Epoch: 15  Batch: 1000/2198  Train Loss: 0.6083  LR: 1.0e-03  Time: 1:50:54
2024-06-16 23:15:13,739 - Epoch: 15  Batch: 1050/2198  Train Loss: 0.6108  LR: 1.0e-03  Time: 1:51:02
2024-06-16 23:15:21,045 - Epoch: 15  Batch: 1100/2198  Train Loss: 0.6100  LR: 1.0e-03  Time: 1:51:09
2024-06-16 23:15:28,438 - Epoch: 15  Batch: 1150/2198  Train Loss: 0.6102  LR: 1.0e-03  Time: 1:51:16
2024-06-16 23:15:36,070 - Epoch: 15  Batch: 1200/2198  Train Loss: 0.6102  LR: 1.0e-03  Time: 1:51:24
2024-06-16 23:15:44,056 - Epoch: 15  Batch: 1250/2198  Train Loss: 0.6109  LR: 1.0e-03  Time: 1:51:32
2024-06-16 23:15:51,632 - Epoch: 15  Batch: 1300/2198  Train Loss: 0.6093  LR: 1.0e-03  Time: 1:51:40
2024-06-16 23:15:59,200 - Epoch: 15  Batch: 1350/2198  Train Loss: 0.6086  LR: 1.0e-03  Time: 1:51:47
2024-06-16 23:16:07,790 - Epoch: 15  Batch: 1400/2198  Train Loss: 0.6109  LR: 1.0e-03  Time: 1:51:56
2024-06-16 23:16:14,907 - Epoch: 15  Batch: 1450/2198  Train Loss: 0.6096  LR: 1.0e-03  Time: 1:52:03
2024-06-16 23:16:22,786 - Epoch: 15  Batch: 1500/2198  Train Loss: 0.6095  LR: 1.0e-03  Time: 1:52:11
2024-06-16 23:16:30,801 - Epoch: 15  Batch: 1550/2198  Train Loss: 0.6098  LR: 1.0e-03  Time: 1:52:19
2024-06-16 23:16:38,269 - Epoch: 15  Batch: 1600/2198  Train Loss: 0.6103  LR: 1.0e-03  Time: 1:52:26
2024-06-16 23:16:46,828 - Epoch: 15  Batch: 1650/2198  Train Loss: 0.6098  LR: 1.0e-03  Time: 1:52:35
2024-06-16 23:16:54,115 - Epoch: 15  Batch: 1700/2198  Train Loss: 0.6116  LR: 1.0e-03  Time: 1:52:42
2024-06-16 23:17:02,547 - Epoch: 15  Batch: 1750/2198  Train Loss: 0.6107  LR: 1.0e-03  Time: 1:52:51
2024-06-16 23:17:10,028 - Epoch: 15  Batch: 1800/2198  Train Loss: 0.6104  LR: 1.0e-03  Time: 1:52:58
2024-06-16 23:17:18,369 - Epoch: 15  Batch: 1850/2198  Train Loss: 0.6096  LR: 1.0e-03  Time: 1:53:06
2024-06-16 23:17:27,433 - Epoch: 15  Batch: 1900/2198  Train Loss: 0.6099  LR: 1.0e-03  Time: 1:53:15
2024-06-16 23:17:34,723 - Epoch: 15  Batch: 1950/2198  Train Loss: 0.6103  LR: 1.0e-03  Time: 1:53:23
2024-06-16 23:17:42,816 - Epoch: 15  Batch: 2000/2198  Train Loss: 0.6096  LR: 1.0e-03  Time: 1:53:31
2024-06-16 23:17:50,612 - Epoch: 15  Batch: 2050/2198  Train Loss: 0.6095  LR: 1.0e-03  Time: 1:53:39
2024-06-16 23:17:57,812 - Epoch: 15  Batch: 2100/2198  Train Loss: 0.6122  LR: 1.0e-03  Time: 1:53:46
2024-06-16 23:18:05,836 - Epoch: 15  Batch: 2150/2198  Train Loss: 0.6097  LR: 1.0e-03  Time: 1:53:54
2024-06-16 23:18:40,944 - 
Epoch: 15  Val Loss: 0.5886  R2 score: 0.2716
2024-06-16 23:18:40,945 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-16 23:18:48,474 - Epoch: 16  Batch: 50/2198  Train Loss: 0.6085  LR: 1.0e-03  Time: 1:54:36
2024-06-16 23:18:56,786 - Epoch: 16  Batch: 100/2198  Train Loss: 0.6082  LR: 1.0e-03  Time: 1:54:45
2024-06-16 23:19:04,506 - Epoch: 16  Batch: 150/2198  Train Loss: 0.6086  LR: 1.0e-03  Time: 1:54:53
2024-06-16 23:19:11,707 - Epoch: 16  Batch: 200/2198  Train Loss: 0.6094  LR: 1.0e-03  Time: 1:55:00
2024-06-16 23:19:19,318 - Epoch: 16  Batch: 250/2198  Train Loss: 0.6074  LR: 1.0e-03  Time: 1:55:07
2024-06-16 23:19:28,183 - Epoch: 16  Batch: 300/2198  Train Loss: 0.6084  LR: 1.0e-03  Time: 1:55:16
2024-06-16 23:19:35,978 - Epoch: 16  Batch: 350/2198  Train Loss: 0.6092  LR: 1.0e-03  Time: 1:55:24
2024-06-16 23:19:43,445 - Epoch: 16  Batch: 400/2198  Train Loss: 0.6080  LR: 1.0e-03  Time: 1:55:31
2024-06-16 23:19:51,979 - Epoch: 16  Batch: 450/2198  Train Loss: 0.6091  LR: 1.0e-03  Time: 1:55:40
2024-06-16 23:19:59,400 - Epoch: 16  Batch: 500/2198  Train Loss: 0.6090  LR: 1.0e-03  Time: 1:55:47
2024-06-16 23:20:06,610 - Epoch: 16  Batch: 550/2198  Train Loss: 0.6084  LR: 1.0e-03  Time: 1:55:55
2024-06-16 23:20:14,674 - Epoch: 16  Batch: 600/2198  Train Loss: 0.6084  LR: 1.0e-03  Time: 1:56:03
2024-06-16 23:20:22,449 - Epoch: 16  Batch: 650/2198  Train Loss: 0.6088  LR: 1.0e-03  Time: 1:56:10
2024-06-16 23:20:29,781 - Epoch: 16  Batch: 700/2198  Train Loss: 0.6068  LR: 1.0e-03  Time: 1:56:18
2024-06-16 23:20:39,104 - Epoch: 16  Batch: 750/2198  Train Loss: 0.6071  LR: 1.0e-03  Time: 1:56:27
2024-06-16 23:20:46,550 - Epoch: 16  Batch: 800/2198  Train Loss: 0.6069  LR: 1.0e-03  Time: 1:56:35
2024-06-16 23:20:54,788 - Epoch: 16  Batch: 850/2198  Train Loss: 0.6101  LR: 1.0e-03  Time: 1:56:43
2024-06-16 23:21:02,515 - Epoch: 16  Batch: 900/2198  Train Loss: 0.6082  LR: 1.0e-03  Time: 1:56:51
2024-06-16 23:21:09,697 - Epoch: 16  Batch: 950/2198  Train Loss: 0.6089  LR: 1.0e-03  Time: 1:56:58
2024-06-16 23:21:17,963 - Epoch: 16  Batch: 1000/2198  Train Loss: 0.6091  LR: 1.0e-03  Time: 1:57:06
2024-06-16 23:21:25,352 - Epoch: 16  Batch: 1050/2198  Train Loss: 0.6081  LR: 1.0e-03  Time: 1:57:13
2024-06-16 23:21:32,508 - Epoch: 16  Batch: 1100/2198  Train Loss: 0.6097  LR: 1.0e-03  Time: 1:57:21
2024-06-16 23:21:40,170 - Epoch: 16  Batch: 1150/2198  Train Loss: 0.6090  LR: 1.0e-03  Time: 1:57:28
2024-06-16 23:21:48,274 - Epoch: 16  Batch: 1200/2198  Train Loss: 0.6095  LR: 1.0e-03  Time: 1:57:36
2024-06-16 23:21:55,535 - Epoch: 16  Batch: 1250/2198  Train Loss: 0.6084  LR: 1.0e-03  Time: 1:57:44
2024-06-16 23:22:02,920 - Epoch: 16  Batch: 1300/2198  Train Loss: 0.6089  LR: 1.0e-03  Time: 1:57:51
2024-06-16 23:22:11,199 - Epoch: 16  Batch: 1350/2198  Train Loss: 0.6086  LR: 1.0e-03  Time: 1:57:59
2024-06-16 23:22:18,763 - Epoch: 16  Batch: 1400/2198  Train Loss: 0.6076  LR: 1.0e-03  Time: 1:58:07
2024-06-16 23:22:26,363 - Epoch: 16  Batch: 1450/2198  Train Loss: 0.6091  LR: 1.0e-03  Time: 1:58:14
2024-06-16 23:22:34,735 - Epoch: 16  Batch: 1500/2198  Train Loss: 0.6093  LR: 1.0e-03  Time: 1:58:23
2024-06-16 23:22:41,930 - Epoch: 16  Batch: 1550/2198  Train Loss: 0.6081  LR: 1.0e-03  Time: 1:58:30
2024-06-16 23:22:50,481 - Epoch: 16  Batch: 1600/2198  Train Loss: 0.6087  LR: 1.0e-03  Time: 1:58:38
2024-06-16 23:22:57,831 - Epoch: 16  Batch: 1650/2198  Train Loss: 0.6092  LR: 1.0e-03  Time: 1:58:46
2024-06-16 23:23:05,415 - Epoch: 16  Batch: 1700/2198  Train Loss: 0.6090  LR: 1.0e-03  Time: 1:58:53
2024-06-16 23:23:13,464 - Epoch: 16  Batch: 1750/2198  Train Loss: 0.6094  LR: 1.0e-03  Time: 1:59:01
2024-06-16 23:23:20,598 - Epoch: 16  Batch: 1800/2198  Train Loss: 0.6097  LR: 1.0e-03  Time: 1:59:09
2024-06-16 23:23:28,493 - Epoch: 16  Batch: 1850/2198  Train Loss: 0.6083  LR: 1.0e-03  Time: 1:59:17
2024-06-16 23:23:36,398 - Epoch: 16  Batch: 1900/2198  Train Loss: 0.6077  LR: 1.0e-03  Time: 1:59:24
2024-06-16 23:23:43,815 - Epoch: 16  Batch: 1950/2198  Train Loss: 0.6084  LR: 1.0e-03  Time: 1:59:32
2024-06-16 23:23:52,400 - Epoch: 16  Batch: 2000/2198  Train Loss: 0.6096  LR: 1.0e-03  Time: 1:59:40
2024-06-16 23:23:59,654 - Epoch: 16  Batch: 2050/2198  Train Loss: 0.6091  LR: 1.0e-03  Time: 1:59:48
2024-06-16 23:24:07,017 - Epoch: 16  Batch: 2100/2198  Train Loss: 0.6084  LR: 1.0e-03  Time: 1:59:55
2024-06-16 23:24:15,171 - Epoch: 16  Batch: 2150/2198  Train Loss: 0.6081  LR: 1.0e-03  Time: 2:00:03
2024-06-16 23:24:49,799 - 
Epoch: 16  Val Loss: 0.5887  R2 score: -0.7500
2024-06-16 23:24:49,799 - No improvement in validation loss for 1 epochs.
2024-06-16 23:24:57,373 - Epoch: 17  Batch: 50/2198  Train Loss: 0.6080  LR: 1.0e-03  Time: 2:00:45
2024-06-16 23:25:04,885 - Epoch: 17  Batch: 100/2198  Train Loss: 0.6066  LR: 1.0e-03  Time: 2:00:53
2024-06-16 23:25:12,671 - Epoch: 17  Batch: 150/2198  Train Loss: 0.6060  LR: 1.0e-03  Time: 2:01:01
2024-06-16 23:25:21,285 - Epoch: 17  Batch: 200/2198  Train Loss: 0.6064  LR: 1.0e-03  Time: 2:01:09
2024-06-16 23:25:28,739 - Epoch: 17  Batch: 250/2198  Train Loss: 0.6064  LR: 1.0e-03  Time: 2:01:17
2024-06-16 23:25:36,974 - Epoch: 17  Batch: 300/2198  Train Loss: 0.6077  LR: 1.0e-03  Time: 2:01:25
2024-06-16 23:25:45,303 - Epoch: 17  Batch: 350/2198  Train Loss: 0.6075  LR: 1.0e-03  Time: 2:01:33
2024-06-16 23:25:52,805 - Epoch: 17  Batch: 400/2198  Train Loss: 0.6078  LR: 1.0e-03  Time: 2:01:41
2024-06-16 23:25:59,934 - Epoch: 17  Batch: 450/2198  Train Loss: 0.6071  LR: 1.0e-03  Time: 2:01:48
2024-06-16 23:26:07,643 - Epoch: 17  Batch: 500/2198  Train Loss: 0.6078  LR: 1.0e-03  Time: 2:01:56
2024-06-16 23:26:15,922 - Epoch: 17  Batch: 550/2198  Train Loss: 0.6064  LR: 1.0e-03  Time: 2:02:04
2024-06-16 23:26:23,380 - Epoch: 17  Batch: 600/2198  Train Loss: 0.6073  LR: 1.0e-03  Time: 2:02:11
2024-06-16 23:26:30,699 - Epoch: 17  Batch: 650/2198  Train Loss: 0.6084  LR: 1.0e-03  Time: 2:02:19
2024-06-16 23:26:38,144 - Epoch: 17  Batch: 700/2198  Train Loss: 0.6068  LR: 1.0e-03  Time: 2:02:26
2024-06-16 23:26:46,354 - Epoch: 17  Batch: 750/2198  Train Loss: 0.6072  LR: 1.0e-03  Time: 2:02:34
2024-06-16 23:26:53,682 - Epoch: 17  Batch: 800/2198  Train Loss: 0.6073  LR: 1.0e-03  Time: 2:02:42
2024-06-16 23:27:00,817 - Epoch: 17  Batch: 850/2198  Train Loss: 0.6073  LR: 1.0e-03  Time: 2:02:49
2024-06-16 23:27:08,982 - Epoch: 17  Batch: 900/2198  Train Loss: 0.6075  LR: 1.0e-03  Time: 2:02:57
2024-06-16 23:27:16,821 - Epoch: 17  Batch: 950/2198  Train Loss: 0.6074  LR: 1.0e-03  Time: 2:03:05
2024-06-16 23:27:24,048 - Epoch: 17  Batch: 1000/2198  Train Loss: 0.6068  LR: 1.0e-03  Time: 2:03:12
2024-06-16 23:27:31,376 - Epoch: 17  Batch: 1050/2198  Train Loss: 0.6080  LR: 1.0e-03  Time: 2:03:19
2024-06-16 23:27:39,787 - Epoch: 17  Batch: 1100/2198  Train Loss: 0.6079  LR: 1.0e-03  Time: 2:03:28
2024-06-16 23:27:47,227 - Epoch: 17  Batch: 1150/2198  Train Loss: 0.6082  LR: 1.0e-03  Time: 2:03:35
2024-06-16 23:27:54,406 - Epoch: 17  Batch: 1200/2198  Train Loss: 0.6072  LR: 1.0e-03  Time: 2:03:42
2024-06-16 23:28:02,101 - Epoch: 17  Batch: 1250/2198  Train Loss: 0.6079  LR: 1.0e-03  Time: 2:03:50
2024-06-16 23:28:10,511 - Epoch: 17  Batch: 1300/2198  Train Loss: 0.6064  LR: 1.0e-03  Time: 2:03:59
2024-06-16 23:28:17,812 - Epoch: 17  Batch: 1350/2198  Train Loss: 0.6077  LR: 1.0e-03  Time: 2:04:06
2024-06-16 23:28:25,058 - Epoch: 17  Batch: 1400/2198  Train Loss: 0.6084  LR: 1.0e-03  Time: 2:04:13
2024-06-16 23:28:33,667 - Epoch: 17  Batch: 1450/2198  Train Loss: 0.6076  LR: 1.0e-03  Time: 2:04:22
2024-06-16 23:28:41,046 - Epoch: 17  Batch: 1500/2198  Train Loss: 0.6068  LR: 1.0e-03  Time: 2:04:29
2024-06-16 23:28:48,345 - Epoch: 17  Batch: 1550/2198  Train Loss: 0.6068  LR: 1.0e-03  Time: 2:04:36
2024-06-16 23:28:56,526 - Epoch: 17  Batch: 1600/2198  Train Loss: 0.6062  LR: 1.0e-03  Time: 2:04:45
2024-06-16 23:29:04,420 - Epoch: 17  Batch: 1650/2198  Train Loss: 0.6071  LR: 1.0e-03  Time: 2:04:52
2024-06-16 23:29:11,618 - Epoch: 17  Batch: 1700/2198  Train Loss: 0.6074  LR: 1.0e-03  Time: 2:05:00
2024-06-16 23:29:19,284 - Epoch: 17  Batch: 1750/2198  Train Loss: 0.6071  LR: 1.0e-03  Time: 2:05:07
2024-06-16 23:29:27,096 - Epoch: 17  Batch: 1800/2198  Train Loss: 0.6081  LR: 1.0e-03  Time: 2:05:15
2024-06-16 23:29:34,595 - Epoch: 17  Batch: 1850/2198  Train Loss: 0.6074  LR: 1.0e-03  Time: 2:05:23
2024-06-16 23:29:41,872 - Epoch: 17  Batch: 1900/2198  Train Loss: 0.6072  LR: 1.0e-03  Time: 2:05:30
2024-06-16 23:29:49,655 - Epoch: 17  Batch: 1950/2198  Train Loss: 0.6085  LR: 1.0e-03  Time: 2:05:38
2024-06-16 23:29:58,238 - Epoch: 17  Batch: 2000/2198  Train Loss: 0.6086  LR: 1.0e-03  Time: 2:05:46
2024-06-16 23:30:05,936 - Epoch: 17  Batch: 2050/2198  Train Loss: 0.6085  LR: 1.0e-03  Time: 2:05:54
2024-06-16 23:30:14,107 - Epoch: 17  Batch: 2100/2198  Train Loss: 0.6074  LR: 1.0e-03  Time: 2:06:02
2024-06-16 23:30:21,467 - Epoch: 17  Batch: 2150/2198  Train Loss: 0.6078  LR: 1.0e-03  Time: 2:06:09
2024-06-16 23:30:55,346 - 
Epoch: 17  Val Loss: 0.5875  R2 score: 0.0572
2024-06-16 23:30:55,347 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-16 23:31:04,721 - Epoch: 18  Batch: 50/2198  Train Loss: 0.6063  LR: 1.0e-03  Time: 2:06:53
2024-06-16 23:31:13,105 - Epoch: 18  Batch: 100/2198  Train Loss: 0.6066  LR: 1.0e-03  Time: 2:07:01
2024-06-16 23:31:22,557 - Epoch: 18  Batch: 150/2198  Train Loss: 0.6047  LR: 1.0e-03  Time: 2:07:11
2024-06-16 23:31:33,460 - Epoch: 18  Batch: 200/2198  Train Loss: 0.6063  LR: 1.0e-03  Time: 2:07:21
2024-06-16 23:31:45,383 - Epoch: 18  Batch: 250/2198  Train Loss: 0.6055  LR: 1.0e-03  Time: 2:07:33
2024-06-16 23:31:53,630 - Epoch: 18  Batch: 300/2198  Train Loss: 0.6069  LR: 1.0e-03  Time: 2:07:42
2024-06-16 23:32:02,296 - Epoch: 18  Batch: 350/2198  Train Loss: 0.6052  LR: 1.0e-03  Time: 2:07:50
2024-06-16 23:32:10,235 - Epoch: 18  Batch: 400/2198  Train Loss: 0.6071  LR: 1.0e-03  Time: 2:07:58
2024-06-16 23:32:19,209 - Epoch: 18  Batch: 450/2198  Train Loss: 0.6056  LR: 1.0e-03  Time: 2:08:07
2024-06-16 23:32:27,155 - Epoch: 18  Batch: 500/2198  Train Loss: 0.6070  LR: 1.0e-03  Time: 2:08:15
2024-06-16 23:32:36,124 - Epoch: 18  Batch: 550/2198  Train Loss: 0.6063  LR: 1.0e-03  Time: 2:08:24
2024-06-16 23:32:44,161 - Epoch: 18  Batch: 600/2198  Train Loss: 0.6074  LR: 1.0e-03  Time: 2:08:32
2024-06-16 23:32:52,054 - Epoch: 18  Batch: 650/2198  Train Loss: 0.6052  LR: 1.0e-03  Time: 2:08:40
2024-06-16 23:33:00,891 - Epoch: 18  Batch: 700/2198  Train Loss: 0.6058  LR: 1.0e-03  Time: 2:08:49
2024-06-16 23:33:09,114 - Epoch: 18  Batch: 750/2198  Train Loss: 0.6067  LR: 1.0e-03  Time: 2:08:57
2024-06-16 23:33:17,651 - Epoch: 18  Batch: 800/2198  Train Loss: 0.6065  LR: 1.0e-03  Time: 2:09:06
2024-06-16 23:33:26,412 - Epoch: 18  Batch: 850/2198  Train Loss: 0.6056  LR: 1.0e-03  Time: 2:09:14
2024-06-16 23:33:34,902 - Epoch: 18  Batch: 900/2198  Train Loss: 0.6056  LR: 1.0e-03  Time: 2:09:23
2024-06-16 23:33:43,593 - Epoch: 18  Batch: 950/2198  Train Loss: 0.6061  LR: 1.0e-03  Time: 2:09:32
2024-06-16 23:33:52,336 - Epoch: 18  Batch: 1000/2198  Train Loss: 0.6065  LR: 1.0e-03  Time: 2:09:40
2024-06-16 23:34:00,418 - Epoch: 18  Batch: 1050/2198  Train Loss: 0.6067  LR: 1.0e-03  Time: 2:09:48
2024-06-16 23:34:08,090 - Epoch: 18  Batch: 1100/2198  Train Loss: 0.6045  LR: 1.0e-03  Time: 2:09:56
2024-06-16 23:34:15,801 - Epoch: 18  Batch: 1150/2198  Train Loss: 0.6067  LR: 1.0e-03  Time: 2:10:04
2024-06-16 23:34:24,187 - Epoch: 18  Batch: 1200/2198  Train Loss: 0.6075  LR: 1.0e-03  Time: 2:10:12
2024-06-16 23:34:32,390 - Epoch: 18  Batch: 1250/2198  Train Loss: 0.6071  LR: 1.0e-03  Time: 2:10:20
2024-06-16 23:34:40,079 - Epoch: 18  Batch: 1300/2198  Train Loss: 0.6078  LR: 1.0e-03  Time: 2:10:28
2024-06-16 23:34:47,715 - Epoch: 18  Batch: 1350/2198  Train Loss: 0.6066  LR: 1.0e-03  Time: 2:10:36
2024-06-16 23:34:55,856 - Epoch: 18  Batch: 1400/2198  Train Loss: 0.6066  LR: 1.0e-03  Time: 2:10:44
2024-06-16 23:35:04,593 - Epoch: 18  Batch: 1450/2198  Train Loss: 0.6070  LR: 1.0e-03  Time: 2:10:53
2024-06-16 23:35:12,390 - Epoch: 18  Batch: 1500/2198  Train Loss: 0.6063  LR: 1.0e-03  Time: 2:11:00
2024-06-16 23:35:20,070 - Epoch: 18  Batch: 1550/2198  Train Loss: 0.6065  LR: 1.0e-03  Time: 2:11:08
2024-06-16 23:35:28,476 - Epoch: 18  Batch: 1600/2198  Train Loss: 0.6042  LR: 1.0e-03  Time: 2:11:16
2024-06-16 23:35:36,784 - Epoch: 18  Batch: 1650/2198  Train Loss: 0.6066  LR: 1.0e-03  Time: 2:11:25
2024-06-16 23:35:44,464 - Epoch: 18  Batch: 1700/2198  Train Loss: 0.6062  LR: 1.0e-03  Time: 2:11:32
2024-06-16 23:35:52,515 - Epoch: 18  Batch: 1750/2198  Train Loss: 0.6082  LR: 1.0e-03  Time: 2:11:41
2024-06-16 23:36:00,702 - Epoch: 18  Batch: 1800/2198  Train Loss: 0.6079  LR: 1.0e-03  Time: 2:11:49
2024-06-16 23:36:08,586 - Epoch: 18  Batch: 1850/2198  Train Loss: 0.6076  LR: 1.0e-03  Time: 2:11:57
2024-06-16 23:36:16,288 - Epoch: 18  Batch: 1900/2198  Train Loss: 0.6066  LR: 1.0e-03  Time: 2:12:04
2024-06-16 23:36:24,748 - Epoch: 18  Batch: 1950/2198  Train Loss: 0.6069  LR: 1.0e-03  Time: 2:12:13
2024-06-16 23:36:33,087 - Epoch: 18  Batch: 2000/2198  Train Loss: 0.6065  LR: 1.0e-03  Time: 2:12:21
2024-06-16 23:36:40,798 - Epoch: 18  Batch: 2050/2198  Train Loss: 0.6076  LR: 1.0e-03  Time: 2:12:29
2024-06-16 23:36:49,909 - Epoch: 18  Batch: 2100/2198  Train Loss: 0.6061  LR: 1.0e-03  Time: 2:12:38
2024-06-16 23:36:57,572 - Epoch: 18  Batch: 2150/2198  Train Loss: 0.6062  LR: 1.0e-03  Time: 2:12:46
2024-06-16 23:37:39,822 - 
Epoch: 18  Val Loss: 0.5855  R2 score: 0.1938
2024-06-16 23:37:39,823 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-16 23:37:49,131 - Epoch: 19  Batch: 50/2198  Train Loss: 0.6060  LR: 1.0e-03  Time: 2:13:37
2024-06-16 23:37:56,663 - Epoch: 19  Batch: 100/2198  Train Loss: 0.6045  LR: 1.0e-03  Time: 2:13:45
2024-06-16 23:38:05,530 - Epoch: 19  Batch: 150/2198  Train Loss: 0.6045  LR: 1.0e-03  Time: 2:13:54
2024-06-16 23:38:12,754 - Epoch: 19  Batch: 200/2198  Train Loss: 0.6049  LR: 1.0e-03  Time: 2:14:01
2024-06-16 23:38:20,280 - Epoch: 19  Batch: 250/2198  Train Loss: 0.6060  LR: 1.0e-03  Time: 2:14:08
2024-06-16 23:38:28,339 - Epoch: 19  Batch: 300/2198  Train Loss: 0.6048  LR: 1.0e-03  Time: 2:14:16
2024-06-16 23:38:35,964 - Epoch: 19  Batch: 350/2198  Train Loss: 0.6047  LR: 1.0e-03  Time: 2:14:24
2024-06-16 23:38:43,224 - Epoch: 19  Batch: 400/2198  Train Loss: 0.6042  LR: 1.0e-03  Time: 2:14:31
2024-06-16 23:38:51,224 - Epoch: 19  Batch: 450/2198  Train Loss: 0.6053  LR: 1.0e-03  Time: 2:14:39
2024-06-16 23:38:59,270 - Epoch: 19  Batch: 500/2198  Train Loss: 0.6047  LR: 1.0e-03  Time: 2:14:47
2024-06-16 23:39:07,658 - Epoch: 19  Batch: 550/2198  Train Loss: 0.6048  LR: 1.0e-03  Time: 2:14:56
2024-06-16 23:39:16,453 - Epoch: 19  Batch: 600/2198  Train Loss: 0.6059  LR: 1.0e-03  Time: 2:15:04
2024-06-16 23:39:26,715 - Epoch: 19  Batch: 650/2198  Train Loss: 0.6058  LR: 1.0e-03  Time: 2:15:15
2024-06-16 23:39:34,197 - Epoch: 19  Batch: 700/2198  Train Loss: 0.6037  LR: 1.0e-03  Time: 2:15:22
2024-06-16 23:39:42,390 - Epoch: 19  Batch: 750/2198  Train Loss: 0.6066  LR: 1.0e-03  Time: 2:15:30
2024-06-16 23:39:50,267 - Epoch: 19  Batch: 800/2198  Train Loss: 0.6069  LR: 1.0e-03  Time: 2:15:38
2024-06-16 23:39:57,683 - Epoch: 19  Batch: 850/2198  Train Loss: 0.6053  LR: 1.0e-03  Time: 2:15:46
2024-06-16 23:40:05,192 - Epoch: 19  Batch: 900/2198  Train Loss: 0.6053  LR: 1.0e-03  Time: 2:15:53
2024-06-16 23:40:13,583 - Epoch: 19  Batch: 950/2198  Train Loss: 0.6065  LR: 1.0e-03  Time: 2:16:02
2024-06-16 23:40:21,940 - Epoch: 19  Batch: 1000/2198  Train Loss: 0.6041  LR: 1.0e-03  Time: 2:16:10
2024-06-16 23:40:29,278 - Epoch: 19  Batch: 1050/2198  Train Loss: 0.6046  LR: 1.0e-03  Time: 2:16:17
2024-06-16 23:40:37,680 - Epoch: 19  Batch: 1100/2198  Train Loss: 0.6060  LR: 1.0e-03  Time: 2:16:26
2024-06-16 23:40:45,694 - Epoch: 19  Batch: 1150/2198  Train Loss: 0.6051  LR: 1.0e-03  Time: 2:16:34
2024-06-16 23:40:53,336 - Epoch: 19  Batch: 1200/2198  Train Loss: 0.6051  LR: 1.0e-03  Time: 2:16:41
2024-06-16 23:41:01,043 - Epoch: 19  Batch: 1250/2198  Train Loss: 0.6052  LR: 1.0e-03  Time: 2:16:49
2024-06-16 23:41:09,493 - Epoch: 19  Batch: 1300/2198  Train Loss: 0.6060  LR: 1.0e-03  Time: 2:16:58
2024-06-16 23:41:16,700 - Epoch: 19  Batch: 1350/2198  Train Loss: 0.6057  LR: 1.0e-03  Time: 2:17:05
2024-06-16 23:41:24,977 - Epoch: 19  Batch: 1400/2198  Train Loss: 0.6063  LR: 1.0e-03  Time: 2:17:13
2024-06-16 23:41:32,655 - Epoch: 19  Batch: 1450/2198  Train Loss: 0.6059  LR: 1.0e-03  Time: 2:17:21
2024-06-16 23:41:40,659 - Epoch: 19  Batch: 1500/2198  Train Loss: 0.6040  LR: 1.0e-03  Time: 2:17:29
2024-06-16 23:41:48,615 - Epoch: 19  Batch: 1550/2198  Train Loss: 0.6045  LR: 1.0e-03  Time: 2:17:37
2024-06-16 23:41:55,821 - Epoch: 19  Batch: 1600/2198  Train Loss: 0.6055  LR: 1.0e-03  Time: 2:17:44
2024-06-16 23:42:03,884 - Epoch: 19  Batch: 1650/2198  Train Loss: 0.6047  LR: 1.0e-03  Time: 2:17:52
2024-06-16 23:42:11,904 - Epoch: 19  Batch: 1700/2198  Train Loss: 0.6054  LR: 1.0e-03  Time: 2:18:00
2024-06-16 23:42:20,012 - Epoch: 19  Batch: 1750/2198  Train Loss: 0.6057  LR: 1.0e-03  Time: 2:18:08
2024-06-16 23:42:28,951 - Epoch: 19  Batch: 1800/2198  Train Loss: 0.6055  LR: 1.0e-03  Time: 2:18:17
2024-06-16 23:42:36,364 - Epoch: 19  Batch: 1850/2198  Train Loss: 0.6065  LR: 1.0e-03  Time: 2:18:24
2024-06-16 23:42:44,722 - Epoch: 19  Batch: 1900/2198  Train Loss: 0.6052  LR: 1.0e-03  Time: 2:18:33
2024-06-16 23:42:52,388 - Epoch: 19  Batch: 1950/2198  Train Loss: 0.6051  LR: 1.0e-03  Time: 2:18:40
2024-06-16 23:42:59,841 - Epoch: 19  Batch: 2000/2198  Train Loss: 0.6056  LR: 1.0e-03  Time: 2:18:48
2024-06-16 23:43:08,369 - Epoch: 19  Batch: 2050/2198  Train Loss: 0.6060  LR: 1.0e-03  Time: 2:18:56
2024-06-16 23:43:15,852 - Epoch: 19  Batch: 2100/2198  Train Loss: 0.6052  LR: 1.0e-03  Time: 2:19:04
2024-06-16 23:43:23,929 - Epoch: 19  Batch: 2150/2198  Train Loss: 0.6061  LR: 1.0e-03  Time: 2:19:12
2024-06-16 23:44:01,485 - 
Epoch: 19  Val Loss: 0.5853  R2 score: 0.3362
2024-06-16 23:44:01,486 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:54:14,659 - -----------------------------------------------------------------------------
2024-06-24 13:54:14,673 - Hyperparameters:
2024-06-24 13:54:14,674 - DATA_PATH: /data01/jhko/LEAP/
2024-06-24 13:54:14,674 - BATCH_SIZE: 1024
2024-06-24 13:54:14,675 - MIN_STD: 1e-8
2024-06-24 13:54:14,675 - SCHEDULER_PATIENCE: 3
2024-06-24 13:54:14,675 - SCHEDULER_FACTOR: 0.316
2024-06-24 13:54:14,675 - EPOCHS: 19
2024-06-24 13:54:14,676 - PATIENCE: 6
2024-06-24 13:54:14,676 - PRINT_FREQ: 50
2024-06-24 13:54:14,676 - BEST_MODEL_PATH: best_model_seq.pth
2024-06-24 13:54:14,677 - LEARNING_RATE: 0.001
2024-06-24 13:54:14,677 - WEIGHT_DECAY: 0.01
2024-06-24 13:54:27,925 - 
Epoch: 1  Val Loss: 3.2956  R2 score: 0.0000
2024-06-24 13:54:27,926 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:54:28,132 - 
Epoch: 2  Val Loss: 1.8277  R2 score: 0.0000
2024-06-24 13:54:28,133 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:54:28,336 - 
Epoch: 3  Val Loss: 1.2729  R2 score: 0.0001
2024-06-24 13:54:28,337 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:54:28,539 - 
Epoch: 4  Val Loss: 1.1302  R2 score: 0.0001
2024-06-24 13:54:28,540 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:54:28,742 - 
Epoch: 5  Val Loss: 1.1653  R2 score: 0.0001
2024-06-24 13:54:28,743 - No improvement in validation loss for 1 epochs.
2024-06-24 13:54:28,948 - 
Epoch: 6  Val Loss: 1.1599  R2 score: 0.0000
2024-06-24 13:54:28,948 - No improvement in validation loss for 2 epochs.
2024-06-24 13:54:29,153 - 
Epoch: 7  Val Loss: 1.0892  R2 score: 0.0000
2024-06-24 13:54:29,154 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:54:29,356 - 
Epoch: 8  Val Loss: 1.0594  R2 score: 0.0000
2024-06-24 13:54:29,357 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:54:29,559 - 
Epoch: 9  Val Loss: 1.0518  R2 score: 0.0001
2024-06-24 13:54:29,560 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:54:29,762 - 
Epoch: 10  Val Loss: 1.0522  R2 score: 0.0000
2024-06-24 13:54:29,763 - No improvement in validation loss for 1 epochs.
2024-06-24 13:54:29,965 - 
Epoch: 11  Val Loss: 1.0408  R2 score: 0.0000
2024-06-24 13:54:29,966 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:54:30,173 - 
Epoch: 12  Val Loss: 1.0050  R2 score: 0.0000
2024-06-24 13:54:30,174 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:54:30,376 - 
Epoch: 13  Val Loss: 1.0229  R2 score: 0.0000
2024-06-24 13:54:30,377 - No improvement in validation loss for 1 epochs.
2024-06-24 13:54:30,580 - 
Epoch: 14  Val Loss: 1.0114  R2 score: 0.0000
2024-06-24 13:54:30,580 - No improvement in validation loss for 2 epochs.
2024-06-24 13:54:30,781 - 
Epoch: 15  Val Loss: 1.0082  R2 score: 0.0000
2024-06-24 13:54:30,782 - No improvement in validation loss for 3 epochs.
2024-06-24 13:54:30,984 - 
Epoch: 16  Val Loss: 0.9893  R2 score: 0.0000
2024-06-24 13:54:30,985 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:54:31,186 - 
Epoch: 17  Val Loss: 0.9967  R2 score: 0.0000
2024-06-24 13:54:31,187 - No improvement in validation loss for 1 epochs.
2024-06-24 13:54:31,402 - 
Epoch: 18  Val Loss: 0.9856  R2 score: 0.0000
2024-06-24 13:54:31,403 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:54:31,605 - 
Epoch: 19  Val Loss: 0.9841  R2 score: 0.0000
2024-06-24 13:54:31,606 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:55:08,322 - -----------------------------------------------------------------------------
2024-06-24 13:55:08,322 - Hyperparameters:
2024-06-24 13:55:08,322 - DATA_PATH: /data01/jhko/LEAP/
2024-06-24 13:55:08,323 - BATCH_SIZE: 1024
2024-06-24 13:55:08,323 - MIN_STD: 1e-8
2024-06-24 13:55:08,324 - SCHEDULER_PATIENCE: 3
2024-06-24 13:55:08,324 - SCHEDULER_FACTOR: 0.316
2024-06-24 13:55:08,324 - EPOCHS: 19
2024-06-24 13:55:08,325 - PATIENCE: 6
2024-06-24 13:55:08,325 - PRINT_FREQ: 50
2024-06-24 13:55:08,325 - BEST_MODEL_PATH: best_model_seq.pth
2024-06-24 13:55:08,326 - LEARNING_RATE: 0.001
2024-06-24 13:55:08,326 - WEIGHT_DECAY: 0.01
2024-06-24 13:55:18,809 - 
Epoch: 1  Val Loss: 3.2956  R2 score: 0.0000
2024-06-24 13:55:18,810 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:55:19,071 - 
Epoch: 2  Val Loss: 1.8277  R2 score: 0.0000
2024-06-24 13:55:19,072 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:55:19,334 - 
Epoch: 3  Val Loss: 1.2729  R2 score: 0.0001
2024-06-24 13:55:19,335 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:55:19,595 - 
Epoch: 4  Val Loss: 1.1302  R2 score: 0.0001
2024-06-24 13:55:19,596 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:55:19,856 - 
Epoch: 5  Val Loss: 1.1653  R2 score: 0.0001
2024-06-24 13:55:19,856 - No improvement in validation loss for 1 epochs.
2024-06-24 13:55:20,116 - 
Epoch: 6  Val Loss: 1.1599  R2 score: 0.0000
2024-06-24 13:55:20,116 - No improvement in validation loss for 2 epochs.
2024-06-24 13:55:20,377 - 
Epoch: 7  Val Loss: 1.0892  R2 score: 0.0000
2024-06-24 13:55:20,377 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:55:20,636 - 
Epoch: 8  Val Loss: 1.0594  R2 score: 0.0000
2024-06-24 13:55:20,637 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:55:20,896 - 
Epoch: 9  Val Loss: 1.0518  R2 score: 0.0001
2024-06-24 13:55:20,896 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:55:21,155 - 
Epoch: 10  Val Loss: 1.0522  R2 score: 0.0000
2024-06-24 13:55:21,156 - No improvement in validation loss for 1 epochs.
2024-06-24 13:55:21,414 - 
Epoch: 11  Val Loss: 1.0408  R2 score: 0.0000
2024-06-24 13:55:21,415 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:55:21,673 - 
Epoch: 12  Val Loss: 1.0050  R2 score: 0.0000
2024-06-24 13:55:21,674 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:55:21,932 - 
Epoch: 13  Val Loss: 1.0229  R2 score: 0.0000
2024-06-24 13:55:21,933 - No improvement in validation loss for 1 epochs.
2024-06-24 13:55:22,196 - 
Epoch: 14  Val Loss: 1.0114  R2 score: 0.0000
2024-06-24 13:55:22,196 - No improvement in validation loss for 2 epochs.
2024-06-24 13:55:22,457 - 
Epoch: 15  Val Loss: 1.0082  R2 score: 0.0000
2024-06-24 13:55:22,458 - No improvement in validation loss for 3 epochs.
2024-06-24 13:55:22,716 - 
Epoch: 16  Val Loss: 0.9893  R2 score: 0.0000
2024-06-24 13:55:22,716 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:55:22,975 - 
Epoch: 17  Val Loss: 0.9967  R2 score: 0.0000
2024-06-24 13:55:22,975 - No improvement in validation loss for 1 epochs.
2024-06-24 13:55:23,234 - 
Epoch: 18  Val Loss: 0.9856  R2 score: 0.0000
2024-06-24 13:55:23,235 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:55:23,493 - 
Epoch: 19  Val Loss: 0.9841  R2 score: 0.0000
2024-06-24 13:55:23,494 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 13:58:09,360 - -----------------------------------------------------------------------------
2024-06-24 13:58:09,361 - Hyperparameters:
2024-06-24 13:58:09,361 - DATA_PATH: /data01/jhko/LEAP/
2024-06-24 13:58:09,362 - BATCH_SIZE: 1024
2024-06-24 13:58:09,362 - MIN_STD: 1e-8
2024-06-24 13:58:09,363 - SCHEDULER_PATIENCE: 3
2024-06-24 13:58:09,363 - SCHEDULER_FACTOR: 0.316
2024-06-24 13:58:09,363 - EPOCHS: 10
2024-06-24 13:58:09,363 - PATIENCE: 6
2024-06-24 13:58:09,364 - PRINT_FREQ: 50
2024-06-24 13:58:09,364 - BEST_MODEL_PATH: best_model_seq.pth
2024-06-24 13:58:09,365 - LEARNING_RATE: 0.001
2024-06-24 13:58:09,365 - WEIGHT_DECAY: 0.01
2024-06-24 13:59:07,884 - -----------------------------------------------------------------------------
2024-06-24 13:59:07,885 - Hyperparameters:
2024-06-24 13:59:07,886 - DATA_PATH: /data01/jhko/LEAP/
2024-06-24 13:59:07,886 - BATCH_SIZE: 1024
2024-06-24 13:59:07,886 - MIN_STD: 1e-8
2024-06-24 13:59:07,887 - SCHEDULER_PATIENCE: 3
2024-06-24 13:59:07,887 - SCHEDULER_FACTOR: 0.316
2024-06-24 13:59:07,887 - EPOCHS: 10
2024-06-24 13:59:07,888 - PATIENCE: 6
2024-06-24 13:59:07,888 - PRINT_FREQ: 50
2024-06-24 13:59:07,888 - BEST_MODEL_PATH: best_model_seq.pth
2024-06-24 13:59:07,889 - LEARNING_RATE: 0.001
2024-06-24 13:59:07,889 - WEIGHT_DECAY: 0.01
2024-06-24 14:00:46,626 - ('  Epoch: 1', '  Batch: 50/2198', '  Train Loss: 1.1479', '  LR: 1.0e-03', '  Time: 0:01:39')
2024-06-24 14:00:51,809 - ('  Epoch: 1', '  Batch: 100/2198', '  Train Loss: 0.9956', '  LR: 1.0e-03', '  Time: 0:01:44')
2024-06-24 14:00:56,766 - ('  Epoch: 1', '  Batch: 150/2198', '  Train Loss: 0.8866', '  LR: 1.0e-03', '  Time: 0:01:49')
2024-06-24 14:01:01,747 - ('  Epoch: 1', '  Batch: 200/2198', '  Train Loss: 0.8625', '  LR: 1.0e-03', '  Time: 0:01:54')
2024-06-24 14:01:06,808 - ('  Epoch: 1', '  Batch: 250/2198', '  Train Loss: 0.8284', '  LR: 1.0e-03', '  Time: 0:01:59')
2024-06-24 14:01:11,859 - ('  Epoch: 1', '  Batch: 300/2198', '  Train Loss: 0.8058', '  LR: 1.0e-03', '  Time: 0:02:04')
2024-06-24 14:01:16,990 - ('  Epoch: 1', '  Batch: 350/2198', '  Train Loss: 0.7851', '  LR: 1.0e-03', '  Time: 0:02:09')
2024-06-24 14:01:21,974 - ('  Epoch: 1', '  Batch: 400/2198', '  Train Loss: 0.7730', '  LR: 1.0e-03', '  Time: 0:02:14')
2024-06-24 14:01:27,389 - ('  Epoch: 1', '  Batch: 450/2198', '  Train Loss: 0.7608', '  LR: 1.0e-03', '  Time: 0:02:19')
2024-06-24 14:01:32,852 - ('  Epoch: 1', '  Batch: 500/2198', '  Train Loss: 0.7540', '  LR: 1.0e-03', '  Time: 0:02:25')
2024-06-24 14:01:37,870 - ('  Epoch: 1', '  Batch: 550/2198', '  Train Loss: 0.7450', '  LR: 1.0e-03', '  Time: 0:02:30')
2024-06-24 14:01:42,928 - ('  Epoch: 1', '  Batch: 600/2198', '  Train Loss: 0.7393', '  LR: 1.0e-03', '  Time: 0:02:35')
2024-06-24 14:01:47,898 - ('  Epoch: 1', '  Batch: 650/2198', '  Train Loss: 0.7295', '  LR: 1.0e-03', '  Time: 0:02:40')
2024-06-24 14:01:52,839 - ('  Epoch: 1', '  Batch: 700/2198', '  Train Loss: 0.7215', '  LR: 1.0e-03', '  Time: 0:02:45')
2024-06-24 14:01:57,950 - ('  Epoch: 1', '  Batch: 750/2198', '  Train Loss: 0.7157', '  LR: 1.0e-03', '  Time: 0:02:50')
2024-06-24 14:02:03,192 - ('  Epoch: 1', '  Batch: 800/2198', '  Train Loss: 0.7101', '  LR: 1.0e-03', '  Time: 0:02:55')
2024-06-24 14:02:08,540 - ('  Epoch: 1', '  Batch: 850/2198', '  Train Loss: 0.7049', '  LR: 1.0e-03', '  Time: 0:03:01')
2024-06-24 14:02:13,730 - ('  Epoch: 1', '  Batch: 900/2198', '  Train Loss: 0.7007', '  LR: 1.0e-03', '  Time: 0:03:06')
2024-06-24 14:02:18,650 - ('  Epoch: 1', '  Batch: 950/2198', '  Train Loss: 0.6951', '  LR: 1.0e-03', '  Time: 0:03:11')
2024-06-24 14:02:23,573 - ('  Epoch: 1', '  Batch: 1000/2198', '  Train Loss: 0.6923', '  LR: 1.0e-03', '  Time: 0:03:16')
2024-06-24 14:02:28,497 - ('  Epoch: 1', '  Batch: 1050/2198', '  Train Loss: 0.6869', '  LR: 1.0e-03', '  Time: 0:03:21')
2024-06-24 14:02:33,430 - ('  Epoch: 1', '  Batch: 1100/2198', '  Train Loss: 0.6843', '  LR: 1.0e-03', '  Time: 0:03:26')
2024-06-24 14:02:38,463 - ('  Epoch: 1', '  Batch: 1150/2198', '  Train Loss: 0.6811', '  LR: 1.0e-03', '  Time: 0:03:31')
2024-06-24 14:02:43,701 - ('  Epoch: 1', '  Batch: 1200/2198', '  Train Loss: 0.6790', '  LR: 1.0e-03', '  Time: 0:03:36')
2024-06-24 14:02:48,909 - ('  Epoch: 1', '  Batch: 1250/2198', '  Train Loss: 0.6741', '  LR: 1.0e-03', '  Time: 0:03:41')
2024-06-24 14:02:54,171 - ('  Epoch: 1', '  Batch: 1300/2198', '  Train Loss: 0.6739', '  LR: 1.0e-03', '  Time: 0:03:46')
2024-06-24 14:02:59,119 - ('  Epoch: 1', '  Batch: 1350/2198', '  Train Loss: 0.6701', '  LR: 1.0e-03', '  Time: 0:03:51')
2024-06-24 14:03:04,144 - ('  Epoch: 1', '  Batch: 1400/2198', '  Train Loss: 0.6696', '  LR: 1.0e-03', '  Time: 0:03:56')
2024-06-24 14:03:09,063 - ('  Epoch: 1', '  Batch: 1450/2198', '  Train Loss: 0.6664', '  LR: 1.0e-03', '  Time: 0:04:01')
2024-06-24 14:03:13,996 - ('  Epoch: 1', '  Batch: 1500/2198', '  Train Loss: 0.6656', '  LR: 1.0e-03', '  Time: 0:04:06')
2024-06-24 14:03:18,937 - ('  Epoch: 1', '  Batch: 1550/2198', '  Train Loss: 0.6616', '  LR: 1.0e-03', '  Time: 0:04:11')
2024-06-24 14:03:23,942 - ('  Epoch: 1', '  Batch: 1600/2198', '  Train Loss: 0.6611', '  LR: 1.0e-03', '  Time: 0:04:16')
2024-06-24 14:03:29,314 - ('  Epoch: 1', '  Batch: 1650/2198', '  Train Loss: 0.6608', '  LR: 1.0e-03', '  Time: 0:04:21')
2024-06-24 14:03:34,551 - ('  Epoch: 1', '  Batch: 1700/2198', '  Train Loss: 0.6577', '  LR: 1.0e-03', '  Time: 0:04:27')
2024-06-24 14:03:39,694 - ('  Epoch: 1', '  Batch: 1750/2198', '  Train Loss: 0.6566', '  LR: 1.0e-03', '  Time: 0:04:32')
2024-06-24 14:03:44,633 - ('  Epoch: 1', '  Batch: 1800/2198', '  Train Loss: 0.6542', '  LR: 1.0e-03', '  Time: 0:04:37')
2024-06-24 14:03:49,552 - ('  Epoch: 1', '  Batch: 1850/2198', '  Train Loss: 0.6517', '  LR: 1.0e-03', '  Time: 0:04:42')
2024-06-24 14:03:54,581 - ('  Epoch: 1', '  Batch: 1900/2198', '  Train Loss: 0.6513', '  LR: 1.0e-03', '  Time: 0:04:47')
2024-06-24 14:03:59,522 - ('  Epoch: 1', '  Batch: 1950/2198', '  Train Loss: 0.6485', '  LR: 1.0e-03', '  Time: 0:04:52')
2024-06-24 14:04:04,444 - ('  Epoch: 1', '  Batch: 2000/2198', '  Train Loss: 0.6506', '  LR: 1.0e-03', '  Time: 0:04:57')
2024-06-24 14:04:09,612 - ('  Epoch: 1', '  Batch: 2050/2198', '  Train Loss: 0.6471', '  LR: 1.0e-03', '  Time: 0:05:02')
2024-06-24 14:04:14,831 - ('  Epoch: 1', '  Batch: 2100/2198', '  Train Loss: 0.6473', '  LR: 1.0e-03', '  Time: 0:05:07')
2024-06-24 14:04:20,090 - ('  Epoch: 1', '  Batch: 2150/2198', '  Train Loss: 0.6476', '  LR: 1.0e-03', '  Time: 0:05:12')
2024-06-24 14:04:48,292 - 
Epoch: 1  Val Loss: 0.6335  R2 score: 0.2891
2024-06-24 14:04:48,294 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 14:04:53,546 - ('  Epoch: 2', '  Batch: 50/2198', '  Train Loss: 0.6440', '  LR: 1.0e-03', '  Time: 0:05:46')
2024-06-24 14:04:58,764 - ('  Epoch: 2', '  Batch: 100/2198', '  Train Loss: 0.6421', '  LR: 1.0e-03', '  Time: 0:05:51')
2024-06-24 14:05:04,170 - ('  Epoch: 2', '  Batch: 150/2198', '  Train Loss: 0.6411', '  LR: 1.0e-03', '  Time: 0:05:56')
2024-06-24 14:05:09,154 - ('  Epoch: 2', '  Batch: 200/2198', '  Train Loss: 0.6412', '  LR: 1.0e-03', '  Time: 0:06:01')
2024-06-24 14:05:14,083 - ('  Epoch: 2', '  Batch: 250/2198', '  Train Loss: 0.6365', '  LR: 1.0e-03', '  Time: 0:06:06')
2024-06-24 14:05:19,018 - ('  Epoch: 2', '  Batch: 300/2198', '  Train Loss: 0.6387', '  LR: 1.0e-03', '  Time: 0:06:11')
2024-06-24 14:05:23,956 - ('  Epoch: 2', '  Batch: 350/2198', '  Train Loss: 0.6381', '  LR: 1.0e-03', '  Time: 0:06:16')
2024-06-24 14:05:29,004 - ('  Epoch: 2', '  Batch: 400/2198', '  Train Loss: 0.6380', '  LR: 1.0e-03', '  Time: 0:06:21')
2024-06-24 14:05:33,940 - ('  Epoch: 2', '  Batch: 450/2198', '  Train Loss: 0.6367', '  LR: 1.0e-03', '  Time: 0:06:26')
2024-06-24 14:05:39,212 - ('  Epoch: 2', '  Batch: 500/2198', '  Train Loss: 0.6350', '  LR: 1.0e-03', '  Time: 0:06:31')
2024-06-24 14:05:44,421 - ('  Epoch: 2', '  Batch: 550/2198', '  Train Loss: 0.6352', '  LR: 1.0e-03', '  Time: 0:06:37')
2024-06-24 14:05:49,684 - ('  Epoch: 2', '  Batch: 600/2198', '  Train Loss: 0.6321', '  LR: 1.0e-03', '  Time: 0:06:42')
2024-06-24 14:05:54,732 - ('  Epoch: 2', '  Batch: 650/2198', '  Train Loss: 0.6330', '  LR: 1.0e-03', '  Time: 0:06:47')
2024-06-24 14:05:59,662 - ('  Epoch: 2', '  Batch: 700/2198', '  Train Loss: 0.6312', '  LR: 1.0e-03', '  Time: 0:06:52')
2024-06-24 14:06:04,590 - ('  Epoch: 2', '  Batch: 750/2198', '  Train Loss: 0.6302', '  LR: 1.0e-03', '  Time: 0:06:57')
2024-06-24 14:06:09,531 - ('  Epoch: 2', '  Batch: 800/2198', '  Train Loss: 0.6297', '  LR: 1.0e-03', '  Time: 0:07:02')
2024-06-24 14:06:14,484 - ('  Epoch: 2', '  Batch: 850/2198', '  Train Loss: 0.6316', '  LR: 1.0e-03', '  Time: 0:07:07')
2024-06-24 14:06:19,640 - ('  Epoch: 2', '  Batch: 900/2198', '  Train Loss: 0.6308', '  LR: 1.0e-03', '  Time: 0:07:12')
2024-06-24 14:06:24,895 - ('  Epoch: 2', '  Batch: 950/2198', '  Train Loss: 0.6284', '  LR: 1.0e-03', '  Time: 0:07:17')
2024-06-24 14:06:30,145 - ('  Epoch: 2', '  Batch: 1000/2198', '  Train Loss: 0.6278', '  LR: 1.0e-03', '  Time: 0:07:22')
2024-06-24 14:06:35,266 - ('  Epoch: 2', '  Batch: 1050/2198', '  Train Loss: 0.6277', '  LR: 1.0e-03', '  Time: 0:07:27')
2024-06-24 14:06:40,207 - ('  Epoch: 2', '  Batch: 1100/2198', '  Train Loss: 0.6257', '  LR: 1.0e-03', '  Time: 0:07:32')
2024-06-24 14:06:45,135 - ('  Epoch: 2', '  Batch: 1150/2198', '  Train Loss: 0.6253', '  LR: 1.0e-03', '  Time: 0:07:37')
2024-06-24 14:06:50,166 - ('  Epoch: 2', '  Batch: 1200/2198', '  Train Loss: 0.6248', '  LR: 1.0e-03', '  Time: 0:07:42')
2024-06-24 14:06:55,111 - ('  Epoch: 2', '  Batch: 1250/2198', '  Train Loss: 0.6238', '  LR: 1.0e-03', '  Time: 0:07:47')
2024-06-24 14:07:00,033 - ('  Epoch: 2', '  Batch: 1300/2198', '  Train Loss: 0.6230', '  LR: 1.0e-03', '  Time: 0:07:52')
2024-06-24 14:07:05,230 - ('  Epoch: 2', '  Batch: 1350/2198', '  Train Loss: 0.6233', '  LR: 1.0e-03', '  Time: 0:07:57')
2024-06-24 14:07:10,442 - ('  Epoch: 2', '  Batch: 1400/2198', '  Train Loss: 0.6217', '  LR: 1.0e-03', '  Time: 0:08:03')
2024-06-24 14:07:15,836 - ('  Epoch: 2', '  Batch: 1450/2198', '  Train Loss: 0.6241', '  LR: 1.0e-03', '  Time: 0:08:08')
2024-06-24 14:07:20,800 - ('  Epoch: 2', '  Batch: 1500/2198', '  Train Loss: 0.6218', '  LR: 1.0e-03', '  Time: 0:08:13')
2024-06-24 14:07:25,722 - ('  Epoch: 2', '  Batch: 1550/2198', '  Train Loss: 0.6200', '  LR: 1.0e-03', '  Time: 0:08:18')
2024-06-24 14:07:30,682 - ('  Epoch: 2', '  Batch: 1600/2198', '  Train Loss: 0.6190', '  LR: 1.0e-03', '  Time: 0:08:23')
2024-06-24 14:07:36,338 - ('  Epoch: 2', '  Batch: 1650/2198', '  Train Loss: 0.6203', '  LR: 1.0e-03', '  Time: 0:08:28')
2024-06-24 14:07:41,800 - ('  Epoch: 2', '  Batch: 1700/2198', '  Train Loss: 0.6206', '  LR: 1.0e-03', '  Time: 0:08:34')
2024-06-24 14:07:46,966 - ('  Epoch: 2', '  Batch: 1750/2198', '  Train Loss: 0.6180', '  LR: 1.0e-03', '  Time: 0:08:39')
2024-06-24 14:07:54,957 - ('  Epoch: 2', '  Batch: 1800/2198', '  Train Loss: 0.6177', '  LR: 1.0e-03', '  Time: 0:08:47')
2024-06-24 14:08:00,091 - ('  Epoch: 2', '  Batch: 1850/2198', '  Train Loss: 0.6168', '  LR: 1.0e-03', '  Time: 0:08:52')
2024-06-24 14:08:05,158 - ('  Epoch: 2', '  Batch: 1900/2198', '  Train Loss: 0.6168', '  LR: 1.0e-03', '  Time: 0:08:57')
2024-06-24 14:08:12,162 - ('  Epoch: 2', '  Batch: 1950/2198', '  Train Loss: 0.6152', '  LR: 1.0e-03', '  Time: 0:09:04')
2024-06-24 14:08:19,424 - ('  Epoch: 2', '  Batch: 2000/2198', '  Train Loss: 0.6168', '  LR: 1.0e-03', '  Time: 0:09:12')
2024-06-24 14:08:24,399 - ('  Epoch: 2', '  Batch: 2050/2198', '  Train Loss: 0.6184', '  LR: 1.0e-03', '  Time: 0:09:17')
2024-06-24 14:08:29,156 - ('  Epoch: 2', '  Batch: 2100/2198', '  Train Loss: 0.6144', '  LR: 1.0e-03', '  Time: 0:09:21')
2024-06-24 14:08:33,916 - ('  Epoch: 2', '  Batch: 2150/2198', '  Train Loss: 0.6150', '  LR: 1.0e-03', '  Time: 0:09:26')
2024-06-24 14:09:02,055 - 
Epoch: 2  Val Loss: 0.6028  R2 score: 0.3273
2024-06-24 14:09:02,058 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 14:09:07,863 - ('  Epoch: 3', '  Batch: 50/2198', '  Train Loss: 0.6127', '  LR: 1.0e-03', '  Time: 0:10:00')
2024-06-24 14:09:13,423 - ('  Epoch: 3', '  Batch: 100/2198', '  Train Loss: 0.6122', '  LR: 1.0e-03', '  Time: 0:10:06')
2024-06-24 14:09:18,702 - ('  Epoch: 3', '  Batch: 150/2198', '  Train Loss: 0.6135', '  LR: 1.0e-03', '  Time: 0:10:11')
2024-06-24 14:09:24,468 - ('  Epoch: 3', '  Batch: 200/2198', '  Train Loss: 0.6132', '  LR: 1.0e-03', '  Time: 0:10:17')
2024-06-24 14:09:29,693 - ('  Epoch: 3', '  Batch: 250/2198', '  Train Loss: 0.6126', '  LR: 1.0e-03', '  Time: 0:10:22')
2024-06-24 14:09:34,735 - ('  Epoch: 3', '  Batch: 300/2198', '  Train Loss: 0.6144', '  LR: 1.0e-03', '  Time: 0:10:27')
2024-06-24 14:09:39,794 - ('  Epoch: 3', '  Batch: 350/2198', '  Train Loss: 0.6098', '  LR: 1.0e-03', '  Time: 0:10:32')
2024-06-24 14:09:47,649 - ('  Epoch: 3', '  Batch: 400/2198', '  Train Loss: 0.6113', '  LR: 1.0e-03', '  Time: 0:10:40')
2024-06-24 14:09:55,495 - ('  Epoch: 3', '  Batch: 450/2198', '  Train Loss: 0.6098', '  LR: 1.0e-03', '  Time: 0:10:48')
2024-06-24 14:10:00,457 - ('  Epoch: 3', '  Batch: 500/2198', '  Train Loss: 0.6090', '  LR: 1.0e-03', '  Time: 0:10:53')
2024-06-24 14:10:05,377 - ('  Epoch: 3', '  Batch: 550/2198', '  Train Loss: 0.6086', '  LR: 1.0e-03', '  Time: 0:10:57')
2024-06-24 14:10:10,301 - ('  Epoch: 3', '  Batch: 600/2198', '  Train Loss: 0.6104', '  LR: 1.0e-03', '  Time: 0:11:02')
2024-06-24 14:10:14,683 - ('  Epoch: 3', '  Batch: 650/2198', '  Train Loss: 0.6083', '  LR: 1.0e-03', '  Time: 0:11:07')
2024-06-24 14:10:18,981 - ('  Epoch: 3', '  Batch: 700/2198', '  Train Loss: 0.6085', '  LR: 1.0e-03', '  Time: 0:11:11')
2024-06-24 14:10:23,540 - ('  Epoch: 3', '  Batch: 750/2198', '  Train Loss: 0.6097', '  LR: 1.0e-03', '  Time: 0:11:16')
2024-06-24 14:10:28,036 - ('  Epoch: 3', '  Batch: 800/2198', '  Train Loss: 0.6058', '  LR: 1.0e-03', '  Time: 0:11:20')
2024-06-24 14:10:32,576 - ('  Epoch: 3', '  Batch: 850/2198', '  Train Loss: 0.6071', '  LR: 1.0e-03', '  Time: 0:11:25')
2024-06-24 14:10:37,082 - ('  Epoch: 3', '  Batch: 900/2198', '  Train Loss: 0.6067', '  LR: 1.0e-03', '  Time: 0:11:29')
2024-06-24 14:10:41,343 - ('  Epoch: 3', '  Batch: 950/2198', '  Train Loss: 0.6082', '  LR: 1.0e-03', '  Time: 0:11:33')
2024-06-24 14:10:45,607 - ('  Epoch: 3', '  Batch: 1000/2198', '  Train Loss: 0.6070', '  LR: 1.0e-03', '  Time: 0:11:38')
2024-06-24 14:10:49,871 - ('  Epoch: 3', '  Batch: 1050/2198', '  Train Loss: 0.6058', '  LR: 1.0e-03', '  Time: 0:11:42')
2024-06-24 14:10:54,144 - ('  Epoch: 3', '  Batch: 1100/2198', '  Train Loss: 0.6051', '  LR: 1.0e-03', '  Time: 0:11:46')
2024-06-24 14:10:58,529 - ('  Epoch: 3', '  Batch: 1150/2198', '  Train Loss: 0.6051', '  LR: 1.0e-03', '  Time: 0:11:51')
2024-06-24 14:11:02,827 - ('  Epoch: 3', '  Batch: 1200/2198', '  Train Loss: 0.6054', '  LR: 1.0e-03', '  Time: 0:11:55')
2024-06-24 14:11:07,388 - ('  Epoch: 3', '  Batch: 1250/2198', '  Train Loss: 0.6045', '  LR: 1.0e-03', '  Time: 0:11:59')
2024-06-24 14:11:11,886 - ('  Epoch: 3', '  Batch: 1300/2198', '  Train Loss: 0.6035', '  LR: 1.0e-03', '  Time: 0:12:04')
2024-06-24 14:11:16,428 - ('  Epoch: 3', '  Batch: 1350/2198', '  Train Loss: 0.6036', '  LR: 1.0e-03', '  Time: 0:12:09')
2024-06-24 14:11:20,826 - ('  Epoch: 3', '  Batch: 1400/2198', '  Train Loss: 0.6053', '  LR: 1.0e-03', '  Time: 0:12:13')
2024-06-24 14:11:25,200 - ('  Epoch: 3', '  Batch: 1450/2198', '  Train Loss: 0.6028', '  LR: 1.0e-03', '  Time: 0:12:17')
2024-06-24 14:11:29,461 - ('  Epoch: 3', '  Batch: 1500/2198', '  Train Loss: 0.6038', '  LR: 1.0e-03', '  Time: 0:12:22')
2024-06-24 14:11:33,726 - ('  Epoch: 3', '  Batch: 1550/2198', '  Train Loss: 0.6021', '  LR: 1.0e-03', '  Time: 0:12:26')
2024-06-24 14:11:37,998 - ('  Epoch: 3', '  Batch: 1600/2198', '  Train Loss: 0.6024', '  LR: 1.0e-03', '  Time: 0:12:30')
2024-06-24 14:11:42,278 - ('  Epoch: 3', '  Batch: 1650/2198', '  Train Loss: 0.6021', '  LR: 1.0e-03', '  Time: 0:12:34')
2024-06-24 14:11:46,676 - ('  Epoch: 3', '  Batch: 1700/2198', '  Train Loss: 0.6004', '  LR: 1.0e-03', '  Time: 0:12:39')
2024-06-24 14:11:51,235 - ('  Epoch: 3', '  Batch: 1750/2198', '  Train Loss: 0.6007', '  LR: 1.0e-03', '  Time: 0:12:43')
2024-06-24 14:11:55,733 - ('  Epoch: 3', '  Batch: 1800/2198', '  Train Loss: 0.6037', '  LR: 1.0e-03', '  Time: 0:12:48')
2024-06-24 14:12:00,267 - ('  Epoch: 3', '  Batch: 1850/2198', '  Train Loss: 0.6009', '  LR: 1.0e-03', '  Time: 0:12:52')
2024-06-24 14:12:04,662 - ('  Epoch: 3', '  Batch: 1900/2198', '  Train Loss: 0.6000', '  LR: 1.0e-03', '  Time: 0:12:57')
2024-06-24 14:12:09,035 - ('  Epoch: 3', '  Batch: 1950/2198', '  Train Loss: 0.5995', '  LR: 1.0e-03', '  Time: 0:13:01')
2024-06-24 14:12:13,300 - ('  Epoch: 3', '  Batch: 2000/2198', '  Train Loss: 0.6000', '  LR: 1.0e-03', '  Time: 0:13:05')
2024-06-24 14:12:17,563 - ('  Epoch: 3', '  Batch: 2050/2198', '  Train Loss: 0.5999', '  LR: 1.0e-03', '  Time: 0:13:10')
2024-06-24 14:12:21,835 - ('  Epoch: 3', '  Batch: 2100/2198', '  Train Loss: 0.6003', '  LR: 1.0e-03', '  Time: 0:13:14')
2024-06-24 14:12:26,113 - ('  Epoch: 3', '  Batch: 2150/2198', '  Train Loss: 0.5993', '  LR: 1.0e-03', '  Time: 0:13:18')
2024-06-24 14:12:50,584 - 
Epoch: 3  Val Loss: 0.5866  R2 score: 0.3481
2024-06-24 14:12:50,585 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 14:12:55,678 - ('  Epoch: 4', '  Batch: 50/2198', '  Train Loss: 0.5966', '  LR: 1.0e-03', '  Time: 0:13:48')
2024-06-24 14:13:01,100 - ('  Epoch: 4', '  Batch: 100/2198', '  Train Loss: 0.5983', '  LR: 1.0e-03', '  Time: 0:13:53')
2024-06-24 14:13:06,623 - ('  Epoch: 4', '  Batch: 150/2198', '  Train Loss: 0.5971', '  LR: 1.0e-03', '  Time: 0:13:59')
2024-06-24 14:13:11,086 - ('  Epoch: 4', '  Batch: 200/2198', '  Train Loss: 0.5968', '  LR: 1.0e-03', '  Time: 0:14:03')
2024-06-24 14:13:15,626 - ('  Epoch: 4', '  Batch: 250/2198', '  Train Loss: 0.5970', '  LR: 1.0e-03', '  Time: 0:14:08')
2024-06-24 14:13:20,597 - ('  Epoch: 4', '  Batch: 300/2198', '  Train Loss: 0.5966', '  LR: 1.0e-03', '  Time: 0:14:13')
2024-06-24 14:13:25,240 - ('  Epoch: 4', '  Batch: 350/2198', '  Train Loss: 0.5954', '  LR: 1.0e-03', '  Time: 0:14:17')
2024-06-24 14:13:29,562 - ('  Epoch: 4', '  Batch: 400/2198', '  Train Loss: 0.5963', '  LR: 1.0e-03', '  Time: 0:14:22')
2024-06-24 14:13:33,902 - ('  Epoch: 4', '  Batch: 450/2198', '  Train Loss: 0.5949', '  LR: 1.0e-03', '  Time: 0:14:26')
2024-06-24 14:13:38,237 - ('  Epoch: 4', '  Batch: 500/2198', '  Train Loss: 0.5959', '  LR: 1.0e-03', '  Time: 0:14:30')
2024-06-24 14:13:42,872 - ('  Epoch: 4', '  Batch: 550/2198', '  Train Loss: 0.5945', '  LR: 1.0e-03', '  Time: 0:14:35')
2024-06-24 14:13:47,493 - ('  Epoch: 4', '  Batch: 600/2198', '  Train Loss: 0.5966', '  LR: 1.0e-03', '  Time: 0:14:40')
2024-06-24 14:13:52,093 - ('  Epoch: 4', '  Batch: 650/2198', '  Train Loss: 0.5934', '  LR: 1.0e-03', '  Time: 0:14:44')
2024-06-24 14:13:56,365 - ('  Epoch: 4', '  Batch: 700/2198', '  Train Loss: 0.5947', '  LR: 1.0e-03', '  Time: 0:14:48')
2024-06-24 14:14:00,638 - ('  Epoch: 4', '  Batch: 750/2198', '  Train Loss: 0.5941', '  LR: 1.0e-03', '  Time: 0:14:53')
2024-06-24 14:14:04,917 - ('  Epoch: 4', '  Batch: 800/2198', '  Train Loss: 0.5939', '  LR: 1.0e-03', '  Time: 0:14:57')
2024-06-24 14:14:09,208 - ('  Epoch: 4', '  Batch: 850/2198', '  Train Loss: 0.5950', '  LR: 1.0e-03', '  Time: 0:15:01')
2024-06-24 14:14:13,587 - ('  Epoch: 4', '  Batch: 900/2198', '  Train Loss: 0.5939', '  LR: 1.0e-03', '  Time: 0:15:06')
2024-06-24 14:14:18,095 - ('  Epoch: 4', '  Batch: 950/2198', '  Train Loss: 0.5932', '  LR: 1.0e-03', '  Time: 0:15:10')
2024-06-24 14:14:22,593 - ('  Epoch: 4', '  Batch: 1000/2198', '  Train Loss: 0.5940', '  LR: 1.0e-03', '  Time: 0:15:15')
2024-06-24 14:14:27,143 - ('  Epoch: 4', '  Batch: 1050/2198', '  Train Loss: 0.5941', '  LR: 1.0e-03', '  Time: 0:15:19')
2024-06-24 14:14:31,616 - ('  Epoch: 4', '  Batch: 1100/2198', '  Train Loss: 0.5917', '  LR: 1.0e-03', '  Time: 0:15:24')
2024-06-24 14:14:35,961 - ('  Epoch: 4', '  Batch: 1150/2198', '  Train Loss: 0.5932', '  LR: 1.0e-03', '  Time: 0:15:28')
2024-06-24 14:14:40,196 - ('  Epoch: 4', '  Batch: 1200/2198', '  Train Loss: 0.5927', '  LR: 1.0e-03', '  Time: 0:15:32')
2024-06-24 14:14:44,430 - ('  Epoch: 4', '  Batch: 1250/2198', '  Train Loss: 0.5927', '  LR: 1.0e-03', '  Time: 0:15:37')
2024-06-24 14:14:49,063 - ('  Epoch: 4', '  Batch: 1300/2198', '  Train Loss: 0.5926', '  LR: 1.0e-03', '  Time: 0:15:41')
2024-06-24 14:14:54,526 - ('  Epoch: 4', '  Batch: 1350/2198', '  Train Loss: 0.5910', '  LR: 1.0e-03', '  Time: 0:15:47')
2024-06-24 14:15:00,394 - ('  Epoch: 4', '  Batch: 1400/2198', '  Train Loss: 0.5909', '  LR: 1.0e-03', '  Time: 0:15:53')
2024-06-24 14:15:05,466 - ('  Epoch: 4', '  Batch: 1450/2198', '  Train Loss: 0.5929', '  LR: 1.0e-03', '  Time: 0:15:58')
2024-06-24 14:15:09,934 - ('  Epoch: 4', '  Batch: 1500/2198', '  Train Loss: 0.5897', '  LR: 1.0e-03', '  Time: 0:16:02')
2024-06-24 14:15:14,840 - ('  Epoch: 4', '  Batch: 1550/2198', '  Train Loss: 0.5904', '  LR: 1.0e-03', '  Time: 0:16:07')
2024-06-24 14:15:19,559 - ('  Epoch: 4', '  Batch: 1600/2198', '  Train Loss: 0.5915', '  LR: 1.0e-03', '  Time: 0:16:12')
2024-06-24 14:15:23,882 - ('  Epoch: 4', '  Batch: 1650/2198', '  Train Loss: 0.5914', '  LR: 1.0e-03', '  Time: 0:16:16')
2024-06-24 14:15:28,297 - ('  Epoch: 4', '  Batch: 1700/2198', '  Train Loss: 0.5897', '  LR: 1.0e-03', '  Time: 0:16:20')
2024-06-24 14:15:32,577 - ('  Epoch: 4', '  Batch: 1750/2198', '  Train Loss: 0.5896', '  LR: 1.0e-03', '  Time: 0:16:25')
2024-06-24 14:15:37,206 - ('  Epoch: 4', '  Batch: 1800/2198', '  Train Loss: 0.5920', '  LR: 1.0e-03', '  Time: 0:16:29')
2024-06-24 14:15:41,797 - ('  Epoch: 4', '  Batch: 1850/2198', '  Train Loss: 0.5903', '  LR: 1.0e-03', '  Time: 0:16:34')
2024-06-24 14:15:46,363 - ('  Epoch: 4', '  Batch: 1900/2198', '  Train Loss: 0.5905', '  LR: 1.0e-03', '  Time: 0:16:38')
2024-06-24 14:15:50,744 - ('  Epoch: 4', '  Batch: 1950/2198', '  Train Loss: 0.5897', '  LR: 1.0e-03', '  Time: 0:16:43')
2024-06-24 14:15:55,015 - ('  Epoch: 4', '  Batch: 2000/2198', '  Train Loss: 0.5887', '  LR: 1.0e-03', '  Time: 0:16:47')
2024-06-24 14:15:59,285 - ('  Epoch: 4', '  Batch: 2050/2198', '  Train Loss: 0.5905', '  LR: 1.0e-03', '  Time: 0:16:51')
2024-06-24 14:16:03,574 - ('  Epoch: 4', '  Batch: 2100/2198', '  Train Loss: 0.5874', '  LR: 1.0e-03', '  Time: 0:16:56')
2024-06-24 14:16:07,846 - ('  Epoch: 4', '  Batch: 2150/2198', '  Train Loss: 0.5884', '  LR: 1.0e-03', '  Time: 0:17:00')
2024-06-24 14:16:32,152 - 
Epoch: 4  Val Loss: 0.5759  R2 score: 0.3592
2024-06-24 14:16:32,153 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 14:16:36,482 - ('  Epoch: 5', '  Batch: 50/2198', '  Train Loss: 0.5878', '  LR: 1.0e-03', '  Time: 0:17:29')
2024-06-24 14:16:40,740 - ('  Epoch: 5', '  Batch: 100/2198', '  Train Loss: 0.5859', '  LR: 1.0e-03', '  Time: 0:17:33')
2024-06-24 14:16:45,006 - ('  Epoch: 5', '  Batch: 150/2198', '  Train Loss: 0.5854', '  LR: 1.0e-03', '  Time: 0:17:37')
2024-06-24 14:16:49,385 - ('  Epoch: 5', '  Batch: 200/2198', '  Train Loss: 0.5861', '  LR: 1.0e-03', '  Time: 0:17:41')
2024-06-24 14:16:53,650 - ('  Epoch: 5', '  Batch: 250/2198', '  Train Loss: 0.5862', '  LR: 1.0e-03', '  Time: 0:17:46')
2024-06-24 14:16:58,143 - ('  Epoch: 5', '  Batch: 300/2198', '  Train Loss: 0.5859', '  LR: 1.0e-03', '  Time: 0:17:50')
2024-06-24 14:17:02,632 - ('  Epoch: 5', '  Batch: 350/2198', '  Train Loss: 0.5851', '  LR: 1.0e-03', '  Time: 0:17:55')
2024-06-24 14:17:07,179 - ('  Epoch: 5', '  Batch: 400/2198', '  Train Loss: 0.5867', '  LR: 1.0e-03', '  Time: 0:17:59')
2024-06-24 14:17:11,760 - ('  Epoch: 5', '  Batch: 450/2198', '  Train Loss: 0.5866', '  LR: 1.0e-03', '  Time: 0:18:04')
2024-06-24 14:17:16,018 - ('  Epoch: 5', '  Batch: 500/2198', '  Train Loss: 0.5849', '  LR: 1.0e-03', '  Time: 0:18:08')
2024-06-24 14:17:20,278 - ('  Epoch: 5', '  Batch: 550/2198', '  Train Loss: 0.5847', '  LR: 1.0e-03', '  Time: 0:18:12')
2024-06-24 14:17:24,539 - ('  Epoch: 5', '  Batch: 600/2198', '  Train Loss: 0.5844', '  LR: 1.0e-03', '  Time: 0:18:17')
2024-06-24 14:17:28,805 - ('  Epoch: 5', '  Batch: 650/2198', '  Train Loss: 0.5844', '  LR: 1.0e-03', '  Time: 0:18:21')
2024-06-24 14:17:33,082 - ('  Epoch: 5', '  Batch: 700/2198', '  Train Loss: 0.5848', '  LR: 1.0e-03', '  Time: 0:18:25')
2024-06-24 14:17:37,455 - ('  Epoch: 5', '  Batch: 750/2198', '  Train Loss: 0.5854', '  LR: 1.0e-03', '  Time: 0:18:30')
2024-06-24 14:17:41,948 - ('  Epoch: 5', '  Batch: 800/2198', '  Train Loss: 0.5849', '  LR: 1.0e-03', '  Time: 0:18:34')
2024-06-24 14:17:46,438 - ('  Epoch: 5', '  Batch: 850/2198', '  Train Loss: 0.5825', '  LR: 1.0e-03', '  Time: 0:18:39')
2024-06-24 14:17:50,981 - ('  Epoch: 5', '  Batch: 900/2198', '  Train Loss: 0.5842', '  LR: 1.0e-03', '  Time: 0:18:43')
2024-06-24 14:17:55,448 - ('  Epoch: 5', '  Batch: 950/2198', '  Train Loss: 0.5841', '  LR: 1.0e-03', '  Time: 0:18:48')
2024-06-24 14:17:59,831 - ('  Epoch: 5', '  Batch: 1000/2198', '  Train Loss: 0.5853', '  LR: 1.0e-03', '  Time: 0:18:52')
2024-06-24 14:18:04,095 - ('  Epoch: 5', '  Batch: 1050/2198', '  Train Loss: 0.5850', '  LR: 1.0e-03', '  Time: 0:18:56')
2024-06-24 14:18:08,354 - ('  Epoch: 5', '  Batch: 1100/2198', '  Train Loss: 0.5837', '  LR: 1.0e-03', '  Time: 0:19:00')
2024-06-24 14:18:12,620 - ('  Epoch: 5', '  Batch: 1150/2198', '  Train Loss: 0.5821', '  LR: 1.0e-03', '  Time: 0:19:05')
2024-06-24 14:18:16,897 - ('  Epoch: 5', '  Batch: 1200/2198', '  Train Loss: 0.5835', '  LR: 1.0e-03', '  Time: 0:19:09')
2024-06-24 14:18:21,271 - ('  Epoch: 5', '  Batch: 1250/2198', '  Train Loss: 0.5833', '  LR: 1.0e-03', '  Time: 0:19:13')
2024-06-24 14:18:25,755 - ('  Epoch: 5', '  Batch: 1300/2198', '  Train Loss: 0.5832', '  LR: 1.0e-03', '  Time: 0:19:18')
2024-06-24 14:18:30,246 - ('  Epoch: 5', '  Batch: 1350/2198', '  Train Loss: 0.5817', '  LR: 1.0e-03', '  Time: 0:19:22')
2024-06-24 14:18:34,789 - ('  Epoch: 5', '  Batch: 1400/2198', '  Train Loss: 0.5812', '  LR: 1.0e-03', '  Time: 0:19:27')
2024-06-24 14:18:39,257 - ('  Epoch: 5', '  Batch: 1450/2198', '  Train Loss: 0.5823', '  LR: 1.0e-03', '  Time: 0:19:31')
2024-06-24 14:18:43,639 - ('  Epoch: 5', '  Batch: 1500/2198', '  Train Loss: 0.5819', '  LR: 1.0e-03', '  Time: 0:19:36')
2024-06-24 14:18:47,901 - ('  Epoch: 5', '  Batch: 1550/2198', '  Train Loss: 0.5824', '  LR: 1.0e-03', '  Time: 0:19:40')
2024-06-24 14:18:52,162 - ('  Epoch: 5', '  Batch: 1600/2198', '  Train Loss: 0.5822', '  LR: 1.0e-03', '  Time: 0:19:44')
2024-06-24 14:18:56,432 - ('  Epoch: 5', '  Batch: 1650/2198', '  Train Loss: 0.5818', '  LR: 1.0e-03', '  Time: 0:19:49')
2024-06-24 14:19:00,712 - ('  Epoch: 5', '  Batch: 1700/2198', '  Train Loss: 0.5821', '  LR: 1.0e-03', '  Time: 0:19:53')
2024-06-24 14:19:04,979 - ('  Epoch: 5', '  Batch: 1750/2198', '  Train Loss: 0.5828', '  LR: 1.0e-03', '  Time: 0:19:57')
2024-06-24 14:19:09,574 - ('  Epoch: 5', '  Batch: 1800/2198', '  Train Loss: 0.5820', '  LR: 1.0e-03', '  Time: 0:20:02')
2024-06-24 14:19:14,072 - ('  Epoch: 5', '  Batch: 1850/2198', '  Train Loss: 0.5825', '  LR: 1.0e-03', '  Time: 0:20:06')
2024-06-24 14:19:18,619 - ('  Epoch: 5', '  Batch: 1900/2198', '  Train Loss: 0.5792', '  LR: 1.0e-03', '  Time: 0:20:11')
2024-06-24 14:19:23,091 - ('  Epoch: 5', '  Batch: 1950/2198', '  Train Loss: 0.5802', '  LR: 1.0e-03', '  Time: 0:20:15')
2024-06-24 14:19:27,364 - ('  Epoch: 5', '  Batch: 2000/2198', '  Train Loss: 0.5811', '  LR: 1.0e-03', '  Time: 0:20:19')
2024-06-24 14:19:31,729 - ('  Epoch: 5', '  Batch: 2050/2198', '  Train Loss: 0.5819', '  LR: 1.0e-03', '  Time: 0:20:24')
2024-06-24 14:19:35,992 - ('  Epoch: 5', '  Batch: 2100/2198', '  Train Loss: 0.5815', '  LR: 1.0e-03', '  Time: 0:20:28')
2024-06-24 14:19:40,265 - ('  Epoch: 5', '  Batch: 2150/2198', '  Train Loss: 0.5809', '  LR: 1.0e-03', '  Time: 0:20:32')
2024-06-24 14:20:04,573 - 
Epoch: 5  Val Loss: 0.5667  R2 score: 0.3687
2024-06-24 14:20:04,575 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 14:20:09,075 - ('  Epoch: 6', '  Batch: 50/2198', '  Train Loss: 0.5787', '  LR: 1.0e-03', '  Time: 0:21:01')
2024-06-24 14:20:13,433 - ('  Epoch: 6', '  Batch: 100/2198', '  Train Loss: 0.5767', '  LR: 1.0e-03', '  Time: 0:21:06')
2024-06-24 14:20:17,692 - ('  Epoch: 6', '  Batch: 150/2198', '  Train Loss: 0.5774', '  LR: 1.0e-03', '  Time: 0:21:10')
2024-06-24 14:20:21,951 - ('  Epoch: 6', '  Batch: 200/2198', '  Train Loss: 0.5775', '  LR: 1.0e-03', '  Time: 0:21:14')
2024-06-24 14:20:26,228 - ('  Epoch: 6', '  Batch: 250/2198', '  Train Loss: 0.5776', '  LR: 1.0e-03', '  Time: 0:21:18')
2024-06-24 14:20:30,483 - ('  Epoch: 6', '  Batch: 300/2198', '  Train Loss: 0.5770', '  LR: 1.0e-03', '  Time: 0:21:23')
2024-06-24 14:20:34,772 - ('  Epoch: 6', '  Batch: 350/2198', '  Train Loss: 0.5778', '  LR: 1.0e-03', '  Time: 0:21:27')
2024-06-24 14:20:39,429 - ('  Epoch: 6', '  Batch: 400/2198', '  Train Loss: 0.5763', '  LR: 1.0e-03', '  Time: 0:21:32')
2024-06-24 14:20:43,925 - ('  Epoch: 6', '  Batch: 450/2198', '  Train Loss: 0.5773', '  LR: 1.0e-03', '  Time: 0:21:36')
2024-06-24 14:20:48,469 - ('  Epoch: 6', '  Batch: 500/2198', '  Train Loss: 0.5769', '  LR: 1.0e-03', '  Time: 0:21:41')
2024-06-24 14:20:52,847 - ('  Epoch: 6', '  Batch: 550/2198', '  Train Loss: 0.5771', '  LR: 1.0e-03', '  Time: 0:21:45')
2024-06-24 14:20:57,104 - ('  Epoch: 6', '  Batch: 600/2198', '  Train Loss: 0.5763', '  LR: 1.0e-03', '  Time: 0:21:49')
2024-06-24 14:21:01,469 - ('  Epoch: 6', '  Batch: 650/2198', '  Train Loss: 0.5777', '  LR: 1.0e-03', '  Time: 0:21:54')
2024-06-24 14:21:05,730 - ('  Epoch: 6', '  Batch: 700/2198', '  Train Loss: 0.5773', '  LR: 1.0e-03', '  Time: 0:21:58')
2024-06-24 14:21:09,997 - ('  Epoch: 6', '  Batch: 750/2198', '  Train Loss: 0.5770', '  LR: 1.0e-03', '  Time: 0:22:02')
2024-06-24 14:21:14,271 - ('  Epoch: 6', '  Batch: 800/2198', '  Train Loss: 0.5781', '  LR: 1.0e-03', '  Time: 0:22:06')
2024-06-24 14:21:18,538 - ('  Epoch: 6', '  Batch: 850/2198', '  Train Loss: 0.5763', '  LR: 1.0e-03', '  Time: 0:22:11')
2024-06-24 14:21:23,183 - ('  Epoch: 6', '  Batch: 900/2198', '  Train Loss: 0.5758', '  LR: 1.0e-03', '  Time: 0:22:15')
2024-06-24 14:21:27,704 - ('  Epoch: 6', '  Batch: 950/2198', '  Train Loss: 0.5772', '  LR: 1.0e-03', '  Time: 0:22:20')
2024-06-24 14:21:32,211 - ('  Epoch: 6', '  Batch: 1000/2198', '  Train Loss: 0.5764', '  LR: 1.0e-03', '  Time: 0:22:24')
2024-06-24 14:21:36,636 - ('  Epoch: 6', '  Batch: 1050/2198', '  Train Loss: 0.5763', '  LR: 1.0e-03', '  Time: 0:22:29')
2024-06-24 14:21:40,904 - ('  Epoch: 6', '  Batch: 1100/2198', '  Train Loss: 0.5762', '  LR: 1.0e-03', '  Time: 0:22:33')
2024-06-24 14:21:45,272 - ('  Epoch: 6', '  Batch: 1150/2198', '  Train Loss: 0.5756', '  LR: 1.0e-03', '  Time: 0:22:37')
2024-06-24 14:21:49,531 - ('  Epoch: 6', '  Batch: 1200/2198', '  Train Loss: 0.5761', '  LR: 1.0e-03', '  Time: 0:22:42')
2024-06-24 14:21:53,803 - ('  Epoch: 6', '  Batch: 1250/2198', '  Train Loss: 0.5750', '  LR: 1.0e-03', '  Time: 0:22:46')
2024-06-24 14:21:58,079 - ('  Epoch: 6', '  Batch: 1300/2198', '  Train Loss: 0.5748', '  LR: 1.0e-03', '  Time: 0:22:50')
2024-06-24 14:22:02,344 - ('  Epoch: 6', '  Batch: 1350/2198', '  Train Loss: 0.5765', '  LR: 1.0e-03', '  Time: 0:22:54')
2024-06-24 14:22:06,883 - ('  Epoch: 6', '  Batch: 1400/2198', '  Train Loss: 0.5754', '  LR: 1.0e-03', '  Time: 0:22:59')
2024-06-24 14:22:11,508 - ('  Epoch: 6', '  Batch: 1450/2198', '  Train Loss: 0.5746', '  LR: 1.0e-03', '  Time: 0:23:04')
2024-06-24 14:22:16,025 - ('  Epoch: 6', '  Batch: 1500/2198', '  Train Loss: 0.5762', '  LR: 1.0e-03', '  Time: 0:23:08')
2024-06-24 14:22:20,452 - ('  Epoch: 6', '  Batch: 1550/2198', '  Train Loss: 0.5756', '  LR: 1.0e-03', '  Time: 0:23:13')
2024-06-24 14:22:24,721 - ('  Epoch: 6', '  Batch: 1600/2198', '  Train Loss: 0.5749', '  LR: 1.0e-03', '  Time: 0:23:17')
2024-06-24 14:22:28,981 - ('  Epoch: 6', '  Batch: 1650/2198', '  Train Loss: 0.5744', '  LR: 1.0e-03', '  Time: 0:23:21')
2024-06-24 14:22:33,354 - ('  Epoch: 6', '  Batch: 1700/2198', '  Train Loss: 0.5759', '  LR: 1.0e-03', '  Time: 0:23:25')
2024-06-24 14:22:37,622 - ('  Epoch: 6', '  Batch: 1750/2198', '  Train Loss: 0.5758', '  LR: 1.0e-03', '  Time: 0:23:30')
2024-06-24 14:22:41,894 - ('  Epoch: 6', '  Batch: 1800/2198', '  Train Loss: 0.5736', '  LR: 1.0e-03', '  Time: 0:23:34')
2024-06-24 14:22:46,181 - ('  Epoch: 6', '  Batch: 1850/2198', '  Train Loss: 0.5749', '  LR: 1.0e-03', '  Time: 0:23:38')
2024-06-24 14:22:50,723 - ('  Epoch: 6', '  Batch: 1900/2198', '  Train Loss: 0.5726', '  LR: 1.0e-03', '  Time: 0:23:43')
2024-06-24 14:22:55,329 - ('  Epoch: 6', '  Batch: 1950/2198', '  Train Loss: 0.5734', '  LR: 1.0e-03', '  Time: 0:23:47')
2024-06-24 14:22:59,868 - ('  Epoch: 6', '  Batch: 2000/2198', '  Train Loss: 0.5740', '  LR: 1.0e-03', '  Time: 0:23:52')
2024-06-24 14:23:04,258 - ('  Epoch: 6', '  Batch: 2050/2198', '  Train Loss: 0.5740', '  LR: 1.0e-03', '  Time: 0:23:56')
2024-06-24 14:23:08,515 - ('  Epoch: 6', '  Batch: 2100/2198', '  Train Loss: 0.5756', '  LR: 1.0e-03', '  Time: 0:24:01')
2024-06-24 14:23:12,774 - ('  Epoch: 6', '  Batch: 2150/2198', '  Train Loss: 0.5739', '  LR: 1.0e-03', '  Time: 0:24:05')
2024-06-24 14:23:36,702 - 
Epoch: 6  Val Loss: 0.5589  R2 score: 0.3789
2024-06-24 14:23:36,704 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 14:23:41,317 - ('  Epoch: 7', '  Batch: 50/2198', '  Train Loss: 0.5714', '  LR: 1.0e-03', '  Time: 0:24:33')
2024-06-24 14:23:45,946 - ('  Epoch: 7', '  Batch: 100/2198', '  Train Loss: 0.5708', '  LR: 1.0e-03', '  Time: 0:24:38')
2024-06-24 14:23:50,232 - ('  Epoch: 7', '  Batch: 150/2198', '  Train Loss: 0.5703', '  LR: 1.0e-03', '  Time: 0:24:42')
2024-06-24 14:23:54,495 - ('  Epoch: 7', '  Batch: 200/2198', '  Train Loss: 0.5713', '  LR: 1.0e-03', '  Time: 0:24:47')
2024-06-24 14:23:58,755 - ('  Epoch: 7', '  Batch: 250/2198', '  Train Loss: 0.5713', '  LR: 1.0e-03', '  Time: 0:24:51')
2024-06-24 14:24:03,025 - ('  Epoch: 7', '  Batch: 300/2198', '  Train Loss: 0.5711', '  LR: 1.0e-03', '  Time: 0:24:55')
2024-06-24 14:24:07,416 - ('  Epoch: 7', '  Batch: 350/2198', '  Train Loss: 0.5718', '  LR: 1.0e-03', '  Time: 0:25:00')
2024-06-24 14:24:11,670 - ('  Epoch: 7', '  Batch: 400/2198', '  Train Loss: 0.5701', '  LR: 1.0e-03', '  Time: 0:25:04')
2024-06-24 14:24:15,970 - ('  Epoch: 7', '  Batch: 450/2198', '  Train Loss: 0.5692', '  LR: 1.0e-03', '  Time: 0:25:08')
2024-06-24 14:24:20,522 - ('  Epoch: 7', '  Batch: 500/2198', '  Train Loss: 0.5702', '  LR: 1.0e-03', '  Time: 0:25:13')
2024-06-24 14:24:25,005 - ('  Epoch: 7', '  Batch: 550/2198', '  Train Loss: 0.5715', '  LR: 1.0e-03', '  Time: 0:25:17')
2024-06-24 14:24:29,534 - ('  Epoch: 7', '  Batch: 600/2198', '  Train Loss: 0.5694', '  LR: 1.0e-03', '  Time: 0:25:22')
2024-06-24 14:24:34,033 - ('  Epoch: 7', '  Batch: 650/2198', '  Train Loss: 0.5710', '  LR: 1.0e-03', '  Time: 0:25:26')
2024-06-24 14:24:38,288 - ('  Epoch: 7', '  Batch: 700/2198', '  Train Loss: 0.5710', '  LR: 1.0e-03', '  Time: 0:25:30')
2024-06-24 14:24:42,545 - ('  Epoch: 7', '  Batch: 750/2198', '  Train Loss: 0.5701', '  LR: 1.0e-03', '  Time: 0:25:35')
2024-06-24 14:24:46,806 - ('  Epoch: 7', '  Batch: 800/2198', '  Train Loss: 0.5683', '  LR: 1.0e-03', '  Time: 0:25:39')
2024-06-24 14:24:51,074 - ('  Epoch: 7', '  Batch: 850/2198', '  Train Loss: 0.5712', '  LR: 1.0e-03', '  Time: 0:25:43')
2024-06-24 14:24:55,448 - ('  Epoch: 7', '  Batch: 900/2198', '  Train Loss: 0.5682', '  LR: 1.0e-03', '  Time: 0:25:48')
2024-06-24 14:24:59,740 - ('  Epoch: 7', '  Batch: 950/2198', '  Train Loss: 0.5700', '  LR: 1.0e-03', '  Time: 0:25:52')
2024-06-24 14:25:04,296 - ('  Epoch: 7', '  Batch: 1000/2198', '  Train Loss: 0.5711', '  LR: 1.0e-03', '  Time: 0:25:56')
2024-06-24 14:25:08,779 - ('  Epoch: 7', '  Batch: 1050/2198', '  Train Loss: 0.5683', '  LR: 1.0e-03', '  Time: 0:26:01')
2024-06-24 14:25:13,313 - ('  Epoch: 7', '  Batch: 1100/2198', '  Train Loss: 0.5685', '  LR: 1.0e-03', '  Time: 0:26:05')
2024-06-24 14:25:17,819 - ('  Epoch: 7', '  Batch: 1150/2198', '  Train Loss: 0.5704', '  LR: 1.0e-03', '  Time: 0:26:10')
2024-06-24 14:25:22,072 - ('  Epoch: 7', '  Batch: 1200/2198', '  Train Loss: 0.5700', '  LR: 1.0e-03', '  Time: 0:26:14')
2024-06-24 14:25:26,331 - ('  Epoch: 7', '  Batch: 1250/2198', '  Train Loss: 0.5690', '  LR: 1.0e-03', '  Time: 0:26:18')
2024-06-24 14:25:30,599 - ('  Epoch: 7', '  Batch: 1300/2198', '  Train Loss: 0.5697', '  LR: 1.0e-03', '  Time: 0:26:23')
2024-06-24 14:25:34,865 - ('  Epoch: 7', '  Batch: 1350/2198', '  Train Loss: 0.5689', '  LR: 1.0e-03', '  Time: 0:26:27')
2024-06-24 14:25:39,241 - ('  Epoch: 7', '  Batch: 1400/2198', '  Train Loss: 0.5694', '  LR: 1.0e-03', '  Time: 0:26:31')
2024-06-24 14:25:43,529 - ('  Epoch: 7', '  Batch: 1450/2198', '  Train Loss: 0.5694', '  LR: 1.0e-03', '  Time: 0:26:36')
2024-06-24 14:25:48,081 - ('  Epoch: 7', '  Batch: 1500/2198', '  Train Loss: 0.5711', '  LR: 1.0e-03', '  Time: 0:26:40')
2024-06-24 14:25:52,566 - ('  Epoch: 7', '  Batch: 1550/2198', '  Train Loss: 0.5687', '  LR: 1.0e-03', '  Time: 0:26:45')
2024-06-24 14:25:57,097 - ('  Epoch: 7', '  Batch: 1600/2198', '  Train Loss: 0.5687', '  LR: 1.0e-03', '  Time: 0:26:49')
2024-06-24 14:26:01,481 - ('  Epoch: 7', '  Batch: 1650/2198', '  Train Loss: 0.5687', '  LR: 1.0e-03', '  Time: 0:26:54')
2024-06-24 14:26:05,847 - ('  Epoch: 7', '  Batch: 1700/2198', '  Train Loss: 0.5699', '  LR: 1.0e-03', '  Time: 0:26:58')
2024-06-24 14:26:10,104 - ('  Epoch: 7', '  Batch: 1750/2198', '  Train Loss: 0.5695', '  LR: 1.0e-03', '  Time: 0:27:02')
2024-06-24 14:26:14,362 - ('  Epoch: 7', '  Batch: 1800/2198', '  Train Loss: 0.5679', '  LR: 1.0e-03', '  Time: 0:27:06')
2024-06-24 14:26:18,629 - ('  Epoch: 7', '  Batch: 1850/2198', '  Train Loss: 0.5688', '  LR: 1.0e-03', '  Time: 0:27:11')
2024-06-24 14:26:22,901 - ('  Epoch: 7', '  Batch: 1900/2198', '  Train Loss: 0.5702', '  LR: 1.0e-03', '  Time: 0:27:15')
2024-06-24 14:26:27,291 - ('  Epoch: 7', '  Batch: 1950/2198', '  Train Loss: 0.5678', '  LR: 1.0e-03', '  Time: 0:27:19')
2024-06-24 14:26:31,838 - ('  Epoch: 7', '  Batch: 2000/2198', '  Train Loss: 0.5685', '  LR: 1.0e-03', '  Time: 0:27:24')
2024-06-24 14:26:36,336 - ('  Epoch: 7', '  Batch: 2050/2198', '  Train Loss: 0.5685', '  LR: 1.0e-03', '  Time: 0:27:28')
2024-06-24 14:26:40,856 - ('  Epoch: 7', '  Batch: 2100/2198', '  Train Loss: 0.5687', '  LR: 1.0e-03', '  Time: 0:27:33')
2024-06-24 14:26:45,252 - ('  Epoch: 7', '  Batch: 2150/2198', '  Train Loss: 0.5693', '  LR: 1.0e-03', '  Time: 0:27:37')
2024-06-24 14:27:08,594 - 
Epoch: 7  Val Loss: 0.5529  R2 score: 0.3851
2024-06-24 14:27:08,596 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 14:27:13,103 - ('  Epoch: 8', '  Batch: 50/2198', '  Train Loss: 0.5649', '  LR: 1.0e-03', '  Time: 0:28:05')
2024-06-24 14:27:17,624 - ('  Epoch: 8', '  Batch: 100/2198', '  Train Loss: 0.5650', '  LR: 1.0e-03', '  Time: 0:28:10')
2024-06-24 14:27:22,157 - ('  Epoch: 8', '  Batch: 150/2198', '  Train Loss: 0.5647', '  LR: 1.0e-03', '  Time: 0:28:14')
2024-06-24 14:27:26,766 - ('  Epoch: 8', '  Batch: 200/2198', '  Train Loss: 0.5644', '  LR: 1.0e-03', '  Time: 0:28:19')
2024-06-24 14:27:31,033 - ('  Epoch: 8', '  Batch: 250/2198', '  Train Loss: 0.5650', '  LR: 1.0e-03', '  Time: 0:28:23')
2024-06-24 14:27:35,287 - ('  Epoch: 8', '  Batch: 300/2198', '  Train Loss: 0.5637', '  LR: 1.0e-03', '  Time: 0:28:27')
2024-06-24 14:27:39,543 - ('  Epoch: 8', '  Batch: 350/2198', '  Train Loss: 0.5668', '  LR: 1.0e-03', '  Time: 0:28:32')
2024-06-24 14:27:43,799 - ('  Epoch: 8', '  Batch: 400/2198', '  Train Loss: 0.5670', '  LR: 1.0e-03', '  Time: 0:28:36')
2024-06-24 14:27:48,183 - ('  Epoch: 8', '  Batch: 450/2198', '  Train Loss: 0.5638', '  LR: 1.0e-03', '  Time: 0:28:40')
2024-06-24 14:27:52,442 - ('  Epoch: 8', '  Batch: 500/2198', '  Train Loss: 0.5646', '  LR: 1.0e-03', '  Time: 0:28:45')
2024-06-24 14:27:56,776 - ('  Epoch: 8', '  Batch: 550/2198', '  Train Loss: 0.5658', '  LR: 1.0e-03', '  Time: 0:28:49')
2024-06-24 14:28:01,341 - ('  Epoch: 8', '  Batch: 600/2198', '  Train Loss: 0.5655', '  LR: 1.0e-03', '  Time: 0:28:53')
2024-06-24 14:28:05,829 - ('  Epoch: 8', '  Batch: 650/2198', '  Train Loss: 0.5650', '  LR: 1.0e-03', '  Time: 0:28:58')
2024-06-24 14:28:10,383 - ('  Epoch: 8', '  Batch: 700/2198', '  Train Loss: 0.5642', '  LR: 1.0e-03', '  Time: 0:29:02')
2024-06-24 14:28:14,805 - ('  Epoch: 8', '  Batch: 750/2198', '  Train Loss: 0.5643', '  LR: 1.0e-03', '  Time: 0:29:07')
2024-06-24 14:28:19,061 - ('  Epoch: 8', '  Batch: 800/2198', '  Train Loss: 0.5655', '  LR: 1.0e-03', '  Time: 0:29:11')
2024-06-24 14:28:23,315 - ('  Epoch: 8', '  Batch: 850/2198', '  Train Loss: 0.5648', '  LR: 1.0e-03', '  Time: 0:29:15')
2024-06-24 14:28:27,576 - ('  Epoch: 8', '  Batch: 900/2198', '  Train Loss: 0.5640', '  LR: 1.0e-03', '  Time: 0:29:20')
2024-06-24 14:28:31,844 - ('  Epoch: 8', '  Batch: 950/2198', '  Train Loss: 0.5657', '  LR: 1.0e-03', '  Time: 0:29:24')
2024-06-24 14:28:36,229 - ('  Epoch: 8', '  Batch: 1000/2198', '  Train Loss: 0.5643', '  LR: 1.0e-03', '  Time: 0:29:28')
2024-06-24 14:28:40,542 - ('  Epoch: 8', '  Batch: 1050/2198', '  Train Loss: 0.5644', '  LR: 1.0e-03', '  Time: 0:29:33')
2024-06-24 14:28:45,116 - ('  Epoch: 8', '  Batch: 1100/2198', '  Train Loss: 0.5638', '  LR: 1.0e-03', '  Time: 0:29:37')
2024-06-24 14:28:49,593 - ('  Epoch: 8', '  Batch: 1150/2198', '  Train Loss: 0.5650', '  LR: 1.0e-03', '  Time: 0:29:42')
2024-06-24 14:28:54,147 - ('  Epoch: 8', '  Batch: 1200/2198', '  Train Loss: 0.5642', '  LR: 1.0e-03', '  Time: 0:29:46')
2024-06-24 14:28:58,592 - ('  Epoch: 8', '  Batch: 1250/2198', '  Train Loss: 0.5626', '  LR: 1.0e-03', '  Time: 0:29:51')
2024-06-24 14:29:02,846 - ('  Epoch: 8', '  Batch: 1300/2198', '  Train Loss: 0.5643', '  LR: 1.0e-03', '  Time: 0:29:55')
2024-06-24 14:29:07,105 - ('  Epoch: 8', '  Batch: 1350/2198', '  Train Loss: 0.5639', '  LR: 1.0e-03', '  Time: 0:29:59')
2024-06-24 14:29:11,368 - ('  Epoch: 8', '  Batch: 1400/2198', '  Train Loss: 0.5654', '  LR: 1.0e-03', '  Time: 0:30:03')
2024-06-24 14:29:15,632 - ('  Epoch: 8', '  Batch: 1450/2198', '  Train Loss: 0.5643', '  LR: 1.0e-03', '  Time: 0:30:08')
2024-06-24 14:29:20,026 - ('  Epoch: 8', '  Batch: 1500/2198', '  Train Loss: 0.5651', '  LR: 1.0e-03', '  Time: 0:30:12')
2024-06-24 14:29:24,322 - ('  Epoch: 8', '  Batch: 1550/2198', '  Train Loss: 0.5651', '  LR: 1.0e-03', '  Time: 0:30:16')
2024-06-24 14:29:28,905 - ('  Epoch: 8', '  Batch: 1600/2198', '  Train Loss: 0.5639', '  LR: 1.0e-03', '  Time: 0:30:21')
2024-06-24 14:29:33,380 - ('  Epoch: 8', '  Batch: 1650/2198', '  Train Loss: 0.5638', '  LR: 1.0e-03', '  Time: 0:30:25')
2024-06-24 14:29:37,930 - ('  Epoch: 8', '  Batch: 1700/2198', '  Train Loss: 0.5655', '  LR: 1.0e-03', '  Time: 0:30:30')
2024-06-24 14:29:42,264 - ('  Epoch: 8', '  Batch: 1750/2198', '  Train Loss: 0.5622', '  LR: 1.0e-03', '  Time: 0:30:34')
2024-06-24 14:29:46,630 - ('  Epoch: 8', '  Batch: 1800/2198', '  Train Loss: 0.5639', '  LR: 1.0e-03', '  Time: 0:30:39')
2024-06-24 14:29:50,886 - ('  Epoch: 8', '  Batch: 1850/2198', '  Train Loss: 0.5662', '  LR: 1.0e-03', '  Time: 0:30:43')
2024-06-24 14:29:55,148 - ('  Epoch: 8', '  Batch: 1900/2198', '  Train Loss: 0.5632', '  LR: 1.0e-03', '  Time: 0:30:47')
2024-06-24 14:29:59,407 - ('  Epoch: 8', '  Batch: 1950/2198', '  Train Loss: 0.5642', '  LR: 1.0e-03', '  Time: 0:30:52')
2024-06-24 14:30:03,684 - ('  Epoch: 8', '  Batch: 2000/2198', '  Train Loss: 0.5646', '  LR: 1.0e-03', '  Time: 0:30:56')
2024-06-24 14:30:08,088 - ('  Epoch: 8', '  Batch: 2050/2198', '  Train Loss: 0.5637', '  LR: 1.0e-03', '  Time: 0:31:00')
2024-06-24 14:30:12,666 - ('  Epoch: 8', '  Batch: 2100/2198', '  Train Loss: 0.5630', '  LR: 1.0e-03', '  Time: 0:31:05')
2024-06-24 14:30:17,145 - ('  Epoch: 8', '  Batch: 2150/2198', '  Train Loss: 0.5649', '  LR: 1.0e-03', '  Time: 0:31:09')
2024-06-24 14:30:40,910 - 
Epoch: 8  Val Loss: 0.5493  R2 score: 0.3867
2024-06-24 14:30:40,911 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 14:30:45,255 - ('  Epoch: 9', '  Batch: 50/2198', '  Train Loss: 0.5614', '  LR: 1.0e-03', '  Time: 0:31:37')
2024-06-24 14:30:49,620 - ('  Epoch: 9', '  Batch: 100/2198', '  Train Loss: 0.5585', '  LR: 1.0e-03', '  Time: 0:31:42')
2024-06-24 14:30:54,016 - ('  Epoch: 9', '  Batch: 150/2198', '  Train Loss: 0.5602', '  LR: 1.0e-03', '  Time: 0:31:46')
2024-06-24 14:30:58,522 - ('  Epoch: 9', '  Batch: 200/2198', '  Train Loss: 0.5595', '  LR: 1.0e-03', '  Time: 0:31:51')
2024-06-24 14:31:03,057 - ('  Epoch: 9', '  Batch: 250/2198', '  Train Loss: 0.5597', '  LR: 1.0e-03', '  Time: 0:31:55')
2024-06-24 14:31:07,581 - ('  Epoch: 9', '  Batch: 300/2198', '  Train Loss: 0.5608', '  LR: 1.0e-03', '  Time: 0:32:00')
2024-06-24 14:31:11,856 - ('  Epoch: 9', '  Batch: 350/2198', '  Train Loss: 0.5611', '  LR: 1.0e-03', '  Time: 0:32:04')
2024-06-24 14:31:16,219 - ('  Epoch: 9', '  Batch: 400/2198', '  Train Loss: 0.5613', '  LR: 1.0e-03', '  Time: 0:32:08')
2024-06-24 14:31:20,473 - ('  Epoch: 9', '  Batch: 450/2198', '  Train Loss: 0.5612', '  LR: 1.0e-03', '  Time: 0:32:13')
2024-06-24 14:31:24,731 - ('  Epoch: 9', '  Batch: 500/2198', '  Train Loss: 0.5620', '  LR: 1.0e-03', '  Time: 0:32:17')
2024-06-24 14:31:29,002 - ('  Epoch: 9', '  Batch: 550/2198', '  Train Loss: 0.5600', '  LR: 1.0e-03', '  Time: 0:32:21')
2024-06-24 14:31:33,258 - ('  Epoch: 9', '  Batch: 600/2198', '  Train Loss: 0.5602', '  LR: 1.0e-03', '  Time: 0:32:25')
2024-06-24 14:31:37,744 - ('  Epoch: 9', '  Batch: 650/2198', '  Train Loss: 0.5605', '  LR: 1.0e-03', '  Time: 0:32:30')
2024-06-24 14:31:42,263 - ('  Epoch: 9', '  Batch: 700/2198', '  Train Loss: 0.5611', '  LR: 1.0e-03', '  Time: 0:32:34')
2024-06-24 14:31:46,790 - ('  Epoch: 9', '  Batch: 750/2198', '  Train Loss: 0.5608', '  LR: 1.0e-03', '  Time: 0:32:39')
2024-06-24 14:31:51,300 - ('  Epoch: 9', '  Batch: 800/2198', '  Train Loss: 0.5603', '  LR: 1.0e-03', '  Time: 0:32:43')
2024-06-24 14:31:55,569 - ('  Epoch: 9', '  Batch: 850/2198', '  Train Loss: 0.5613', '  LR: 1.0e-03', '  Time: 0:32:48')
2024-06-24 14:31:59,920 - ('  Epoch: 9', '  Batch: 900/2198', '  Train Loss: 0.5593', '  LR: 1.0e-03', '  Time: 0:32:52')
2024-06-24 14:32:04,170 - ('  Epoch: 9', '  Batch: 950/2198', '  Train Loss: 0.5602', '  LR: 1.0e-03', '  Time: 0:32:56')
2024-06-24 14:32:08,423 - ('  Epoch: 9', '  Batch: 1000/2198', '  Train Loss: 0.5605', '  LR: 1.0e-03', '  Time: 0:33:01')
2024-06-24 14:32:12,695 - ('  Epoch: 9', '  Batch: 1050/2198', '  Train Loss: 0.5597', '  LR: 1.0e-03', '  Time: 0:33:05')
2024-06-24 14:32:16,954 - ('  Epoch: 9', '  Batch: 1100/2198', '  Train Loss: 0.5596', '  LR: 1.0e-03', '  Time: 0:33:09')
2024-06-24 14:32:21,468 - ('  Epoch: 9', '  Batch: 1150/2198', '  Train Loss: 0.5592', '  LR: 1.0e-03', '  Time: 0:33:14')
2024-06-24 14:32:25,982 - ('  Epoch: 9', '  Batch: 1200/2198', '  Train Loss: 0.5617', '  LR: 1.0e-03', '  Time: 0:33:18')
2024-06-24 14:32:30,506 - ('  Epoch: 9', '  Batch: 1250/2198', '  Train Loss: 0.5607', '  LR: 1.0e-03', '  Time: 0:33:23')
2024-06-24 14:32:34,996 - ('  Epoch: 9', '  Batch: 1300/2198', '  Train Loss: 0.5605', '  LR: 1.0e-03', '  Time: 0:33:27')
2024-06-24 14:32:39,262 - ('  Epoch: 9', '  Batch: 1350/2198', '  Train Loss: 0.5604', '  LR: 1.0e-03', '  Time: 0:33:31')
2024-06-24 14:32:43,513 - ('  Epoch: 9', '  Batch: 1400/2198', '  Train Loss: 0.5598', '  LR: 1.0e-03', '  Time: 0:33:36')
2024-06-24 14:32:47,871 - ('  Epoch: 9', '  Batch: 1450/2198', '  Train Loss: 0.5613', '  LR: 1.0e-03', '  Time: 0:33:40')
2024-06-24 14:32:52,126 - ('  Epoch: 9', '  Batch: 1500/2198', '  Train Loss: 0.5605', '  LR: 1.0e-03', '  Time: 0:33:44')
2024-06-24 14:32:56,401 - ('  Epoch: 9', '  Batch: 1550/2198', '  Train Loss: 0.5600', '  LR: 1.0e-03', '  Time: 0:33:49')
2024-06-24 14:33:00,654 - ('  Epoch: 9', '  Batch: 1600/2198', '  Train Loss: 0.5589', '  LR: 1.0e-03', '  Time: 0:33:53')
2024-06-24 14:33:05,036 - ('  Epoch: 9', '  Batch: 1650/2198', '  Train Loss: 0.5608', '  LR: 1.0e-03', '  Time: 0:33:57')
2024-06-24 14:33:09,660 - ('  Epoch: 9', '  Batch: 1700/2198', '  Train Loss: 0.5584', '  LR: 1.0e-03', '  Time: 0:34:02')
2024-06-24 14:33:14,185 - ('  Epoch: 9', '  Batch: 1750/2198', '  Train Loss: 0.5605', '  LR: 1.0e-03', '  Time: 0:34:06')
2024-06-24 14:33:18,695 - ('  Epoch: 9', '  Batch: 1800/2198', '  Train Loss: 0.5595', '  LR: 1.0e-03', '  Time: 0:34:11')
2024-06-24 14:33:22,965 - ('  Epoch: 9', '  Batch: 1850/2198', '  Train Loss: 0.5618', '  LR: 1.0e-03', '  Time: 0:34:15')
2024-06-24 14:33:27,212 - ('  Epoch: 9', '  Batch: 1900/2198', '  Train Loss: 0.5600', '  LR: 1.0e-03', '  Time: 0:34:19')
2024-06-24 14:33:31,565 - ('  Epoch: 9', '  Batch: 1950/2198', '  Train Loss: 0.5596', '  LR: 1.0e-03', '  Time: 0:34:24')
2024-06-24 14:33:35,819 - ('  Epoch: 9', '  Batch: 2000/2198', '  Train Loss: 0.5579', '  LR: 1.0e-03', '  Time: 0:34:28')
2024-06-24 14:33:40,092 - ('  Epoch: 9', '  Batch: 2050/2198', '  Train Loss: 0.5602', '  LR: 1.0e-03', '  Time: 0:34:32')
2024-06-24 14:33:44,343 - ('  Epoch: 9', '  Batch: 2100/2198', '  Train Loss: 0.5597', '  LR: 1.0e-03', '  Time: 0:34:36')
2024-06-24 14:33:48,715 - ('  Epoch: 9', '  Batch: 2150/2198', '  Train Loss: 0.5585', '  LR: 1.0e-03', '  Time: 0:34:41')
2024-06-24 14:34:12,980 - 
Epoch: 9  Val Loss: 0.5442  R2 score: 0.3949
2024-06-24 14:34:12,982 - Validation loss decreased, saving new best model and resetting patience counter.
2024-06-24 14:34:17,386 - ('  Epoch: 10', '  Batch: 50/2198', '  Train Loss: 0.5571', '  LR: 1.0e-03', '  Time: 0:35:09')
2024-06-24 14:34:21,764 - ('  Epoch: 10', '  Batch: 100/2198', '  Train Loss: 0.5552', '  LR: 1.0e-03', '  Time: 0:35:14')
2024-06-24 14:34:26,003 - ('  Epoch: 10', '  Batch: 150/2198', '  Train Loss: 0.5574', '  LR: 1.0e-03', '  Time: 0:35:18')
2024-06-24 14:34:30,259 - ('  Epoch: 10', '  Batch: 200/2198', '  Train Loss: 0.5558', '  LR: 1.0e-03', '  Time: 0:35:22')
2024-06-24 14:34:34,731 - ('  Epoch: 10', '  Batch: 250/2198', '  Train Loss: 0.5575', '  LR: 1.0e-03', '  Time: 0:35:27')
2024-06-24 14:34:39,216 - ('  Epoch: 10', '  Batch: 300/2198', '  Train Loss: 0.5572', '  LR: 1.0e-03', '  Time: 0:35:31')
2024-06-24 14:34:43,844 - ('  Epoch: 10', '  Batch: 350/2198', '  Train Loss: 0.5561', '  LR: 1.0e-03', '  Time: 0:35:36')
2024-06-24 14:34:48,312 - ('  Epoch: 10', '  Batch: 400/2198', '  Train Loss: 0.5547', '  LR: 1.0e-03', '  Time: 0:35:40')
2024-06-24 14:34:52,558 - ('  Epoch: 10', '  Batch: 450/2198', '  Train Loss: 0.5566', '  LR: 1.0e-03', '  Time: 0:35:45')
2024-06-24 14:34:56,804 - ('  Epoch: 10', '  Batch: 500/2198', '  Train Loss: 0.5566', '  LR: 1.0e-03', '  Time: 0:35:49')
2024-06-24 14:35:01,053 - ('  Epoch: 10', '  Batch: 550/2198', '  Train Loss: 0.5566', '  LR: 1.0e-03', '  Time: 0:35:53')
2024-06-24 14:35:05,301 - ('  Epoch: 10', '  Batch: 600/2198', '  Train Loss: 0.5555', '  LR: 1.0e-03', '  Time: 0:35:57')
2024-06-24 14:35:09,665 - ('  Epoch: 10', '  Batch: 650/2198', '  Train Loss: 0.5561', '  LR: 1.0e-03', '  Time: 0:36:02')
2024-06-24 14:35:13,917 - ('  Epoch: 10', '  Batch: 700/2198', '  Train Loss: 0.5581', '  LR: 1.0e-03', '  Time: 0:36:06')
2024-06-24 14:35:18,380 - ('  Epoch: 10', '  Batch: 750/2198', '  Train Loss: 0.5570', '  LR: 1.0e-03', '  Time: 0:36:10')
2024-06-24 14:35:22,868 - ('  Epoch: 10', '  Batch: 800/2198', '  Train Loss: 0.5567', '  LR: 1.0e-03', '  Time: 0:36:15')
2024-06-24 14:35:27,389 - ('  Epoch: 10', '  Batch: 850/2198', '  Train Loss: 0.5573', '  LR: 1.0e-03', '  Time: 0:36:19')
2024-06-24 14:35:31,964 - ('  Epoch: 10', '  Batch: 900/2198', '  Train Loss: 0.5581', '  LR: 1.0e-03', '  Time: 0:36:24')
2024-06-24 14:35:36,214 - ('  Epoch: 10', '  Batch: 950/2198', '  Train Loss: 0.5568', '  LR: 1.0e-03', '  Time: 0:36:28')
2024-06-24 14:35:40,457 - ('  Epoch: 10', '  Batch: 1000/2198', '  Train Loss: 0.5558', '  LR: 1.0e-03', '  Time: 0:36:33')
2024-06-24 14:35:44,707 - ('  Epoch: 10', '  Batch: 1050/2198', '  Train Loss: 0.5562', '  LR: 1.0e-03', '  Time: 0:36:37')
2024-06-24 14:35:48,957 - ('  Epoch: 10', '  Batch: 1100/2198', '  Train Loss: 0.5553', '  LR: 1.0e-03', '  Time: 0:36:41')
2024-06-24 14:35:53,323 - ('  Epoch: 10', '  Batch: 1150/2198', '  Train Loss: 0.5574', '  LR: 1.0e-03', '  Time: 0:36:45')
2024-06-24 14:35:57,575 - ('  Epoch: 10', '  Batch: 1200/2198', '  Train Loss: 0.5572', '  LR: 1.0e-03', '  Time: 0:36:50')
2024-06-24 14:36:02,004 - ('  Epoch: 10', '  Batch: 1250/2198', '  Train Loss: 0.5576', '  LR: 1.0e-03', '  Time: 0:36:54')
2024-06-24 14:36:06,531 - ('  Epoch: 10', '  Batch: 1300/2198', '  Train Loss: 0.5578', '  LR: 1.0e-03', '  Time: 0:36:59')
2024-06-24 14:36:11,045 - ('  Epoch: 10', '  Batch: 1350/2198', '  Train Loss: 0.5565', '  LR: 1.0e-03', '  Time: 0:37:03')
2024-06-24 14:36:15,639 - ('  Epoch: 10', '  Batch: 1400/2198', '  Train Loss: 0.5567', '  LR: 1.0e-03', '  Time: 0:37:08')
2024-06-24 14:36:19,883 - ('  Epoch: 10', '  Batch: 1450/2198', '  Train Loss: 0.5568', '  LR: 1.0e-03', '  Time: 0:37:12')
2024-06-24 14:36:24,132 - ('  Epoch: 10', '  Batch: 1500/2198', '  Train Loss: 0.5558', '  LR: 1.0e-03', '  Time: 0:37:16')
2024-06-24 14:36:28,381 - ('  Epoch: 10', '  Batch: 1550/2198', '  Train Loss: 0.5576', '  LR: 1.0e-03', '  Time: 0:37:20')
2024-06-24 14:36:32,632 - ('  Epoch: 10', '  Batch: 1600/2198', '  Train Loss: 0.5556', '  LR: 1.0e-03', '  Time: 0:37:25')
2024-06-24 14:36:36,886 - ('  Epoch: 10', '  Batch: 1650/2198', '  Train Loss: 0.5570', '  LR: 1.0e-03', '  Time: 0:37:29')
2024-06-24 14:36:41,254 - ('  Epoch: 10', '  Batch: 1700/2198', '  Train Loss: 0.5555', '  LR: 1.0e-03', '  Time: 0:37:33')
2024-06-24 14:36:45,679 - ('  Epoch: 10', '  Batch: 1750/2198', '  Train Loss: 0.5577', '  LR: 1.0e-03', '  Time: 0:37:38')
2024-06-24 14:36:50,192 - ('  Epoch: 10', '  Batch: 1800/2198', '  Train Loss: 0.5552', '  LR: 1.0e-03', '  Time: 0:37:42')
2024-06-24 14:36:54,713 - ('  Epoch: 10', '  Batch: 1850/2198', '  Train Loss: 0.5553', '  LR: 1.0e-03', '  Time: 0:37:47')
2024-06-24 14:36:59,193 - ('  Epoch: 10', '  Batch: 1900/2198', '  Train Loss: 0.5557', '  LR: 1.0e-03', '  Time: 0:37:51')
2024-06-24 14:37:03,557 - ('  Epoch: 10', '  Batch: 1950/2198', '  Train Loss: 0.5564', '  LR: 1.0e-03', '  Time: 0:37:56')
2024-06-24 14:37:07,809 - ('  Epoch: 10', '  Batch: 2000/2198', '  Train Loss: 0.5591', '  LR: 1.0e-03', '  Time: 0:38:00')
2024-06-24 14:37:12,061 - ('  Epoch: 10', '  Batch: 2050/2198', '  Train Loss: 0.5559', '  LR: 1.0e-03', '  Time: 0:38:04')
2024-06-24 14:37:16,318 - ('  Epoch: 10', '  Batch: 2100/2198', '  Train Loss: 0.5568', '  LR: 1.0e-03', '  Time: 0:38:08')
2024-06-24 14:37:20,586 - ('  Epoch: 10', '  Batch: 2150/2198', '  Train Loss: 0.5569', '  LR: 1.0e-03', '  Time: 0:38:13')
2024-06-24 14:37:44,853 - 
Epoch: 10  Val Loss: 0.5409  R2 score: 0.3963
2024-06-24 14:37:44,855 - Validation loss decreased, saving new best model and resetting patience counter.
