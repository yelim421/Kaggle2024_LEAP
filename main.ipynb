{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import argparse\n",
    "import yaml\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activated Kaggle user: limkim\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def activate_kaggle_user(user):\n",
    "    # 사용자 키가 저장된 파일 경로\n",
    "    filepath = '/home/jhko/LEAP_ClimSim/kaggle_copy.json'\n",
    "\n",
    "    # kaggle.json 파일 읽기\n",
    "    with open(filepath, 'r') as f:\n",
    "        users = json.load(f)\n",
    "    \n",
    "    if user not in users:\n",
    "        raise ValueError(f\"User {user} not found in the kaggle.json file\")\n",
    "\n",
    "    user_info = users[user]\n",
    "\n",
    "    # kaggle.json 파일에 덮어쓰기\n",
    "    with open('/home/jhko/LEAP_ClimSim/kaggle.json', 'w') as f:\n",
    "        json.dump(user_info, f)\n",
    "\n",
    "    # 파일 권한 설정\n",
    "    os.chmod('/home/jhko/LEAP_ClimSim/kaggle.json', 0o600)\n",
    "    print(f\"Activated Kaggle user: {user}\")\n",
    "\n",
    "# 예시 사용법\n",
    "activate_kaggle_user('limkim')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "def setup_logging(log_file):\n",
    "    logging.basicConfig(level = logging.INFO, format='%(asctime)s - %(message)s', handlers = [\n",
    "        logging.FileHandler(log_file, mode = 'a'),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "    \n",
    "def log_hyperparameters(config):\n",
    "    for key, value in config.items():\n",
    "        logging.info(f\"{key}: {value}\")\n",
    "\n",
    "def format_time(elapsed):\n",
    "    \"\"\"Take a time in seconds and return a string hh:mm:ss.\"\"\"\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def seed_everything(seed_val=1325):\n",
    "    \"\"\"Seed everything.\"\"\"\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train():\n",
    "\n",
    "    weights = pd.read_csv('/data01/jhko/LEAP/sample_submission.csv', nrows = 1)\n",
    "    del weights['sample_id']\n",
    "    weights = weights.T\n",
    "    weights = weights.to_dict()[0]\n",
    "\n",
    "    df_train = pl.read_csv('/data01/jhko/LEAP/train.csv', n_rows = 2_500_500)\n",
    "    for target in weights:\n",
    "        df_train = df_train.with_columns(pl.col(target).mul(weights[target]))\n",
    "    print('time to read dataset:', format_time(time.time()-ts), flush = True)\n",
    "\n",
    "    FEAT_COLS = df_train.columns[1:557]\n",
    "    TARGET_COLS = df_train.columns[557:]\n",
    "\n",
    "    for col in FEAT_COLS:\n",
    "        df_train = df_train.with_columns(pl.col(col).cast(pl.Float32))\n",
    "\n",
    "    for col in TARGET_COLS:\n",
    "        df_train = df_train.with_columns(pl.col(col).cast(pl.Float32))\n",
    "\n",
    "    x_train = df_train.select(FEAT_COLS).to_numpy()\n",
    "    y_train = df_train.select(TARGET_COLS).to_numpy()\n",
    "\n",
    "    del df_train\n",
    "    gc.collect()\n",
    "    \n",
    "    return x_train, y_train, FEAT_COLS, TARGET_COLS\n",
    "\n",
    "def normalization(x_train, y_train):\n",
    "    mx = x_train.mean(axis=0)\n",
    "    sx = np.maximum(x_train.std(axis=0), 1e-8)\n",
    "    x_train = (x_train - mx.reshape(1,-1)) / sx.reshape(1,-1)\n",
    "\n",
    "    my = y_train.mean(axis=0)\n",
    "    sy = np.maximum(np.sqrt((y_train*y_train).mean(axis=0)), 1e-8)\n",
    "    y_train = (y_train - my.reshape(1,-1)) / sy.reshape(1,-1)\n",
    "\n",
    "    return x_train, y_train, mx, sx, my, sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        assert x.shape[0] == y.shape[0], \"x, y sample shape not same\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.from_numpy(self.x[index]).float().to(device), torch.from_numpy(self.y[index]).float().to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        previous_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(previous_size, hidden_size))\n",
    "            layers.append(nn.LayerNorm(hidden_size))\n",
    "            layers.append(nn.LeakyReLU(inplace=True))\n",
    "            layers.append(nn.Dropout(p=0.1))\n",
    "            previous_size = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(previous_size, output_size))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LpLoss(object):\n",
    "    def __init__(self, d=2, p=2, size_average=True, reduction=True):\n",
    "        super(LpLoss, self).__init__()\n",
    "\n",
    "        assert d>0 and p>0\n",
    "\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.reduction = reduction\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def abs(self, x, y):\n",
    "        num_examples = x.size()[0]\n",
    "        h = 1.0 / (x.size()[1] - 1.0)\n",
    "\n",
    "        all_norms = (h**(self.d/self.p)) * torch.norm(x.view(num_examples, -1) - y.view(num_examples, -1), self.p, 1)\n",
    "        if self.reduction:\n",
    "            if self.size_average:\n",
    "                return torch.mean(all_norms)\n",
    "            else:\n",
    "                return torch.sum(all_norms)\n",
    "\n",
    "        return all_norms\n",
    "\n",
    "    def rel(self, x, y):\n",
    "        epsilon = 1e-8\n",
    "        num_examples = x.size()[0]\n",
    "\n",
    "        diff_norms = torch.norm(x.reshape(num_examples, -1) - y.reshape(num_examples, -1), self.p, 1)\n",
    "        y_norms = torch.norm(y.reshape(num_examples, -1), self.p, 1) + epsilon\n",
    "\n",
    "        if self.reduction:\n",
    "            if self.size_average:\n",
    "                return torch.mean(diff_norms / y_norms)\n",
    "            else:\n",
    "                return torch.sum(diff_norms / y_norms)\n",
    "        return diff_norms / y_norms\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        return self.rel(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 13:59:07,884 - -----------------------------------------------------------------------------\n",
      "2024-06-24 13:59:07,885 - Hyperparameters:\n",
      "2024-06-24 13:59:07,886 - DATA_PATH: /data01/jhko/LEAP/\n",
      "2024-06-24 13:59:07,886 - BATCH_SIZE: 1024\n",
      "2024-06-24 13:59:07,886 - MIN_STD: 1e-8\n",
      "2024-06-24 13:59:07,887 - SCHEDULER_PATIENCE: 3\n",
      "2024-06-24 13:59:07,887 - SCHEDULER_FACTOR: 0.316\n",
      "2024-06-24 13:59:07,887 - EPOCHS: 10\n",
      "2024-06-24 13:59:07,888 - PATIENCE: 6\n",
      "2024-06-24 13:59:07,888 - PRINT_FREQ: 50\n",
      "2024-06-24 13:59:07,888 - BEST_MODEL_PATH: best_model_seq.pth\n",
      "2024-06-24 13:59:07,889 - LEARNING_RATE: 0.001\n",
      "2024-06-24 13:59:07,889 - WEIGHT_DECAY: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to read dataset: 0:00:36\n",
      "am i using gpu? : cuda:0\n",
      "Time after all preparations: 0:01:32\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 14:00:46,626 - ('  Epoch: 1', '  Batch: 50/2198', '  Train Loss: 1.1479', '  LR: 1.0e-03', '  Time: 0:01:39')\n",
      "2024-06-24 14:00:51,809 - ('  Epoch: 1', '  Batch: 100/2198', '  Train Loss: 0.9956', '  LR: 1.0e-03', '  Time: 0:01:44')\n",
      "2024-06-24 14:00:56,766 - ('  Epoch: 1', '  Batch: 150/2198', '  Train Loss: 0.8866', '  LR: 1.0e-03', '  Time: 0:01:49')\n",
      "2024-06-24 14:01:01,747 - ('  Epoch: 1', '  Batch: 200/2198', '  Train Loss: 0.8625', '  LR: 1.0e-03', '  Time: 0:01:54')\n",
      "2024-06-24 14:01:06,808 - ('  Epoch: 1', '  Batch: 250/2198', '  Train Loss: 0.8284', '  LR: 1.0e-03', '  Time: 0:01:59')\n",
      "2024-06-24 14:01:11,859 - ('  Epoch: 1', '  Batch: 300/2198', '  Train Loss: 0.8058', '  LR: 1.0e-03', '  Time: 0:02:04')\n",
      "2024-06-24 14:01:16,990 - ('  Epoch: 1', '  Batch: 350/2198', '  Train Loss: 0.7851', '  LR: 1.0e-03', '  Time: 0:02:09')\n",
      "2024-06-24 14:01:21,974 - ('  Epoch: 1', '  Batch: 400/2198', '  Train Loss: 0.7730', '  LR: 1.0e-03', '  Time: 0:02:14')\n",
      "2024-06-24 14:01:27,389 - ('  Epoch: 1', '  Batch: 450/2198', '  Train Loss: 0.7608', '  LR: 1.0e-03', '  Time: 0:02:19')\n",
      "2024-06-24 14:01:32,852 - ('  Epoch: 1', '  Batch: 500/2198', '  Train Loss: 0.7540', '  LR: 1.0e-03', '  Time: 0:02:25')\n",
      "2024-06-24 14:01:37,870 - ('  Epoch: 1', '  Batch: 550/2198', '  Train Loss: 0.7450', '  LR: 1.0e-03', '  Time: 0:02:30')\n",
      "2024-06-24 14:01:42,928 - ('  Epoch: 1', '  Batch: 600/2198', '  Train Loss: 0.7393', '  LR: 1.0e-03', '  Time: 0:02:35')\n",
      "2024-06-24 14:01:47,898 - ('  Epoch: 1', '  Batch: 650/2198', '  Train Loss: 0.7295', '  LR: 1.0e-03', '  Time: 0:02:40')\n",
      "2024-06-24 14:01:52,839 - ('  Epoch: 1', '  Batch: 700/2198', '  Train Loss: 0.7215', '  LR: 1.0e-03', '  Time: 0:02:45')\n",
      "2024-06-24 14:01:57,950 - ('  Epoch: 1', '  Batch: 750/2198', '  Train Loss: 0.7157', '  LR: 1.0e-03', '  Time: 0:02:50')\n",
      "2024-06-24 14:02:03,192 - ('  Epoch: 1', '  Batch: 800/2198', '  Train Loss: 0.7101', '  LR: 1.0e-03', '  Time: 0:02:55')\n",
      "2024-06-24 14:02:08,540 - ('  Epoch: 1', '  Batch: 850/2198', '  Train Loss: 0.7049', '  LR: 1.0e-03', '  Time: 0:03:01')\n",
      "2024-06-24 14:02:13,730 - ('  Epoch: 1', '  Batch: 900/2198', '  Train Loss: 0.7007', '  LR: 1.0e-03', '  Time: 0:03:06')\n",
      "2024-06-24 14:02:18,650 - ('  Epoch: 1', '  Batch: 950/2198', '  Train Loss: 0.6951', '  LR: 1.0e-03', '  Time: 0:03:11')\n",
      "2024-06-24 14:02:23,573 - ('  Epoch: 1', '  Batch: 1000/2198', '  Train Loss: 0.6923', '  LR: 1.0e-03', '  Time: 0:03:16')\n",
      "2024-06-24 14:02:28,497 - ('  Epoch: 1', '  Batch: 1050/2198', '  Train Loss: 0.6869', '  LR: 1.0e-03', '  Time: 0:03:21')\n",
      "2024-06-24 14:02:33,430 - ('  Epoch: 1', '  Batch: 1100/2198', '  Train Loss: 0.6843', '  LR: 1.0e-03', '  Time: 0:03:26')\n",
      "2024-06-24 14:02:38,463 - ('  Epoch: 1', '  Batch: 1150/2198', '  Train Loss: 0.6811', '  LR: 1.0e-03', '  Time: 0:03:31')\n",
      "2024-06-24 14:02:43,701 - ('  Epoch: 1', '  Batch: 1200/2198', '  Train Loss: 0.6790', '  LR: 1.0e-03', '  Time: 0:03:36')\n",
      "2024-06-24 14:02:48,909 - ('  Epoch: 1', '  Batch: 1250/2198', '  Train Loss: 0.6741', '  LR: 1.0e-03', '  Time: 0:03:41')\n",
      "2024-06-24 14:02:54,171 - ('  Epoch: 1', '  Batch: 1300/2198', '  Train Loss: 0.6739', '  LR: 1.0e-03', '  Time: 0:03:46')\n",
      "2024-06-24 14:02:59,119 - ('  Epoch: 1', '  Batch: 1350/2198', '  Train Loss: 0.6701', '  LR: 1.0e-03', '  Time: 0:03:51')\n",
      "2024-06-24 14:03:04,144 - ('  Epoch: 1', '  Batch: 1400/2198', '  Train Loss: 0.6696', '  LR: 1.0e-03', '  Time: 0:03:56')\n",
      "2024-06-24 14:03:09,063 - ('  Epoch: 1', '  Batch: 1450/2198', '  Train Loss: 0.6664', '  LR: 1.0e-03', '  Time: 0:04:01')\n",
      "2024-06-24 14:03:13,996 - ('  Epoch: 1', '  Batch: 1500/2198', '  Train Loss: 0.6656', '  LR: 1.0e-03', '  Time: 0:04:06')\n",
      "2024-06-24 14:03:18,937 - ('  Epoch: 1', '  Batch: 1550/2198', '  Train Loss: 0.6616', '  LR: 1.0e-03', '  Time: 0:04:11')\n",
      "2024-06-24 14:03:23,942 - ('  Epoch: 1', '  Batch: 1600/2198', '  Train Loss: 0.6611', '  LR: 1.0e-03', '  Time: 0:04:16')\n",
      "2024-06-24 14:03:29,314 - ('  Epoch: 1', '  Batch: 1650/2198', '  Train Loss: 0.6608', '  LR: 1.0e-03', '  Time: 0:04:21')\n",
      "2024-06-24 14:03:34,551 - ('  Epoch: 1', '  Batch: 1700/2198', '  Train Loss: 0.6577', '  LR: 1.0e-03', '  Time: 0:04:27')\n",
      "2024-06-24 14:03:39,694 - ('  Epoch: 1', '  Batch: 1750/2198', '  Train Loss: 0.6566', '  LR: 1.0e-03', '  Time: 0:04:32')\n",
      "2024-06-24 14:03:44,633 - ('  Epoch: 1', '  Batch: 1800/2198', '  Train Loss: 0.6542', '  LR: 1.0e-03', '  Time: 0:04:37')\n",
      "2024-06-24 14:03:49,552 - ('  Epoch: 1', '  Batch: 1850/2198', '  Train Loss: 0.6517', '  LR: 1.0e-03', '  Time: 0:04:42')\n",
      "2024-06-24 14:03:54,581 - ('  Epoch: 1', '  Batch: 1900/2198', '  Train Loss: 0.6513', '  LR: 1.0e-03', '  Time: 0:04:47')\n",
      "2024-06-24 14:03:59,522 - ('  Epoch: 1', '  Batch: 1950/2198', '  Train Loss: 0.6485', '  LR: 1.0e-03', '  Time: 0:04:52')\n",
      "2024-06-24 14:04:04,444 - ('  Epoch: 1', '  Batch: 2000/2198', '  Train Loss: 0.6506', '  LR: 1.0e-03', '  Time: 0:04:57')\n",
      "2024-06-24 14:04:09,612 - ('  Epoch: 1', '  Batch: 2050/2198', '  Train Loss: 0.6471', '  LR: 1.0e-03', '  Time: 0:05:02')\n",
      "2024-06-24 14:04:14,831 - ('  Epoch: 1', '  Batch: 2100/2198', '  Train Loss: 0.6473', '  LR: 1.0e-03', '  Time: 0:05:07')\n",
      "2024-06-24 14:04:20,090 - ('  Epoch: 1', '  Batch: 2150/2198', '  Train Loss: 0.6476', '  LR: 1.0e-03', '  Time: 0:05:12')\n",
      "2024-06-24 14:04:48,292 - \n",
      "Epoch: 1  Val Loss: 0.6335  R2 score: 0.2891\n",
      "2024-06-24 14:04:48,294 - Validation loss decreased, saving new best model and resetting patience counter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 targets were excluded during evaluation of R2 score.\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 14:04:53,546 - ('  Epoch: 2', '  Batch: 50/2198', '  Train Loss: 0.6440', '  LR: 1.0e-03', '  Time: 0:05:46')\n",
      "2024-06-24 14:04:58,764 - ('  Epoch: 2', '  Batch: 100/2198', '  Train Loss: 0.6421', '  LR: 1.0e-03', '  Time: 0:05:51')\n",
      "2024-06-24 14:05:04,170 - ('  Epoch: 2', '  Batch: 150/2198', '  Train Loss: 0.6411', '  LR: 1.0e-03', '  Time: 0:05:56')\n",
      "2024-06-24 14:05:09,154 - ('  Epoch: 2', '  Batch: 200/2198', '  Train Loss: 0.6412', '  LR: 1.0e-03', '  Time: 0:06:01')\n",
      "2024-06-24 14:05:14,083 - ('  Epoch: 2', '  Batch: 250/2198', '  Train Loss: 0.6365', '  LR: 1.0e-03', '  Time: 0:06:06')\n",
      "2024-06-24 14:05:19,018 - ('  Epoch: 2', '  Batch: 300/2198', '  Train Loss: 0.6387', '  LR: 1.0e-03', '  Time: 0:06:11')\n",
      "2024-06-24 14:05:23,956 - ('  Epoch: 2', '  Batch: 350/2198', '  Train Loss: 0.6381', '  LR: 1.0e-03', '  Time: 0:06:16')\n",
      "2024-06-24 14:05:29,004 - ('  Epoch: 2', '  Batch: 400/2198', '  Train Loss: 0.6380', '  LR: 1.0e-03', '  Time: 0:06:21')\n",
      "2024-06-24 14:05:33,940 - ('  Epoch: 2', '  Batch: 450/2198', '  Train Loss: 0.6367', '  LR: 1.0e-03', '  Time: 0:06:26')\n",
      "2024-06-24 14:05:39,212 - ('  Epoch: 2', '  Batch: 500/2198', '  Train Loss: 0.6350', '  LR: 1.0e-03', '  Time: 0:06:31')\n",
      "2024-06-24 14:05:44,421 - ('  Epoch: 2', '  Batch: 550/2198', '  Train Loss: 0.6352', '  LR: 1.0e-03', '  Time: 0:06:37')\n",
      "2024-06-24 14:05:49,684 - ('  Epoch: 2', '  Batch: 600/2198', '  Train Loss: 0.6321', '  LR: 1.0e-03', '  Time: 0:06:42')\n",
      "2024-06-24 14:05:54,732 - ('  Epoch: 2', '  Batch: 650/2198', '  Train Loss: 0.6330', '  LR: 1.0e-03', '  Time: 0:06:47')\n",
      "2024-06-24 14:05:59,662 - ('  Epoch: 2', '  Batch: 700/2198', '  Train Loss: 0.6312', '  LR: 1.0e-03', '  Time: 0:06:52')\n",
      "2024-06-24 14:06:04,590 - ('  Epoch: 2', '  Batch: 750/2198', '  Train Loss: 0.6302', '  LR: 1.0e-03', '  Time: 0:06:57')\n",
      "2024-06-24 14:06:09,531 - ('  Epoch: 2', '  Batch: 800/2198', '  Train Loss: 0.6297', '  LR: 1.0e-03', '  Time: 0:07:02')\n",
      "2024-06-24 14:06:14,484 - ('  Epoch: 2', '  Batch: 850/2198', '  Train Loss: 0.6316', '  LR: 1.0e-03', '  Time: 0:07:07')\n",
      "2024-06-24 14:06:19,640 - ('  Epoch: 2', '  Batch: 900/2198', '  Train Loss: 0.6308', '  LR: 1.0e-03', '  Time: 0:07:12')\n",
      "2024-06-24 14:06:24,895 - ('  Epoch: 2', '  Batch: 950/2198', '  Train Loss: 0.6284', '  LR: 1.0e-03', '  Time: 0:07:17')\n",
      "2024-06-24 14:06:30,145 - ('  Epoch: 2', '  Batch: 1000/2198', '  Train Loss: 0.6278', '  LR: 1.0e-03', '  Time: 0:07:22')\n",
      "2024-06-24 14:06:35,266 - ('  Epoch: 2', '  Batch: 1050/2198', '  Train Loss: 0.6277', '  LR: 1.0e-03', '  Time: 0:07:27')\n",
      "2024-06-24 14:06:40,207 - ('  Epoch: 2', '  Batch: 1100/2198', '  Train Loss: 0.6257', '  LR: 1.0e-03', '  Time: 0:07:32')\n",
      "2024-06-24 14:06:45,135 - ('  Epoch: 2', '  Batch: 1150/2198', '  Train Loss: 0.6253', '  LR: 1.0e-03', '  Time: 0:07:37')\n",
      "2024-06-24 14:06:50,166 - ('  Epoch: 2', '  Batch: 1200/2198', '  Train Loss: 0.6248', '  LR: 1.0e-03', '  Time: 0:07:42')\n",
      "2024-06-24 14:06:55,111 - ('  Epoch: 2', '  Batch: 1250/2198', '  Train Loss: 0.6238', '  LR: 1.0e-03', '  Time: 0:07:47')\n",
      "2024-06-24 14:07:00,033 - ('  Epoch: 2', '  Batch: 1300/2198', '  Train Loss: 0.6230', '  LR: 1.0e-03', '  Time: 0:07:52')\n",
      "2024-06-24 14:07:05,230 - ('  Epoch: 2', '  Batch: 1350/2198', '  Train Loss: 0.6233', '  LR: 1.0e-03', '  Time: 0:07:57')\n",
      "2024-06-24 14:07:10,442 - ('  Epoch: 2', '  Batch: 1400/2198', '  Train Loss: 0.6217', '  LR: 1.0e-03', '  Time: 0:08:03')\n",
      "2024-06-24 14:07:15,836 - ('  Epoch: 2', '  Batch: 1450/2198', '  Train Loss: 0.6241', '  LR: 1.0e-03', '  Time: 0:08:08')\n",
      "2024-06-24 14:07:20,800 - ('  Epoch: 2', '  Batch: 1500/2198', '  Train Loss: 0.6218', '  LR: 1.0e-03', '  Time: 0:08:13')\n",
      "2024-06-24 14:07:25,722 - ('  Epoch: 2', '  Batch: 1550/2198', '  Train Loss: 0.6200', '  LR: 1.0e-03', '  Time: 0:08:18')\n",
      "2024-06-24 14:07:30,682 - ('  Epoch: 2', '  Batch: 1600/2198', '  Train Loss: 0.6190', '  LR: 1.0e-03', '  Time: 0:08:23')\n",
      "2024-06-24 14:07:36,338 - ('  Epoch: 2', '  Batch: 1650/2198', '  Train Loss: 0.6203', '  LR: 1.0e-03', '  Time: 0:08:28')\n",
      "2024-06-24 14:07:41,800 - ('  Epoch: 2', '  Batch: 1700/2198', '  Train Loss: 0.6206', '  LR: 1.0e-03', '  Time: 0:08:34')\n",
      "2024-06-24 14:07:46,966 - ('  Epoch: 2', '  Batch: 1750/2198', '  Train Loss: 0.6180', '  LR: 1.0e-03', '  Time: 0:08:39')\n",
      "2024-06-24 14:07:54,957 - ('  Epoch: 2', '  Batch: 1800/2198', '  Train Loss: 0.6177', '  LR: 1.0e-03', '  Time: 0:08:47')\n",
      "2024-06-24 14:08:00,091 - ('  Epoch: 2', '  Batch: 1850/2198', '  Train Loss: 0.6168', '  LR: 1.0e-03', '  Time: 0:08:52')\n",
      "2024-06-24 14:08:05,158 - ('  Epoch: 2', '  Batch: 1900/2198', '  Train Loss: 0.6168', '  LR: 1.0e-03', '  Time: 0:08:57')\n",
      "2024-06-24 14:08:12,162 - ('  Epoch: 2', '  Batch: 1950/2198', '  Train Loss: 0.6152', '  LR: 1.0e-03', '  Time: 0:09:04')\n",
      "2024-06-24 14:08:19,424 - ('  Epoch: 2', '  Batch: 2000/2198', '  Train Loss: 0.6168', '  LR: 1.0e-03', '  Time: 0:09:12')\n",
      "2024-06-24 14:08:24,399 - ('  Epoch: 2', '  Batch: 2050/2198', '  Train Loss: 0.6184', '  LR: 1.0e-03', '  Time: 0:09:17')\n",
      "2024-06-24 14:08:29,156 - ('  Epoch: 2', '  Batch: 2100/2198', '  Train Loss: 0.6144', '  LR: 1.0e-03', '  Time: 0:09:21')\n",
      "2024-06-24 14:08:33,916 - ('  Epoch: 2', '  Batch: 2150/2198', '  Train Loss: 0.6150', '  LR: 1.0e-03', '  Time: 0:09:26')\n",
      "2024-06-24 14:09:02,055 - \n",
      "Epoch: 2  Val Loss: 0.6028  R2 score: 0.3273\n",
      "2024-06-24 14:09:02,058 - Validation loss decreased, saving new best model and resetting patience counter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 targets were excluded during evaluation of R2 score.\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 14:09:07,863 - ('  Epoch: 3', '  Batch: 50/2198', '  Train Loss: 0.6127', '  LR: 1.0e-03', '  Time: 0:10:00')\n",
      "2024-06-24 14:09:13,423 - ('  Epoch: 3', '  Batch: 100/2198', '  Train Loss: 0.6122', '  LR: 1.0e-03', '  Time: 0:10:06')\n",
      "2024-06-24 14:09:18,702 - ('  Epoch: 3', '  Batch: 150/2198', '  Train Loss: 0.6135', '  LR: 1.0e-03', '  Time: 0:10:11')\n",
      "2024-06-24 14:09:24,468 - ('  Epoch: 3', '  Batch: 200/2198', '  Train Loss: 0.6132', '  LR: 1.0e-03', '  Time: 0:10:17')\n",
      "2024-06-24 14:09:29,693 - ('  Epoch: 3', '  Batch: 250/2198', '  Train Loss: 0.6126', '  LR: 1.0e-03', '  Time: 0:10:22')\n",
      "2024-06-24 14:09:34,735 - ('  Epoch: 3', '  Batch: 300/2198', '  Train Loss: 0.6144', '  LR: 1.0e-03', '  Time: 0:10:27')\n",
      "2024-06-24 14:09:39,794 - ('  Epoch: 3', '  Batch: 350/2198', '  Train Loss: 0.6098', '  LR: 1.0e-03', '  Time: 0:10:32')\n",
      "2024-06-24 14:09:47,649 - ('  Epoch: 3', '  Batch: 400/2198', '  Train Loss: 0.6113', '  LR: 1.0e-03', '  Time: 0:10:40')\n",
      "2024-06-24 14:09:55,495 - ('  Epoch: 3', '  Batch: 450/2198', '  Train Loss: 0.6098', '  LR: 1.0e-03', '  Time: 0:10:48')\n",
      "2024-06-24 14:10:00,457 - ('  Epoch: 3', '  Batch: 500/2198', '  Train Loss: 0.6090', '  LR: 1.0e-03', '  Time: 0:10:53')\n",
      "2024-06-24 14:10:05,377 - ('  Epoch: 3', '  Batch: 550/2198', '  Train Loss: 0.6086', '  LR: 1.0e-03', '  Time: 0:10:57')\n",
      "2024-06-24 14:10:10,301 - ('  Epoch: 3', '  Batch: 600/2198', '  Train Loss: 0.6104', '  LR: 1.0e-03', '  Time: 0:11:02')\n",
      "2024-06-24 14:10:14,683 - ('  Epoch: 3', '  Batch: 650/2198', '  Train Loss: 0.6083', '  LR: 1.0e-03', '  Time: 0:11:07')\n",
      "2024-06-24 14:10:18,981 - ('  Epoch: 3', '  Batch: 700/2198', '  Train Loss: 0.6085', '  LR: 1.0e-03', '  Time: 0:11:11')\n",
      "2024-06-24 14:10:23,540 - ('  Epoch: 3', '  Batch: 750/2198', '  Train Loss: 0.6097', '  LR: 1.0e-03', '  Time: 0:11:16')\n",
      "2024-06-24 14:10:28,036 - ('  Epoch: 3', '  Batch: 800/2198', '  Train Loss: 0.6058', '  LR: 1.0e-03', '  Time: 0:11:20')\n",
      "2024-06-24 14:10:32,576 - ('  Epoch: 3', '  Batch: 850/2198', '  Train Loss: 0.6071', '  LR: 1.0e-03', '  Time: 0:11:25')\n",
      "2024-06-24 14:10:37,082 - ('  Epoch: 3', '  Batch: 900/2198', '  Train Loss: 0.6067', '  LR: 1.0e-03', '  Time: 0:11:29')\n",
      "2024-06-24 14:10:41,343 - ('  Epoch: 3', '  Batch: 950/2198', '  Train Loss: 0.6082', '  LR: 1.0e-03', '  Time: 0:11:33')\n",
      "2024-06-24 14:10:45,607 - ('  Epoch: 3', '  Batch: 1000/2198', '  Train Loss: 0.6070', '  LR: 1.0e-03', '  Time: 0:11:38')\n",
      "2024-06-24 14:10:49,871 - ('  Epoch: 3', '  Batch: 1050/2198', '  Train Loss: 0.6058', '  LR: 1.0e-03', '  Time: 0:11:42')\n",
      "2024-06-24 14:10:54,144 - ('  Epoch: 3', '  Batch: 1100/2198', '  Train Loss: 0.6051', '  LR: 1.0e-03', '  Time: 0:11:46')\n",
      "2024-06-24 14:10:58,529 - ('  Epoch: 3', '  Batch: 1150/2198', '  Train Loss: 0.6051', '  LR: 1.0e-03', '  Time: 0:11:51')\n",
      "2024-06-24 14:11:02,827 - ('  Epoch: 3', '  Batch: 1200/2198', '  Train Loss: 0.6054', '  LR: 1.0e-03', '  Time: 0:11:55')\n",
      "2024-06-24 14:11:07,388 - ('  Epoch: 3', '  Batch: 1250/2198', '  Train Loss: 0.6045', '  LR: 1.0e-03', '  Time: 0:11:59')\n",
      "2024-06-24 14:11:11,886 - ('  Epoch: 3', '  Batch: 1300/2198', '  Train Loss: 0.6035', '  LR: 1.0e-03', '  Time: 0:12:04')\n",
      "2024-06-24 14:11:16,428 - ('  Epoch: 3', '  Batch: 1350/2198', '  Train Loss: 0.6036', '  LR: 1.0e-03', '  Time: 0:12:09')\n",
      "2024-06-24 14:11:20,826 - ('  Epoch: 3', '  Batch: 1400/2198', '  Train Loss: 0.6053', '  LR: 1.0e-03', '  Time: 0:12:13')\n",
      "2024-06-24 14:11:25,200 - ('  Epoch: 3', '  Batch: 1450/2198', '  Train Loss: 0.6028', '  LR: 1.0e-03', '  Time: 0:12:17')\n",
      "2024-06-24 14:11:29,461 - ('  Epoch: 3', '  Batch: 1500/2198', '  Train Loss: 0.6038', '  LR: 1.0e-03', '  Time: 0:12:22')\n",
      "2024-06-24 14:11:33,726 - ('  Epoch: 3', '  Batch: 1550/2198', '  Train Loss: 0.6021', '  LR: 1.0e-03', '  Time: 0:12:26')\n",
      "2024-06-24 14:11:37,998 - ('  Epoch: 3', '  Batch: 1600/2198', '  Train Loss: 0.6024', '  LR: 1.0e-03', '  Time: 0:12:30')\n",
      "2024-06-24 14:11:42,278 - ('  Epoch: 3', '  Batch: 1650/2198', '  Train Loss: 0.6021', '  LR: 1.0e-03', '  Time: 0:12:34')\n",
      "2024-06-24 14:11:46,676 - ('  Epoch: 3', '  Batch: 1700/2198', '  Train Loss: 0.6004', '  LR: 1.0e-03', '  Time: 0:12:39')\n",
      "2024-06-24 14:11:51,235 - ('  Epoch: 3', '  Batch: 1750/2198', '  Train Loss: 0.6007', '  LR: 1.0e-03', '  Time: 0:12:43')\n",
      "2024-06-24 14:11:55,733 - ('  Epoch: 3', '  Batch: 1800/2198', '  Train Loss: 0.6037', '  LR: 1.0e-03', '  Time: 0:12:48')\n",
      "2024-06-24 14:12:00,267 - ('  Epoch: 3', '  Batch: 1850/2198', '  Train Loss: 0.6009', '  LR: 1.0e-03', '  Time: 0:12:52')\n",
      "2024-06-24 14:12:04,662 - ('  Epoch: 3', '  Batch: 1900/2198', '  Train Loss: 0.6000', '  LR: 1.0e-03', '  Time: 0:12:57')\n",
      "2024-06-24 14:12:09,035 - ('  Epoch: 3', '  Batch: 1950/2198', '  Train Loss: 0.5995', '  LR: 1.0e-03', '  Time: 0:13:01')\n",
      "2024-06-24 14:12:13,300 - ('  Epoch: 3', '  Batch: 2000/2198', '  Train Loss: 0.6000', '  LR: 1.0e-03', '  Time: 0:13:05')\n",
      "2024-06-24 14:12:17,563 - ('  Epoch: 3', '  Batch: 2050/2198', '  Train Loss: 0.5999', '  LR: 1.0e-03', '  Time: 0:13:10')\n",
      "2024-06-24 14:12:21,835 - ('  Epoch: 3', '  Batch: 2100/2198', '  Train Loss: 0.6003', '  LR: 1.0e-03', '  Time: 0:13:14')\n",
      "2024-06-24 14:12:26,113 - ('  Epoch: 3', '  Batch: 2150/2198', '  Train Loss: 0.5993', '  LR: 1.0e-03', '  Time: 0:13:18')\n",
      "2024-06-24 14:12:50,584 - \n",
      "Epoch: 3  Val Loss: 0.5866  R2 score: 0.3481\n",
      "2024-06-24 14:12:50,585 - Validation loss decreased, saving new best model and resetting patience counter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 targets were excluded during evaluation of R2 score.\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 14:12:55,678 - ('  Epoch: 4', '  Batch: 50/2198', '  Train Loss: 0.5966', '  LR: 1.0e-03', '  Time: 0:13:48')\n",
      "2024-06-24 14:13:01,100 - ('  Epoch: 4', '  Batch: 100/2198', '  Train Loss: 0.5983', '  LR: 1.0e-03', '  Time: 0:13:53')\n",
      "2024-06-24 14:13:06,623 - ('  Epoch: 4', '  Batch: 150/2198', '  Train Loss: 0.5971', '  LR: 1.0e-03', '  Time: 0:13:59')\n",
      "2024-06-24 14:13:11,086 - ('  Epoch: 4', '  Batch: 200/2198', '  Train Loss: 0.5968', '  LR: 1.0e-03', '  Time: 0:14:03')\n",
      "2024-06-24 14:13:15,626 - ('  Epoch: 4', '  Batch: 250/2198', '  Train Loss: 0.5970', '  LR: 1.0e-03', '  Time: 0:14:08')\n",
      "2024-06-24 14:13:20,597 - ('  Epoch: 4', '  Batch: 300/2198', '  Train Loss: 0.5966', '  LR: 1.0e-03', '  Time: 0:14:13')\n",
      "2024-06-24 14:13:25,240 - ('  Epoch: 4', '  Batch: 350/2198', '  Train Loss: 0.5954', '  LR: 1.0e-03', '  Time: 0:14:17')\n",
      "2024-06-24 14:13:29,562 - ('  Epoch: 4', '  Batch: 400/2198', '  Train Loss: 0.5963', '  LR: 1.0e-03', '  Time: 0:14:22')\n",
      "2024-06-24 14:13:33,902 - ('  Epoch: 4', '  Batch: 450/2198', '  Train Loss: 0.5949', '  LR: 1.0e-03', '  Time: 0:14:26')\n",
      "2024-06-24 14:13:38,237 - ('  Epoch: 4', '  Batch: 500/2198', '  Train Loss: 0.5959', '  LR: 1.0e-03', '  Time: 0:14:30')\n",
      "2024-06-24 14:13:42,872 - ('  Epoch: 4', '  Batch: 550/2198', '  Train Loss: 0.5945', '  LR: 1.0e-03', '  Time: 0:14:35')\n",
      "2024-06-24 14:13:47,493 - ('  Epoch: 4', '  Batch: 600/2198', '  Train Loss: 0.5966', '  LR: 1.0e-03', '  Time: 0:14:40')\n",
      "2024-06-24 14:13:52,093 - ('  Epoch: 4', '  Batch: 650/2198', '  Train Loss: 0.5934', '  LR: 1.0e-03', '  Time: 0:14:44')\n",
      "2024-06-24 14:13:56,365 - ('  Epoch: 4', '  Batch: 700/2198', '  Train Loss: 0.5947', '  LR: 1.0e-03', '  Time: 0:14:48')\n",
      "2024-06-24 14:14:00,638 - ('  Epoch: 4', '  Batch: 750/2198', '  Train Loss: 0.5941', '  LR: 1.0e-03', '  Time: 0:14:53')\n",
      "2024-06-24 14:14:04,917 - ('  Epoch: 4', '  Batch: 800/2198', '  Train Loss: 0.5939', '  LR: 1.0e-03', '  Time: 0:14:57')\n",
      "2024-06-24 14:14:09,208 - ('  Epoch: 4', '  Batch: 850/2198', '  Train Loss: 0.5950', '  LR: 1.0e-03', '  Time: 0:15:01')\n",
      "2024-06-24 14:14:13,587 - ('  Epoch: 4', '  Batch: 900/2198', '  Train Loss: 0.5939', '  LR: 1.0e-03', '  Time: 0:15:06')\n",
      "2024-06-24 14:14:18,095 - ('  Epoch: 4', '  Batch: 950/2198', '  Train Loss: 0.5932', '  LR: 1.0e-03', '  Time: 0:15:10')\n",
      "2024-06-24 14:14:22,593 - ('  Epoch: 4', '  Batch: 1000/2198', '  Train Loss: 0.5940', '  LR: 1.0e-03', '  Time: 0:15:15')\n",
      "2024-06-24 14:14:27,143 - ('  Epoch: 4', '  Batch: 1050/2198', '  Train Loss: 0.5941', '  LR: 1.0e-03', '  Time: 0:15:19')\n",
      "2024-06-24 14:14:31,616 - ('  Epoch: 4', '  Batch: 1100/2198', '  Train Loss: 0.5917', '  LR: 1.0e-03', '  Time: 0:15:24')\n",
      "2024-06-24 14:14:35,961 - ('  Epoch: 4', '  Batch: 1150/2198', '  Train Loss: 0.5932', '  LR: 1.0e-03', '  Time: 0:15:28')\n",
      "2024-06-24 14:14:40,196 - ('  Epoch: 4', '  Batch: 1200/2198', '  Train Loss: 0.5927', '  LR: 1.0e-03', '  Time: 0:15:32')\n",
      "2024-06-24 14:14:44,430 - ('  Epoch: 4', '  Batch: 1250/2198', '  Train Loss: 0.5927', '  LR: 1.0e-03', '  Time: 0:15:37')\n",
      "2024-06-24 14:14:49,063 - ('  Epoch: 4', '  Batch: 1300/2198', '  Train Loss: 0.5926', '  LR: 1.0e-03', '  Time: 0:15:41')\n",
      "2024-06-24 14:14:54,526 - ('  Epoch: 4', '  Batch: 1350/2198', '  Train Loss: 0.5910', '  LR: 1.0e-03', '  Time: 0:15:47')\n",
      "2024-06-24 14:15:00,394 - ('  Epoch: 4', '  Batch: 1400/2198', '  Train Loss: 0.5909', '  LR: 1.0e-03', '  Time: 0:15:53')\n",
      "2024-06-24 14:15:05,466 - ('  Epoch: 4', '  Batch: 1450/2198', '  Train Loss: 0.5929', '  LR: 1.0e-03', '  Time: 0:15:58')\n",
      "2024-06-24 14:15:09,934 - ('  Epoch: 4', '  Batch: 1500/2198', '  Train Loss: 0.5897', '  LR: 1.0e-03', '  Time: 0:16:02')\n",
      "2024-06-24 14:15:14,840 - ('  Epoch: 4', '  Batch: 1550/2198', '  Train Loss: 0.5904', '  LR: 1.0e-03', '  Time: 0:16:07')\n",
      "2024-06-24 14:15:19,559 - ('  Epoch: 4', '  Batch: 1600/2198', '  Train Loss: 0.5915', '  LR: 1.0e-03', '  Time: 0:16:12')\n",
      "2024-06-24 14:15:23,882 - ('  Epoch: 4', '  Batch: 1650/2198', '  Train Loss: 0.5914', '  LR: 1.0e-03', '  Time: 0:16:16')\n",
      "2024-06-24 14:15:28,297 - ('  Epoch: 4', '  Batch: 1700/2198', '  Train Loss: 0.5897', '  LR: 1.0e-03', '  Time: 0:16:20')\n",
      "2024-06-24 14:15:32,577 - ('  Epoch: 4', '  Batch: 1750/2198', '  Train Loss: 0.5896', '  LR: 1.0e-03', '  Time: 0:16:25')\n",
      "2024-06-24 14:15:37,206 - ('  Epoch: 4', '  Batch: 1800/2198', '  Train Loss: 0.5920', '  LR: 1.0e-03', '  Time: 0:16:29')\n",
      "2024-06-24 14:15:41,797 - ('  Epoch: 4', '  Batch: 1850/2198', '  Train Loss: 0.5903', '  LR: 1.0e-03', '  Time: 0:16:34')\n",
      "2024-06-24 14:15:46,363 - ('  Epoch: 4', '  Batch: 1900/2198', '  Train Loss: 0.5905', '  LR: 1.0e-03', '  Time: 0:16:38')\n",
      "2024-06-24 14:15:50,744 - ('  Epoch: 4', '  Batch: 1950/2198', '  Train Loss: 0.5897', '  LR: 1.0e-03', '  Time: 0:16:43')\n",
      "2024-06-24 14:15:55,015 - ('  Epoch: 4', '  Batch: 2000/2198', '  Train Loss: 0.5887', '  LR: 1.0e-03', '  Time: 0:16:47')\n",
      "2024-06-24 14:15:59,285 - ('  Epoch: 4', '  Batch: 2050/2198', '  Train Loss: 0.5905', '  LR: 1.0e-03', '  Time: 0:16:51')\n",
      "2024-06-24 14:16:03,574 - ('  Epoch: 4', '  Batch: 2100/2198', '  Train Loss: 0.5874', '  LR: 1.0e-03', '  Time: 0:16:56')\n",
      "2024-06-24 14:16:07,846 - ('  Epoch: 4', '  Batch: 2150/2198', '  Train Loss: 0.5884', '  LR: 1.0e-03', '  Time: 0:17:00')\n",
      "2024-06-24 14:16:32,152 - \n",
      "Epoch: 4  Val Loss: 0.5759  R2 score: 0.3592\n",
      "2024-06-24 14:16:32,153 - Validation loss decreased, saving new best model and resetting patience counter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 targets were excluded during evaluation of R2 score.\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 14:16:36,482 - ('  Epoch: 5', '  Batch: 50/2198', '  Train Loss: 0.5878', '  LR: 1.0e-03', '  Time: 0:17:29')\n",
      "2024-06-24 14:16:40,740 - ('  Epoch: 5', '  Batch: 100/2198', '  Train Loss: 0.5859', '  LR: 1.0e-03', '  Time: 0:17:33')\n",
      "2024-06-24 14:16:45,006 - ('  Epoch: 5', '  Batch: 150/2198', '  Train Loss: 0.5854', '  LR: 1.0e-03', '  Time: 0:17:37')\n",
      "2024-06-24 14:16:49,385 - ('  Epoch: 5', '  Batch: 200/2198', '  Train Loss: 0.5861', '  LR: 1.0e-03', '  Time: 0:17:41')\n",
      "2024-06-24 14:16:53,650 - ('  Epoch: 5', '  Batch: 250/2198', '  Train Loss: 0.5862', '  LR: 1.0e-03', '  Time: 0:17:46')\n",
      "2024-06-24 14:16:58,143 - ('  Epoch: 5', '  Batch: 300/2198', '  Train Loss: 0.5859', '  LR: 1.0e-03', '  Time: 0:17:50')\n",
      "2024-06-24 14:17:02,632 - ('  Epoch: 5', '  Batch: 350/2198', '  Train Loss: 0.5851', '  LR: 1.0e-03', '  Time: 0:17:55')\n",
      "2024-06-24 14:17:07,179 - ('  Epoch: 5', '  Batch: 400/2198', '  Train Loss: 0.5867', '  LR: 1.0e-03', '  Time: 0:17:59')\n",
      "2024-06-24 14:17:11,760 - ('  Epoch: 5', '  Batch: 450/2198', '  Train Loss: 0.5866', '  LR: 1.0e-03', '  Time: 0:18:04')\n",
      "2024-06-24 14:17:16,018 - ('  Epoch: 5', '  Batch: 500/2198', '  Train Loss: 0.5849', '  LR: 1.0e-03', '  Time: 0:18:08')\n",
      "2024-06-24 14:17:20,278 - ('  Epoch: 5', '  Batch: 550/2198', '  Train Loss: 0.5847', '  LR: 1.0e-03', '  Time: 0:18:12')\n",
      "2024-06-24 14:17:24,539 - ('  Epoch: 5', '  Batch: 600/2198', '  Train Loss: 0.5844', '  LR: 1.0e-03', '  Time: 0:18:17')\n",
      "2024-06-24 14:17:28,805 - ('  Epoch: 5', '  Batch: 650/2198', '  Train Loss: 0.5844', '  LR: 1.0e-03', '  Time: 0:18:21')\n",
      "2024-06-24 14:17:33,082 - ('  Epoch: 5', '  Batch: 700/2198', '  Train Loss: 0.5848', '  LR: 1.0e-03', '  Time: 0:18:25')\n",
      "2024-06-24 14:17:37,455 - ('  Epoch: 5', '  Batch: 750/2198', '  Train Loss: 0.5854', '  LR: 1.0e-03', '  Time: 0:18:30')\n",
      "2024-06-24 14:17:41,948 - ('  Epoch: 5', '  Batch: 800/2198', '  Train Loss: 0.5849', '  LR: 1.0e-03', '  Time: 0:18:34')\n",
      "2024-06-24 14:17:46,438 - ('  Epoch: 5', '  Batch: 850/2198', '  Train Loss: 0.5825', '  LR: 1.0e-03', '  Time: 0:18:39')\n",
      "2024-06-24 14:17:50,981 - ('  Epoch: 5', '  Batch: 900/2198', '  Train Loss: 0.5842', '  LR: 1.0e-03', '  Time: 0:18:43')\n",
      "2024-06-24 14:17:55,448 - ('  Epoch: 5', '  Batch: 950/2198', '  Train Loss: 0.5841', '  LR: 1.0e-03', '  Time: 0:18:48')\n",
      "2024-06-24 14:17:59,831 - ('  Epoch: 5', '  Batch: 1000/2198', '  Train Loss: 0.5853', '  LR: 1.0e-03', '  Time: 0:18:52')\n",
      "2024-06-24 14:18:04,095 - ('  Epoch: 5', '  Batch: 1050/2198', '  Train Loss: 0.5850', '  LR: 1.0e-03', '  Time: 0:18:56')\n",
      "2024-06-24 14:18:08,354 - ('  Epoch: 5', '  Batch: 1100/2198', '  Train Loss: 0.5837', '  LR: 1.0e-03', '  Time: 0:19:00')\n",
      "2024-06-24 14:18:12,620 - ('  Epoch: 5', '  Batch: 1150/2198', '  Train Loss: 0.5821', '  LR: 1.0e-03', '  Time: 0:19:05')\n",
      "2024-06-24 14:18:16,897 - ('  Epoch: 5', '  Batch: 1200/2198', '  Train Loss: 0.5835', '  LR: 1.0e-03', '  Time: 0:19:09')\n",
      "2024-06-24 14:18:21,271 - ('  Epoch: 5', '  Batch: 1250/2198', '  Train Loss: 0.5833', '  LR: 1.0e-03', '  Time: 0:19:13')\n",
      "2024-06-24 14:18:25,755 - ('  Epoch: 5', '  Batch: 1300/2198', '  Train Loss: 0.5832', '  LR: 1.0e-03', '  Time: 0:19:18')\n",
      "2024-06-24 14:18:30,246 - ('  Epoch: 5', '  Batch: 1350/2198', '  Train Loss: 0.5817', '  LR: 1.0e-03', '  Time: 0:19:22')\n",
      "2024-06-24 14:18:34,789 - ('  Epoch: 5', '  Batch: 1400/2198', '  Train Loss: 0.5812', '  LR: 1.0e-03', '  Time: 0:19:27')\n",
      "2024-06-24 14:18:39,257 - ('  Epoch: 5', '  Batch: 1450/2198', '  Train Loss: 0.5823', '  LR: 1.0e-03', '  Time: 0:19:31')\n",
      "2024-06-24 14:18:43,639 - ('  Epoch: 5', '  Batch: 1500/2198', '  Train Loss: 0.5819', '  LR: 1.0e-03', '  Time: 0:19:36')\n",
      "2024-06-24 14:18:47,901 - ('  Epoch: 5', '  Batch: 1550/2198', '  Train Loss: 0.5824', '  LR: 1.0e-03', '  Time: 0:19:40')\n",
      "2024-06-24 14:18:52,162 - ('  Epoch: 5', '  Batch: 1600/2198', '  Train Loss: 0.5822', '  LR: 1.0e-03', '  Time: 0:19:44')\n",
      "2024-06-24 14:18:56,432 - ('  Epoch: 5', '  Batch: 1650/2198', '  Train Loss: 0.5818', '  LR: 1.0e-03', '  Time: 0:19:49')\n",
      "2024-06-24 14:19:00,712 - ('  Epoch: 5', '  Batch: 1700/2198', '  Train Loss: 0.5821', '  LR: 1.0e-03', '  Time: 0:19:53')\n",
      "2024-06-24 14:19:04,979 - ('  Epoch: 5', '  Batch: 1750/2198', '  Train Loss: 0.5828', '  LR: 1.0e-03', '  Time: 0:19:57')\n",
      "2024-06-24 14:19:09,574 - ('  Epoch: 5', '  Batch: 1800/2198', '  Train Loss: 0.5820', '  LR: 1.0e-03', '  Time: 0:20:02')\n",
      "2024-06-24 14:19:14,072 - ('  Epoch: 5', '  Batch: 1850/2198', '  Train Loss: 0.5825', '  LR: 1.0e-03', '  Time: 0:20:06')\n",
      "2024-06-24 14:19:18,619 - ('  Epoch: 5', '  Batch: 1900/2198', '  Train Loss: 0.5792', '  LR: 1.0e-03', '  Time: 0:20:11')\n",
      "2024-06-24 14:19:23,091 - ('  Epoch: 5', '  Batch: 1950/2198', '  Train Loss: 0.5802', '  LR: 1.0e-03', '  Time: 0:20:15')\n",
      "2024-06-24 14:19:27,364 - ('  Epoch: 5', '  Batch: 2000/2198', '  Train Loss: 0.5811', '  LR: 1.0e-03', '  Time: 0:20:19')\n",
      "2024-06-24 14:19:31,729 - ('  Epoch: 5', '  Batch: 2050/2198', '  Train Loss: 0.5819', '  LR: 1.0e-03', '  Time: 0:20:24')\n",
      "2024-06-24 14:19:35,992 - ('  Epoch: 5', '  Batch: 2100/2198', '  Train Loss: 0.5815', '  LR: 1.0e-03', '  Time: 0:20:28')\n",
      "2024-06-24 14:19:40,265 - ('  Epoch: 5', '  Batch: 2150/2198', '  Train Loss: 0.5809', '  LR: 1.0e-03', '  Time: 0:20:32')\n",
      "2024-06-24 14:20:04,573 - \n",
      "Epoch: 5  Val Loss: 0.5667  R2 score: 0.3687\n",
      "2024-06-24 14:20:04,575 - Validation loss decreased, saving new best model and resetting patience counter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 targets were excluded during evaluation of R2 score.\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 14:20:09,075 - ('  Epoch: 6', '  Batch: 50/2198', '  Train Loss: 0.5787', '  LR: 1.0e-03', '  Time: 0:21:01')\n",
      "2024-06-24 14:20:13,433 - ('  Epoch: 6', '  Batch: 100/2198', '  Train Loss: 0.5767', '  LR: 1.0e-03', '  Time: 0:21:06')\n",
      "2024-06-24 14:20:17,692 - ('  Epoch: 6', '  Batch: 150/2198', '  Train Loss: 0.5774', '  LR: 1.0e-03', '  Time: 0:21:10')\n",
      "2024-06-24 14:20:21,951 - ('  Epoch: 6', '  Batch: 200/2198', '  Train Loss: 0.5775', '  LR: 1.0e-03', '  Time: 0:21:14')\n",
      "2024-06-24 14:20:26,228 - ('  Epoch: 6', '  Batch: 250/2198', '  Train Loss: 0.5776', '  LR: 1.0e-03', '  Time: 0:21:18')\n",
      "2024-06-24 14:20:30,483 - ('  Epoch: 6', '  Batch: 300/2198', '  Train Loss: 0.5770', '  LR: 1.0e-03', '  Time: 0:21:23')\n",
      "2024-06-24 14:20:34,772 - ('  Epoch: 6', '  Batch: 350/2198', '  Train Loss: 0.5778', '  LR: 1.0e-03', '  Time: 0:21:27')\n",
      "2024-06-24 14:20:39,429 - ('  Epoch: 6', '  Batch: 400/2198', '  Train Loss: 0.5763', '  LR: 1.0e-03', '  Time: 0:21:32')\n",
      "2024-06-24 14:20:43,925 - ('  Epoch: 6', '  Batch: 450/2198', '  Train Loss: 0.5773', '  LR: 1.0e-03', '  Time: 0:21:36')\n",
      "2024-06-24 14:20:48,469 - ('  Epoch: 6', '  Batch: 500/2198', '  Train Loss: 0.5769', '  LR: 1.0e-03', '  Time: 0:21:41')\n",
      "2024-06-24 14:20:52,847 - ('  Epoch: 6', '  Batch: 550/2198', '  Train Loss: 0.5771', '  LR: 1.0e-03', '  Time: 0:21:45')\n",
      "2024-06-24 14:20:57,104 - ('  Epoch: 6', '  Batch: 600/2198', '  Train Loss: 0.5763', '  LR: 1.0e-03', '  Time: 0:21:49')\n",
      "2024-06-24 14:21:01,469 - ('  Epoch: 6', '  Batch: 650/2198', '  Train Loss: 0.5777', '  LR: 1.0e-03', '  Time: 0:21:54')\n",
      "2024-06-24 14:21:05,730 - ('  Epoch: 6', '  Batch: 700/2198', '  Train Loss: 0.5773', '  LR: 1.0e-03', '  Time: 0:21:58')\n",
      "2024-06-24 14:21:09,997 - ('  Epoch: 6', '  Batch: 750/2198', '  Train Loss: 0.5770', '  LR: 1.0e-03', '  Time: 0:22:02')\n",
      "2024-06-24 14:21:14,271 - ('  Epoch: 6', '  Batch: 800/2198', '  Train Loss: 0.5781', '  LR: 1.0e-03', '  Time: 0:22:06')\n",
      "2024-06-24 14:21:18,538 - ('  Epoch: 6', '  Batch: 850/2198', '  Train Loss: 0.5763', '  LR: 1.0e-03', '  Time: 0:22:11')\n",
      "2024-06-24 14:21:23,183 - ('  Epoch: 6', '  Batch: 900/2198', '  Train Loss: 0.5758', '  LR: 1.0e-03', '  Time: 0:22:15')\n",
      "2024-06-24 14:21:27,704 - ('  Epoch: 6', '  Batch: 950/2198', '  Train Loss: 0.5772', '  LR: 1.0e-03', '  Time: 0:22:20')\n",
      "2024-06-24 14:21:32,211 - ('  Epoch: 6', '  Batch: 1000/2198', '  Train Loss: 0.5764', '  LR: 1.0e-03', '  Time: 0:22:24')\n",
      "2024-06-24 14:21:36,636 - ('  Epoch: 6', '  Batch: 1050/2198', '  Train Loss: 0.5763', '  LR: 1.0e-03', '  Time: 0:22:29')\n",
      "2024-06-24 14:21:40,904 - ('  Epoch: 6', '  Batch: 1100/2198', '  Train Loss: 0.5762', '  LR: 1.0e-03', '  Time: 0:22:33')\n",
      "2024-06-24 14:21:45,272 - ('  Epoch: 6', '  Batch: 1150/2198', '  Train Loss: 0.5756', '  LR: 1.0e-03', '  Time: 0:22:37')\n",
      "2024-06-24 14:21:49,531 - ('  Epoch: 6', '  Batch: 1200/2198', '  Train Loss: 0.5761', '  LR: 1.0e-03', '  Time: 0:22:42')\n",
      "2024-06-24 14:21:53,803 - ('  Epoch: 6', '  Batch: 1250/2198', '  Train Loss: 0.5750', '  LR: 1.0e-03', '  Time: 0:22:46')\n",
      "2024-06-24 14:21:58,079 - ('  Epoch: 6', '  Batch: 1300/2198', '  Train Loss: 0.5748', '  LR: 1.0e-03', '  Time: 0:22:50')\n",
      "2024-06-24 14:22:02,344 - ('  Epoch: 6', '  Batch: 1350/2198', '  Train Loss: 0.5765', '  LR: 1.0e-03', '  Time: 0:22:54')\n",
      "2024-06-24 14:22:06,883 - ('  Epoch: 6', '  Batch: 1400/2198', '  Train Loss: 0.5754', '  LR: 1.0e-03', '  Time: 0:22:59')\n",
      "2024-06-24 14:22:11,508 - ('  Epoch: 6', '  Batch: 1450/2198', '  Train Loss: 0.5746', '  LR: 1.0e-03', '  Time: 0:23:04')\n",
      "2024-06-24 14:22:16,025 - ('  Epoch: 6', '  Batch: 1500/2198', '  Train Loss: 0.5762', '  LR: 1.0e-03', '  Time: 0:23:08')\n",
      "2024-06-24 14:22:20,452 - ('  Epoch: 6', '  Batch: 1550/2198', '  Train Loss: 0.5756', '  LR: 1.0e-03', '  Time: 0:23:13')\n",
      "2024-06-24 14:22:24,721 - ('  Epoch: 6', '  Batch: 1600/2198', '  Train Loss: 0.5749', '  LR: 1.0e-03', '  Time: 0:23:17')\n",
      "2024-06-24 14:22:28,981 - ('  Epoch: 6', '  Batch: 1650/2198', '  Train Loss: 0.5744', '  LR: 1.0e-03', '  Time: 0:23:21')\n",
      "2024-06-24 14:22:33,354 - ('  Epoch: 6', '  Batch: 1700/2198', '  Train Loss: 0.5759', '  LR: 1.0e-03', '  Time: 0:23:25')\n",
      "2024-06-24 14:22:37,622 - ('  Epoch: 6', '  Batch: 1750/2198', '  Train Loss: 0.5758', '  LR: 1.0e-03', '  Time: 0:23:30')\n",
      "2024-06-24 14:22:41,894 - ('  Epoch: 6', '  Batch: 1800/2198', '  Train Loss: 0.5736', '  LR: 1.0e-03', '  Time: 0:23:34')\n",
      "2024-06-24 14:22:46,181 - ('  Epoch: 6', '  Batch: 1850/2198', '  Train Loss: 0.5749', '  LR: 1.0e-03', '  Time: 0:23:38')\n",
      "2024-06-24 14:22:50,723 - ('  Epoch: 6', '  Batch: 1900/2198', '  Train Loss: 0.5726', '  LR: 1.0e-03', '  Time: 0:23:43')\n",
      "2024-06-24 14:22:55,329 - ('  Epoch: 6', '  Batch: 1950/2198', '  Train Loss: 0.5734', '  LR: 1.0e-03', '  Time: 0:23:47')\n",
      "2024-06-24 14:22:59,868 - ('  Epoch: 6', '  Batch: 2000/2198', '  Train Loss: 0.5740', '  LR: 1.0e-03', '  Time: 0:23:52')\n",
      "2024-06-24 14:23:04,258 - ('  Epoch: 6', '  Batch: 2050/2198', '  Train Loss: 0.5740', '  LR: 1.0e-03', '  Time: 0:23:56')\n",
      "2024-06-24 14:23:08,515 - ('  Epoch: 6', '  Batch: 2100/2198', '  Train Loss: 0.5756', '  LR: 1.0e-03', '  Time: 0:24:01')\n",
      "2024-06-24 14:23:12,774 - ('  Epoch: 6', '  Batch: 2150/2198', '  Train Loss: 0.5739', '  LR: 1.0e-03', '  Time: 0:24:05')\n",
      "2024-06-24 14:23:36,702 - \n",
      "Epoch: 6  Val Loss: 0.5589  R2 score: 0.3789\n",
      "2024-06-24 14:23:36,704 - Validation loss decreased, saving new best model and resetting patience counter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 targets were excluded during evaluation of R2 score.\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 14:23:41,317 - ('  Epoch: 7', '  Batch: 50/2198', '  Train Loss: 0.5714', '  LR: 1.0e-03', '  Time: 0:24:33')\n",
      "2024-06-24 14:23:45,946 - ('  Epoch: 7', '  Batch: 100/2198', '  Train Loss: 0.5708', '  LR: 1.0e-03', '  Time: 0:24:38')\n",
      "2024-06-24 14:23:50,232 - ('  Epoch: 7', '  Batch: 150/2198', '  Train Loss: 0.5703', '  LR: 1.0e-03', '  Time: 0:24:42')\n",
      "2024-06-24 14:23:54,495 - ('  Epoch: 7', '  Batch: 200/2198', '  Train Loss: 0.5713', '  LR: 1.0e-03', '  Time: 0:24:47')\n",
      "2024-06-24 14:23:58,755 - ('  Epoch: 7', '  Batch: 250/2198', '  Train Loss: 0.5713', '  LR: 1.0e-03', '  Time: 0:24:51')\n",
      "2024-06-24 14:24:03,025 - ('  Epoch: 7', '  Batch: 300/2198', '  Train Loss: 0.5711', '  LR: 1.0e-03', '  Time: 0:24:55')\n",
      "2024-06-24 14:24:07,416 - ('  Epoch: 7', '  Batch: 350/2198', '  Train Loss: 0.5718', '  LR: 1.0e-03', '  Time: 0:25:00')\n",
      "2024-06-24 14:24:11,670 - ('  Epoch: 7', '  Batch: 400/2198', '  Train Loss: 0.5701', '  LR: 1.0e-03', '  Time: 0:25:04')\n",
      "2024-06-24 14:24:15,970 - ('  Epoch: 7', '  Batch: 450/2198', '  Train Loss: 0.5692', '  LR: 1.0e-03', '  Time: 0:25:08')\n",
      "2024-06-24 14:24:20,522 - ('  Epoch: 7', '  Batch: 500/2198', '  Train Loss: 0.5702', '  LR: 1.0e-03', '  Time: 0:25:13')\n",
      "2024-06-24 14:24:25,005 - ('  Epoch: 7', '  Batch: 550/2198', '  Train Loss: 0.5715', '  LR: 1.0e-03', '  Time: 0:25:17')\n",
      "2024-06-24 14:24:29,534 - ('  Epoch: 7', '  Batch: 600/2198', '  Train Loss: 0.5694', '  LR: 1.0e-03', '  Time: 0:25:22')\n",
      "2024-06-24 14:24:34,033 - ('  Epoch: 7', '  Batch: 650/2198', '  Train Loss: 0.5710', '  LR: 1.0e-03', '  Time: 0:25:26')\n",
      "2024-06-24 14:24:38,288 - ('  Epoch: 7', '  Batch: 700/2198', '  Train Loss: 0.5710', '  LR: 1.0e-03', '  Time: 0:25:30')\n",
      "2024-06-24 14:24:42,545 - ('  Epoch: 7', '  Batch: 750/2198', '  Train Loss: 0.5701', '  LR: 1.0e-03', '  Time: 0:25:35')\n",
      "2024-06-24 14:24:46,806 - ('  Epoch: 7', '  Batch: 800/2198', '  Train Loss: 0.5683', '  LR: 1.0e-03', '  Time: 0:25:39')\n",
      "2024-06-24 14:24:51,074 - ('  Epoch: 7', '  Batch: 850/2198', '  Train Loss: 0.5712', '  LR: 1.0e-03', '  Time: 0:25:43')\n",
      "2024-06-24 14:24:55,448 - ('  Epoch: 7', '  Batch: 900/2198', '  Train Loss: 0.5682', '  LR: 1.0e-03', '  Time: 0:25:48')\n",
      "2024-06-24 14:24:59,740 - ('  Epoch: 7', '  Batch: 950/2198', '  Train Loss: 0.5700', '  LR: 1.0e-03', '  Time: 0:25:52')\n",
      "2024-06-24 14:25:04,296 - ('  Epoch: 7', '  Batch: 1000/2198', '  Train Loss: 0.5711', '  LR: 1.0e-03', '  Time: 0:25:56')\n",
      "2024-06-24 14:25:08,779 - ('  Epoch: 7', '  Batch: 1050/2198', '  Train Loss: 0.5683', '  LR: 1.0e-03', '  Time: 0:26:01')\n",
      "2024-06-24 14:25:13,313 - ('  Epoch: 7', '  Batch: 1100/2198', '  Train Loss: 0.5685', '  LR: 1.0e-03', '  Time: 0:26:05')\n",
      "2024-06-24 14:25:17,819 - ('  Epoch: 7', '  Batch: 1150/2198', '  Train Loss: 0.5704', '  LR: 1.0e-03', '  Time: 0:26:10')\n",
      "2024-06-24 14:25:22,072 - ('  Epoch: 7', '  Batch: 1200/2198', '  Train Loss: 0.5700', '  LR: 1.0e-03', '  Time: 0:26:14')\n",
      "2024-06-24 14:25:26,331 - ('  Epoch: 7', '  Batch: 1250/2198', '  Train Loss: 0.5690', '  LR: 1.0e-03', '  Time: 0:26:18')\n",
      "2024-06-24 14:25:30,599 - ('  Epoch: 7', '  Batch: 1300/2198', '  Train Loss: 0.5697', '  LR: 1.0e-03', '  Time: 0:26:23')\n",
      "2024-06-24 14:25:34,865 - ('  Epoch: 7', '  Batch: 1350/2198', '  Train Loss: 0.5689', '  LR: 1.0e-03', '  Time: 0:26:27')\n",
      "2024-06-24 14:25:39,241 - ('  Epoch: 7', '  Batch: 1400/2198', '  Train Loss: 0.5694', '  LR: 1.0e-03', '  Time: 0:26:31')\n",
      "2024-06-24 14:25:43,529 - ('  Epoch: 7', '  Batch: 1450/2198', '  Train Loss: 0.5694', '  LR: 1.0e-03', '  Time: 0:26:36')\n",
      "2024-06-24 14:25:48,081 - ('  Epoch: 7', '  Batch: 1500/2198', '  Train Loss: 0.5711', '  LR: 1.0e-03', '  Time: 0:26:40')\n",
      "2024-06-24 14:25:52,566 - ('  Epoch: 7', '  Batch: 1550/2198', '  Train Loss: 0.5687', '  LR: 1.0e-03', '  Time: 0:26:45')\n",
      "2024-06-24 14:25:57,097 - ('  Epoch: 7', '  Batch: 1600/2198', '  Train Loss: 0.5687', '  LR: 1.0e-03', '  Time: 0:26:49')\n",
      "2024-06-24 14:26:01,481 - ('  Epoch: 7', '  Batch: 1650/2198', '  Train Loss: 0.5687', '  LR: 1.0e-03', '  Time: 0:26:54')\n",
      "2024-06-24 14:26:05,847 - ('  Epoch: 7', '  Batch: 1700/2198', '  Train Loss: 0.5699', '  LR: 1.0e-03', '  Time: 0:26:58')\n",
      "2024-06-24 14:26:10,104 - ('  Epoch: 7', '  Batch: 1750/2198', '  Train Loss: 0.5695', '  LR: 1.0e-03', '  Time: 0:27:02')\n",
      "2024-06-24 14:26:14,362 - ('  Epoch: 7', '  Batch: 1800/2198', '  Train Loss: 0.5679', '  LR: 1.0e-03', '  Time: 0:27:06')\n",
      "2024-06-24 14:26:18,629 - ('  Epoch: 7', '  Batch: 1850/2198', '  Train Loss: 0.5688', '  LR: 1.0e-03', '  Time: 0:27:11')\n",
      "2024-06-24 14:26:22,901 - ('  Epoch: 7', '  Batch: 1900/2198', '  Train Loss: 0.5702', '  LR: 1.0e-03', '  Time: 0:27:15')\n",
      "2024-06-24 14:26:27,291 - ('  Epoch: 7', '  Batch: 1950/2198', '  Train Loss: 0.5678', '  LR: 1.0e-03', '  Time: 0:27:19')\n",
      "2024-06-24 14:26:31,838 - ('  Epoch: 7', '  Batch: 2000/2198', '  Train Loss: 0.5685', '  LR: 1.0e-03', '  Time: 0:27:24')\n",
      "2024-06-24 14:26:36,336 - ('  Epoch: 7', '  Batch: 2050/2198', '  Train Loss: 0.5685', '  LR: 1.0e-03', '  Time: 0:27:28')\n",
      "2024-06-24 14:26:40,856 - ('  Epoch: 7', '  Batch: 2100/2198', '  Train Loss: 0.5687', '  LR: 1.0e-03', '  Time: 0:27:33')\n",
      "2024-06-24 14:26:45,252 - ('  Epoch: 7', '  Batch: 2150/2198', '  Train Loss: 0.5693', '  LR: 1.0e-03', '  Time: 0:27:37')\n",
      "2024-06-24 14:27:08,594 - \n",
      "Epoch: 7  Val Loss: 0.5529  R2 score: 0.3851\n",
      "2024-06-24 14:27:08,596 - Validation loss decreased, saving new best model and resetting patience counter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 targets were excluded during evaluation of R2 score.\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 14:27:13,103 - ('  Epoch: 8', '  Batch: 50/2198', '  Train Loss: 0.5649', '  LR: 1.0e-03', '  Time: 0:28:05')\n",
      "2024-06-24 14:27:17,624 - ('  Epoch: 8', '  Batch: 100/2198', '  Train Loss: 0.5650', '  LR: 1.0e-03', '  Time: 0:28:10')\n",
      "2024-06-24 14:27:22,157 - ('  Epoch: 8', '  Batch: 150/2198', '  Train Loss: 0.5647', '  LR: 1.0e-03', '  Time: 0:28:14')\n",
      "2024-06-24 14:27:26,766 - ('  Epoch: 8', '  Batch: 200/2198', '  Train Loss: 0.5644', '  LR: 1.0e-03', '  Time: 0:28:19')\n",
      "2024-06-24 14:27:31,033 - ('  Epoch: 8', '  Batch: 250/2198', '  Train Loss: 0.5650', '  LR: 1.0e-03', '  Time: 0:28:23')\n",
      "2024-06-24 14:27:35,287 - ('  Epoch: 8', '  Batch: 300/2198', '  Train Loss: 0.5637', '  LR: 1.0e-03', '  Time: 0:28:27')\n",
      "2024-06-24 14:27:39,543 - ('  Epoch: 8', '  Batch: 350/2198', '  Train Loss: 0.5668', '  LR: 1.0e-03', '  Time: 0:28:32')\n",
      "2024-06-24 14:27:43,799 - ('  Epoch: 8', '  Batch: 400/2198', '  Train Loss: 0.5670', '  LR: 1.0e-03', '  Time: 0:28:36')\n",
      "2024-06-24 14:27:48,183 - ('  Epoch: 8', '  Batch: 450/2198', '  Train Loss: 0.5638', '  LR: 1.0e-03', '  Time: 0:28:40')\n",
      "2024-06-24 14:27:52,442 - ('  Epoch: 8', '  Batch: 500/2198', '  Train Loss: 0.5646', '  LR: 1.0e-03', '  Time: 0:28:45')\n",
      "2024-06-24 14:27:56,776 - ('  Epoch: 8', '  Batch: 550/2198', '  Train Loss: 0.5658', '  LR: 1.0e-03', '  Time: 0:28:49')\n",
      "2024-06-24 14:28:01,341 - ('  Epoch: 8', '  Batch: 600/2198', '  Train Loss: 0.5655', '  LR: 1.0e-03', '  Time: 0:28:53')\n",
      "2024-06-24 14:28:05,829 - ('  Epoch: 8', '  Batch: 650/2198', '  Train Loss: 0.5650', '  LR: 1.0e-03', '  Time: 0:28:58')\n",
      "2024-06-24 14:28:10,383 - ('  Epoch: 8', '  Batch: 700/2198', '  Train Loss: 0.5642', '  LR: 1.0e-03', '  Time: 0:29:02')\n",
      "2024-06-24 14:28:14,805 - ('  Epoch: 8', '  Batch: 750/2198', '  Train Loss: 0.5643', '  LR: 1.0e-03', '  Time: 0:29:07')\n",
      "2024-06-24 14:28:19,061 - ('  Epoch: 8', '  Batch: 800/2198', '  Train Loss: 0.5655', '  LR: 1.0e-03', '  Time: 0:29:11')\n",
      "2024-06-24 14:28:23,315 - ('  Epoch: 8', '  Batch: 850/2198', '  Train Loss: 0.5648', '  LR: 1.0e-03', '  Time: 0:29:15')\n",
      "2024-06-24 14:28:27,576 - ('  Epoch: 8', '  Batch: 900/2198', '  Train Loss: 0.5640', '  LR: 1.0e-03', '  Time: 0:29:20')\n",
      "2024-06-24 14:28:31,844 - ('  Epoch: 8', '  Batch: 950/2198', '  Train Loss: 0.5657', '  LR: 1.0e-03', '  Time: 0:29:24')\n",
      "2024-06-24 14:28:36,229 - ('  Epoch: 8', '  Batch: 1000/2198', '  Train Loss: 0.5643', '  LR: 1.0e-03', '  Time: 0:29:28')\n",
      "2024-06-24 14:28:40,542 - ('  Epoch: 8', '  Batch: 1050/2198', '  Train Loss: 0.5644', '  LR: 1.0e-03', '  Time: 0:29:33')\n",
      "2024-06-24 14:28:45,116 - ('  Epoch: 8', '  Batch: 1100/2198', '  Train Loss: 0.5638', '  LR: 1.0e-03', '  Time: 0:29:37')\n",
      "2024-06-24 14:28:49,593 - ('  Epoch: 8', '  Batch: 1150/2198', '  Train Loss: 0.5650', '  LR: 1.0e-03', '  Time: 0:29:42')\n",
      "2024-06-24 14:28:54,147 - ('  Epoch: 8', '  Batch: 1200/2198', '  Train Loss: 0.5642', '  LR: 1.0e-03', '  Time: 0:29:46')\n",
      "2024-06-24 14:28:58,592 - ('  Epoch: 8', '  Batch: 1250/2198', '  Train Loss: 0.5626', '  LR: 1.0e-03', '  Time: 0:29:51')\n",
      "2024-06-24 14:29:02,846 - ('  Epoch: 8', '  Batch: 1300/2198', '  Train Loss: 0.5643', '  LR: 1.0e-03', '  Time: 0:29:55')\n",
      "2024-06-24 14:29:07,105 - ('  Epoch: 8', '  Batch: 1350/2198', '  Train Loss: 0.5639', '  LR: 1.0e-03', '  Time: 0:29:59')\n",
      "2024-06-24 14:29:11,368 - ('  Epoch: 8', '  Batch: 1400/2198', '  Train Loss: 0.5654', '  LR: 1.0e-03', '  Time: 0:30:03')\n",
      "2024-06-24 14:29:15,632 - ('  Epoch: 8', '  Batch: 1450/2198', '  Train Loss: 0.5643', '  LR: 1.0e-03', '  Time: 0:30:08')\n",
      "2024-06-24 14:29:20,026 - ('  Epoch: 8', '  Batch: 1500/2198', '  Train Loss: 0.5651', '  LR: 1.0e-03', '  Time: 0:30:12')\n",
      "2024-06-24 14:29:24,322 - ('  Epoch: 8', '  Batch: 1550/2198', '  Train Loss: 0.5651', '  LR: 1.0e-03', '  Time: 0:30:16')\n",
      "2024-06-24 14:29:28,905 - ('  Epoch: 8', '  Batch: 1600/2198', '  Train Loss: 0.5639', '  LR: 1.0e-03', '  Time: 0:30:21')\n",
      "2024-06-24 14:29:33,380 - ('  Epoch: 8', '  Batch: 1650/2198', '  Train Loss: 0.5638', '  LR: 1.0e-03', '  Time: 0:30:25')\n",
      "2024-06-24 14:29:37,930 - ('  Epoch: 8', '  Batch: 1700/2198', '  Train Loss: 0.5655', '  LR: 1.0e-03', '  Time: 0:30:30')\n",
      "2024-06-24 14:29:42,264 - ('  Epoch: 8', '  Batch: 1750/2198', '  Train Loss: 0.5622', '  LR: 1.0e-03', '  Time: 0:30:34')\n",
      "2024-06-24 14:29:46,630 - ('  Epoch: 8', '  Batch: 1800/2198', '  Train Loss: 0.5639', '  LR: 1.0e-03', '  Time: 0:30:39')\n",
      "2024-06-24 14:29:50,886 - ('  Epoch: 8', '  Batch: 1850/2198', '  Train Loss: 0.5662', '  LR: 1.0e-03', '  Time: 0:30:43')\n",
      "2024-06-24 14:29:55,148 - ('  Epoch: 8', '  Batch: 1900/2198', '  Train Loss: 0.5632', '  LR: 1.0e-03', '  Time: 0:30:47')\n",
      "2024-06-24 14:29:59,407 - ('  Epoch: 8', '  Batch: 1950/2198', '  Train Loss: 0.5642', '  LR: 1.0e-03', '  Time: 0:30:52')\n",
      "2024-06-24 14:30:03,684 - ('  Epoch: 8', '  Batch: 2000/2198', '  Train Loss: 0.5646', '  LR: 1.0e-03', '  Time: 0:30:56')\n",
      "2024-06-24 14:30:08,088 - ('  Epoch: 8', '  Batch: 2050/2198', '  Train Loss: 0.5637', '  LR: 1.0e-03', '  Time: 0:31:00')\n",
      "2024-06-24 14:30:12,666 - ('  Epoch: 8', '  Batch: 2100/2198', '  Train Loss: 0.5630', '  LR: 1.0e-03', '  Time: 0:31:05')\n",
      "2024-06-24 14:30:17,145 - ('  Epoch: 8', '  Batch: 2150/2198', '  Train Loss: 0.5649', '  LR: 1.0e-03', '  Time: 0:31:09')\n",
      "2024-06-24 14:30:40,910 - \n",
      "Epoch: 8  Val Loss: 0.5493  R2 score: 0.3867\n",
      "2024-06-24 14:30:40,911 - Validation loss decreased, saving new best model and resetting patience counter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 targets were excluded during evaluation of R2 score.\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 14:30:45,255 - ('  Epoch: 9', '  Batch: 50/2198', '  Train Loss: 0.5614', '  LR: 1.0e-03', '  Time: 0:31:37')\n",
      "2024-06-24 14:30:49,620 - ('  Epoch: 9', '  Batch: 100/2198', '  Train Loss: 0.5585', '  LR: 1.0e-03', '  Time: 0:31:42')\n",
      "2024-06-24 14:30:54,016 - ('  Epoch: 9', '  Batch: 150/2198', '  Train Loss: 0.5602', '  LR: 1.0e-03', '  Time: 0:31:46')\n",
      "2024-06-24 14:30:58,522 - ('  Epoch: 9', '  Batch: 200/2198', '  Train Loss: 0.5595', '  LR: 1.0e-03', '  Time: 0:31:51')\n",
      "2024-06-24 14:31:03,057 - ('  Epoch: 9', '  Batch: 250/2198', '  Train Loss: 0.5597', '  LR: 1.0e-03', '  Time: 0:31:55')\n",
      "2024-06-24 14:31:07,581 - ('  Epoch: 9', '  Batch: 300/2198', '  Train Loss: 0.5608', '  LR: 1.0e-03', '  Time: 0:32:00')\n",
      "2024-06-24 14:31:11,856 - ('  Epoch: 9', '  Batch: 350/2198', '  Train Loss: 0.5611', '  LR: 1.0e-03', '  Time: 0:32:04')\n",
      "2024-06-24 14:31:16,219 - ('  Epoch: 9', '  Batch: 400/2198', '  Train Loss: 0.5613', '  LR: 1.0e-03', '  Time: 0:32:08')\n",
      "2024-06-24 14:31:20,473 - ('  Epoch: 9', '  Batch: 450/2198', '  Train Loss: 0.5612', '  LR: 1.0e-03', '  Time: 0:32:13')\n",
      "2024-06-24 14:31:24,731 - ('  Epoch: 9', '  Batch: 500/2198', '  Train Loss: 0.5620', '  LR: 1.0e-03', '  Time: 0:32:17')\n",
      "2024-06-24 14:31:29,002 - ('  Epoch: 9', '  Batch: 550/2198', '  Train Loss: 0.5600', '  LR: 1.0e-03', '  Time: 0:32:21')\n",
      "2024-06-24 14:31:33,258 - ('  Epoch: 9', '  Batch: 600/2198', '  Train Loss: 0.5602', '  LR: 1.0e-03', '  Time: 0:32:25')\n",
      "2024-06-24 14:31:37,744 - ('  Epoch: 9', '  Batch: 650/2198', '  Train Loss: 0.5605', '  LR: 1.0e-03', '  Time: 0:32:30')\n",
      "2024-06-24 14:31:42,263 - ('  Epoch: 9', '  Batch: 700/2198', '  Train Loss: 0.5611', '  LR: 1.0e-03', '  Time: 0:32:34')\n",
      "2024-06-24 14:31:46,790 - ('  Epoch: 9', '  Batch: 750/2198', '  Train Loss: 0.5608', '  LR: 1.0e-03', '  Time: 0:32:39')\n",
      "2024-06-24 14:31:51,300 - ('  Epoch: 9', '  Batch: 800/2198', '  Train Loss: 0.5603', '  LR: 1.0e-03', '  Time: 0:32:43')\n",
      "2024-06-24 14:31:55,569 - ('  Epoch: 9', '  Batch: 850/2198', '  Train Loss: 0.5613', '  LR: 1.0e-03', '  Time: 0:32:48')\n",
      "2024-06-24 14:31:59,920 - ('  Epoch: 9', '  Batch: 900/2198', '  Train Loss: 0.5593', '  LR: 1.0e-03', '  Time: 0:32:52')\n",
      "2024-06-24 14:32:04,170 - ('  Epoch: 9', '  Batch: 950/2198', '  Train Loss: 0.5602', '  LR: 1.0e-03', '  Time: 0:32:56')\n",
      "2024-06-24 14:32:08,423 - ('  Epoch: 9', '  Batch: 1000/2198', '  Train Loss: 0.5605', '  LR: 1.0e-03', '  Time: 0:33:01')\n",
      "2024-06-24 14:32:12,695 - ('  Epoch: 9', '  Batch: 1050/2198', '  Train Loss: 0.5597', '  LR: 1.0e-03', '  Time: 0:33:05')\n",
      "2024-06-24 14:32:16,954 - ('  Epoch: 9', '  Batch: 1100/2198', '  Train Loss: 0.5596', '  LR: 1.0e-03', '  Time: 0:33:09')\n",
      "2024-06-24 14:32:21,468 - ('  Epoch: 9', '  Batch: 1150/2198', '  Train Loss: 0.5592', '  LR: 1.0e-03', '  Time: 0:33:14')\n",
      "2024-06-24 14:32:25,982 - ('  Epoch: 9', '  Batch: 1200/2198', '  Train Loss: 0.5617', '  LR: 1.0e-03', '  Time: 0:33:18')\n",
      "2024-06-24 14:32:30,506 - ('  Epoch: 9', '  Batch: 1250/2198', '  Train Loss: 0.5607', '  LR: 1.0e-03', '  Time: 0:33:23')\n",
      "2024-06-24 14:32:34,996 - ('  Epoch: 9', '  Batch: 1300/2198', '  Train Loss: 0.5605', '  LR: 1.0e-03', '  Time: 0:33:27')\n",
      "2024-06-24 14:32:39,262 - ('  Epoch: 9', '  Batch: 1350/2198', '  Train Loss: 0.5604', '  LR: 1.0e-03', '  Time: 0:33:31')\n",
      "2024-06-24 14:32:43,513 - ('  Epoch: 9', '  Batch: 1400/2198', '  Train Loss: 0.5598', '  LR: 1.0e-03', '  Time: 0:33:36')\n",
      "2024-06-24 14:32:47,871 - ('  Epoch: 9', '  Batch: 1450/2198', '  Train Loss: 0.5613', '  LR: 1.0e-03', '  Time: 0:33:40')\n",
      "2024-06-24 14:32:52,126 - ('  Epoch: 9', '  Batch: 1500/2198', '  Train Loss: 0.5605', '  LR: 1.0e-03', '  Time: 0:33:44')\n",
      "2024-06-24 14:32:56,401 - ('  Epoch: 9', '  Batch: 1550/2198', '  Train Loss: 0.5600', '  LR: 1.0e-03', '  Time: 0:33:49')\n",
      "2024-06-24 14:33:00,654 - ('  Epoch: 9', '  Batch: 1600/2198', '  Train Loss: 0.5589', '  LR: 1.0e-03', '  Time: 0:33:53')\n",
      "2024-06-24 14:33:05,036 - ('  Epoch: 9', '  Batch: 1650/2198', '  Train Loss: 0.5608', '  LR: 1.0e-03', '  Time: 0:33:57')\n",
      "2024-06-24 14:33:09,660 - ('  Epoch: 9', '  Batch: 1700/2198', '  Train Loss: 0.5584', '  LR: 1.0e-03', '  Time: 0:34:02')\n",
      "2024-06-24 14:33:14,185 - ('  Epoch: 9', '  Batch: 1750/2198', '  Train Loss: 0.5605', '  LR: 1.0e-03', '  Time: 0:34:06')\n",
      "2024-06-24 14:33:18,695 - ('  Epoch: 9', '  Batch: 1800/2198', '  Train Loss: 0.5595', '  LR: 1.0e-03', '  Time: 0:34:11')\n",
      "2024-06-24 14:33:22,965 - ('  Epoch: 9', '  Batch: 1850/2198', '  Train Loss: 0.5618', '  LR: 1.0e-03', '  Time: 0:34:15')\n",
      "2024-06-24 14:33:27,212 - ('  Epoch: 9', '  Batch: 1900/2198', '  Train Loss: 0.5600', '  LR: 1.0e-03', '  Time: 0:34:19')\n",
      "2024-06-24 14:33:31,565 - ('  Epoch: 9', '  Batch: 1950/2198', '  Train Loss: 0.5596', '  LR: 1.0e-03', '  Time: 0:34:24')\n",
      "2024-06-24 14:33:35,819 - ('  Epoch: 9', '  Batch: 2000/2198', '  Train Loss: 0.5579', '  LR: 1.0e-03', '  Time: 0:34:28')\n",
      "2024-06-24 14:33:40,092 - ('  Epoch: 9', '  Batch: 2050/2198', '  Train Loss: 0.5602', '  LR: 1.0e-03', '  Time: 0:34:32')\n",
      "2024-06-24 14:33:44,343 - ('  Epoch: 9', '  Batch: 2100/2198', '  Train Loss: 0.5597', '  LR: 1.0e-03', '  Time: 0:34:36')\n",
      "2024-06-24 14:33:48,715 - ('  Epoch: 9', '  Batch: 2150/2198', '  Train Loss: 0.5585', '  LR: 1.0e-03', '  Time: 0:34:41')\n",
      "2024-06-24 14:34:12,980 - \n",
      "Epoch: 9  Val Loss: 0.5442  R2 score: 0.3949\n",
      "2024-06-24 14:34:12,982 - Validation loss decreased, saving new best model and resetting patience counter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 targets were excluded during evaluation of R2 score.\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 14:34:17,386 - ('  Epoch: 10', '  Batch: 50/2198', '  Train Loss: 0.5571', '  LR: 1.0e-03', '  Time: 0:35:09')\n",
      "2024-06-24 14:34:21,764 - ('  Epoch: 10', '  Batch: 100/2198', '  Train Loss: 0.5552', '  LR: 1.0e-03', '  Time: 0:35:14')\n",
      "2024-06-24 14:34:26,003 - ('  Epoch: 10', '  Batch: 150/2198', '  Train Loss: 0.5574', '  LR: 1.0e-03', '  Time: 0:35:18')\n",
      "2024-06-24 14:34:30,259 - ('  Epoch: 10', '  Batch: 200/2198', '  Train Loss: 0.5558', '  LR: 1.0e-03', '  Time: 0:35:22')\n",
      "2024-06-24 14:34:34,731 - ('  Epoch: 10', '  Batch: 250/2198', '  Train Loss: 0.5575', '  LR: 1.0e-03', '  Time: 0:35:27')\n",
      "2024-06-24 14:34:39,216 - ('  Epoch: 10', '  Batch: 300/2198', '  Train Loss: 0.5572', '  LR: 1.0e-03', '  Time: 0:35:31')\n",
      "2024-06-24 14:34:43,844 - ('  Epoch: 10', '  Batch: 350/2198', '  Train Loss: 0.5561', '  LR: 1.0e-03', '  Time: 0:35:36')\n",
      "2024-06-24 14:34:48,312 - ('  Epoch: 10', '  Batch: 400/2198', '  Train Loss: 0.5547', '  LR: 1.0e-03', '  Time: 0:35:40')\n",
      "2024-06-24 14:34:52,558 - ('  Epoch: 10', '  Batch: 450/2198', '  Train Loss: 0.5566', '  LR: 1.0e-03', '  Time: 0:35:45')\n",
      "2024-06-24 14:34:56,804 - ('  Epoch: 10', '  Batch: 500/2198', '  Train Loss: 0.5566', '  LR: 1.0e-03', '  Time: 0:35:49')\n",
      "2024-06-24 14:35:01,053 - ('  Epoch: 10', '  Batch: 550/2198', '  Train Loss: 0.5566', '  LR: 1.0e-03', '  Time: 0:35:53')\n",
      "2024-06-24 14:35:05,301 - ('  Epoch: 10', '  Batch: 600/2198', '  Train Loss: 0.5555', '  LR: 1.0e-03', '  Time: 0:35:57')\n",
      "2024-06-24 14:35:09,665 - ('  Epoch: 10', '  Batch: 650/2198', '  Train Loss: 0.5561', '  LR: 1.0e-03', '  Time: 0:36:02')\n",
      "2024-06-24 14:35:13,917 - ('  Epoch: 10', '  Batch: 700/2198', '  Train Loss: 0.5581', '  LR: 1.0e-03', '  Time: 0:36:06')\n",
      "2024-06-24 14:35:18,380 - ('  Epoch: 10', '  Batch: 750/2198', '  Train Loss: 0.5570', '  LR: 1.0e-03', '  Time: 0:36:10')\n",
      "2024-06-24 14:35:22,868 - ('  Epoch: 10', '  Batch: 800/2198', '  Train Loss: 0.5567', '  LR: 1.0e-03', '  Time: 0:36:15')\n",
      "2024-06-24 14:35:27,389 - ('  Epoch: 10', '  Batch: 850/2198', '  Train Loss: 0.5573', '  LR: 1.0e-03', '  Time: 0:36:19')\n",
      "2024-06-24 14:35:31,964 - ('  Epoch: 10', '  Batch: 900/2198', '  Train Loss: 0.5581', '  LR: 1.0e-03', '  Time: 0:36:24')\n",
      "2024-06-24 14:35:36,214 - ('  Epoch: 10', '  Batch: 950/2198', '  Train Loss: 0.5568', '  LR: 1.0e-03', '  Time: 0:36:28')\n",
      "2024-06-24 14:35:40,457 - ('  Epoch: 10', '  Batch: 1000/2198', '  Train Loss: 0.5558', '  LR: 1.0e-03', '  Time: 0:36:33')\n",
      "2024-06-24 14:35:44,707 - ('  Epoch: 10', '  Batch: 1050/2198', '  Train Loss: 0.5562', '  LR: 1.0e-03', '  Time: 0:36:37')\n",
      "2024-06-24 14:35:48,957 - ('  Epoch: 10', '  Batch: 1100/2198', '  Train Loss: 0.5553', '  LR: 1.0e-03', '  Time: 0:36:41')\n",
      "2024-06-24 14:35:53,323 - ('  Epoch: 10', '  Batch: 1150/2198', '  Train Loss: 0.5574', '  LR: 1.0e-03', '  Time: 0:36:45')\n",
      "2024-06-24 14:35:57,575 - ('  Epoch: 10', '  Batch: 1200/2198', '  Train Loss: 0.5572', '  LR: 1.0e-03', '  Time: 0:36:50')\n",
      "2024-06-24 14:36:02,004 - ('  Epoch: 10', '  Batch: 1250/2198', '  Train Loss: 0.5576', '  LR: 1.0e-03', '  Time: 0:36:54')\n",
      "2024-06-24 14:36:06,531 - ('  Epoch: 10', '  Batch: 1300/2198', '  Train Loss: 0.5578', '  LR: 1.0e-03', '  Time: 0:36:59')\n",
      "2024-06-24 14:36:11,045 - ('  Epoch: 10', '  Batch: 1350/2198', '  Train Loss: 0.5565', '  LR: 1.0e-03', '  Time: 0:37:03')\n",
      "2024-06-24 14:36:15,639 - ('  Epoch: 10', '  Batch: 1400/2198', '  Train Loss: 0.5567', '  LR: 1.0e-03', '  Time: 0:37:08')\n",
      "2024-06-24 14:36:19,883 - ('  Epoch: 10', '  Batch: 1450/2198', '  Train Loss: 0.5568', '  LR: 1.0e-03', '  Time: 0:37:12')\n",
      "2024-06-24 14:36:24,132 - ('  Epoch: 10', '  Batch: 1500/2198', '  Train Loss: 0.5558', '  LR: 1.0e-03', '  Time: 0:37:16')\n",
      "2024-06-24 14:36:28,381 - ('  Epoch: 10', '  Batch: 1550/2198', '  Train Loss: 0.5576', '  LR: 1.0e-03', '  Time: 0:37:20')\n",
      "2024-06-24 14:36:32,632 - ('  Epoch: 10', '  Batch: 1600/2198', '  Train Loss: 0.5556', '  LR: 1.0e-03', '  Time: 0:37:25')\n",
      "2024-06-24 14:36:36,886 - ('  Epoch: 10', '  Batch: 1650/2198', '  Train Loss: 0.5570', '  LR: 1.0e-03', '  Time: 0:37:29')\n",
      "2024-06-24 14:36:41,254 - ('  Epoch: 10', '  Batch: 1700/2198', '  Train Loss: 0.5555', '  LR: 1.0e-03', '  Time: 0:37:33')\n",
      "2024-06-24 14:36:45,679 - ('  Epoch: 10', '  Batch: 1750/2198', '  Train Loss: 0.5577', '  LR: 1.0e-03', '  Time: 0:37:38')\n",
      "2024-06-24 14:36:50,192 - ('  Epoch: 10', '  Batch: 1800/2198', '  Train Loss: 0.5552', '  LR: 1.0e-03', '  Time: 0:37:42')\n",
      "2024-06-24 14:36:54,713 - ('  Epoch: 10', '  Batch: 1850/2198', '  Train Loss: 0.5553', '  LR: 1.0e-03', '  Time: 0:37:47')\n",
      "2024-06-24 14:36:59,193 - ('  Epoch: 10', '  Batch: 1900/2198', '  Train Loss: 0.5557', '  LR: 1.0e-03', '  Time: 0:37:51')\n",
      "2024-06-24 14:37:03,557 - ('  Epoch: 10', '  Batch: 1950/2198', '  Train Loss: 0.5564', '  LR: 1.0e-03', '  Time: 0:37:56')\n",
      "2024-06-24 14:37:07,809 - ('  Epoch: 10', '  Batch: 2000/2198', '  Train Loss: 0.5591', '  LR: 1.0e-03', '  Time: 0:38:00')\n",
      "2024-06-24 14:37:12,061 - ('  Epoch: 10', '  Batch: 2050/2198', '  Train Loss: 0.5559', '  LR: 1.0e-03', '  Time: 0:38:04')\n",
      "2024-06-24 14:37:16,318 - ('  Epoch: 10', '  Batch: 2100/2198', '  Train Loss: 0.5568', '  LR: 1.0e-03', '  Time: 0:38:08')\n",
      "2024-06-24 14:37:20,586 - ('  Epoch: 10', '  Batch: 2150/2198', '  Train Loss: 0.5569', '  LR: 1.0e-03', '  Time: 0:38:13')\n",
      "2024-06-24 14:37:44,853 - \n",
      "Epoch: 10  Val Loss: 0.5409  R2 score: 0.3963\n",
      "2024-06-24 14:37:44,855 - Validation loss decreased, saving new best model and resetting patience counter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 targets were excluded during evaluation of R2 score.\n",
      "Total time: 0:39:22\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torchmetrics.regression import R2Score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Main script for training the model.')\n",
    "    parser.add_argument('--config', type=str, default='hyper.yaml', help='Path to the config file..')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    config = load_config(args.config)\n",
    "    setup_logging('training.log')\n",
    "    \n",
    "    logging.info(\"-----------------------------------------------------------------------------\")\n",
    "    logging.info(f\"Hyperparameters:\")\n",
    "    log_hyperparameters(config)\n",
    "\n",
    "    ts = time.time()\n",
    "    \n",
    "    x_train, y_train, FEAT_COLS, TARGET_COLS = load_train() \n",
    "    x_train, y_train, mx, sx, my, sy = normalization(x_train, y_train) # x_train:[_, 556(feat_cols)], y_train:[_, 368(target_cols)]\n",
    "    \n",
    "    seed_everything()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"am i using gpu? :\", device)\n",
    "    \n",
    "    dataset = NumpyDataset(x_train, y_train)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size = config['BATCH_SIZE'] , shuffle = True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = config['BATCH_SIZE'], shuffle=False)\n",
    "    \n",
    "    input_size = x_train.shape[1] #556\n",
    "    output_size = y_train.shape[1] #368\n",
    "    hidden_size = input_size + output_size #556+368\n",
    "    model = FFNN(input_size, [3*hidden_size, 2*hidden_size, 2*hidden_size, 2*hidden_size, 3*hidden_size], output_size).to(device)\n",
    "    criterion = LpLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr = config['LEARNING_RATE'], weight_decay = config['WEIGHT_DECAY'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=config['SCHEDULER_FACTOR'], patience=config['SCHEDULER_PATIENCE'])\n",
    "    print(\"Time after all preparations:\", format_time(time.time()-ts), flush=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_count = 0\n",
    "    r2score = R2Score(num_outputs=len(TARGET_COLS)).to(device)\n",
    "    \n",
    "    for epoch in range(config['EPOCHS']):\n",
    "        print(\" \")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        steps = 0\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels) #EDA\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            steps += 1\n",
    "\n",
    "            if (batch_idx + 1) % config['PRINT_FREQ'] == 0 :\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                elapsed_time = format_time(time.time() - ts)\n",
    "                log_message = (\n",
    "                              f'  Epoch: {epoch+1}',\\\n",
    "                              f'  Batch: {batch_idx + 1}/{len(train_loader)}',\\\n",
    "                              f'  Train Loss: {total_loss / steps:.4f}',\\\n",
    "                              f'  LR: {current_lr:.1e}',\\\n",
    "                              f'  Time: {elapsed_time}'\n",
    "                              )\n",
    "                logging.info(log_message)\n",
    "                total_loss = 0\n",
    "                steps = 0\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        y_true = torch.tensor([], device = device)\n",
    "        all_outputs = torch.tensor([], device = device)\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                y_true = torch.cat((y_true, labels), 0)\n",
    "                all_outputs = torch.cat((all_outputs, outputs), 0) #vs. train EDA\n",
    "        r2 = 0\n",
    "        r2_broken = []\n",
    "        r2_broken_names = []\n",
    "        for i in range(output_size): # 368\n",
    "            r2_i = r2score(all_outputs[:, i], y_true[:, i])\n",
    "            if r2_i > 1e-6:\n",
    "                r2 += r2_i\n",
    "            else:\n",
    "                r2_broken.append(i)\n",
    "                r2_broken_names.append(FEAT_COLS[i])\n",
    "        r2 /= output_size\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_log = (f'\\nEpoch: {epoch+1}  Val Loss: {avg_val_loss:.4f}  R2 score: {r2:.4f}')\n",
    "        logging.info(val_log)\n",
    "\n",
    "        print(f'{len(r2_broken)} targets were excluded during evaluation of R2 score.')\n",
    "        #print(f\"r2_broken:, {r2_broken}\")\n",
    "        #print(f\"r2_broken_names: {r2_broken_names}\")\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_count = 0\n",
    "            log_msg = (\"Validation loss decreased, saving new best model and resetting patience counter.\")\n",
    "            logging.info(log_msg)\n",
    "        else:\n",
    "            patience_count += 1\n",
    "            log_msg = (f\"No improvement in validation loss for {patience_count} epochs.\")\n",
    "            logging.info(log_msg)\n",
    "\n",
    "        if patience_count >= config['PATIENCE']:\n",
    "            log_msg = (\"Stopping early due to no improvement in validation loss.\")\n",
    "            logging.info(log_msg)\n",
    "            break\n",
    "\n",
    "del x_train, y_train\n",
    "gc.collect()\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "model.eval()\n",
    "\n",
    "df_test = pl.read_csv('/data01/jhko/LEAP/test.csv')\n",
    "\n",
    "for col in FEAT_COLS:\n",
    "    df_test = df_test.with_columns(pl.col(col).cast(pl.Float32))\n",
    "    \n",
    "x_test = df_test.select(FEAT_COLS).to_numpy()\n",
    "x_test = (x_test - mx.reshape(1,-1)) / sx.reshape(1,-1)\n",
    "predt = np.zeros([x_test.shape[0], output_size], dtype=np.float32)\n",
    "\n",
    "i1 = 0\n",
    "for i in range(10000):\n",
    "    i2 = np.minimum(i1 + config['BATCH_SIZE'], x_test.shape[0])\n",
    "    if i1 == i2:\n",
    "        break\n",
    "        \n",
    "    inputs = torch.from_numpy(x_test[i1:i2, :]).float().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        predt[i1:i2, :] = outputs.cpu().numpy()\n",
    "        \n",
    "    i1 = i2\n",
    "    \n",
    "    if i2 >= x_test.shape[0]:\n",
    "        break\n",
    "        \n",
    "for i in range(sy.shape[0]):\n",
    "    if sy[i] < 1e-8 * 1.1:\n",
    "        predt[:, i] = 0\n",
    "        \n",
    "predt = predt * sy.reshape(1, -1) + my.reshape(1, -1)\n",
    "\n",
    "ss = pd.read_csv(\"/data01/jhko/LEAP/sample_submission.csv\")\n",
    "ss.iloc[:, 1:] = ss.iloc[:, 1:].astype('float')\n",
    "ss.iloc[:, 1:] = predt\n",
    "\n",
    "del predt\n",
    "gc.collect()\n",
    "\n",
    "use_cols = []\n",
    "for i in range(27):\n",
    "    use_cols.append(f\"ptend_q0002_{i}\")\n",
    "\n",
    "ss2 = pd.read_csv(\"/data01/jhko/LEAP/sample_submission.csv\")\n",
    "df_test = df_test.to_pandas()\n",
    "\n",
    "for col in use_cols:\n",
    "    ss[col] = -df_test[col.replace(\"ptend\", \"state\")]*ss2[col]/1200.\n",
    "\n",
    "test_polars = pl.from_pandas(ss[[\"sample_id\"]+TARGET_COLS])\n",
    "test_polars.write_csv(\"submission.csv\")\n",
    "\n",
    "print(\"Total time:\", format_time(time.time()-ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LEAP_dchong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
